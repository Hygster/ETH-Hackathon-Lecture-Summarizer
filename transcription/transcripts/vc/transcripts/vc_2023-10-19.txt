 [MUSIC PLAYING]
 OK, so good afternoon.
 So today we'll continue from Tuesday
 with those unitary transforms.
 And then we'll also look at pyramids and wavelets.
 So this is what we saw last time.
 In particular, we get for a given photo collection,
 for example, faces, we can represent them
 as vectors in a high dimensional vector space.
 We stack them all together.
 We don't forget to subtract the average
 so that we actually take a distribution wherever
 it is in that space, in a vector space,
 and we move it to be centered around 0.
 That's this one.
 Then we do closest rank k approximation, k being based on--
 we do the singular value decomposition of this matrix,
 not this one, this one.
 And we see how many singular values are significant,
 are important to capture most of the energy, most of what's
 going on in the data set.
 So we choose that k to provide a good approximation here.
 So that rank of the matrix, the number of singular values
 we preserve.
 And essentially, what we do in practice
 is all the remaining ones we zero out.
 We assume it's OK to zero them out.
 If we do that, then computing the difference
 between a query image and one of the images in database,
 which is the metric we'll use to say that two images are close,
 this is the same in the shifted space.
 So in taking the space, translating to 0
 doesn't change distances.
 So that's the same.
 And then this is an approximation.
 So that's where we have to choose what is appropriate,
 what errors can we cope with, also
 knowing that some of those differences
 might actually not be relevant.
 We saw in particular that noise, for example, it would actually
 be a really bad idea to do this modeling somehow
 on to try to preserve everything and to do it
 on a really high resolution image
 because then any error we made in the alignment of the faces
 and finding a nice crop of the face
 would actually show up as differences.
 And it's something we'd have to model.
 And then we would count this as differences.
 We would count it against a match,
 that your face is just slightly shifted.
 Then every small detail that's different
 because of the shift would actually be counted
 against the recognition.
 So it's actually a good idea to bring this down
 to a smaller dimensional space, in particular
 to suppress noise and things like this.
 In a sense, doing this closest rank approximation
 will automatically, for a big part,
 eliminate a lot of the noise.
 Because as you can see here, this average face,
 for example, is very much smoothed out.
 So if you combine a lot of data, you'll
 kind of get the average, the dominant signals,
 and somehow ignore the noise.
 OK, so essentially, then a face, for example, a face image
 gets decomposed by you take the average,
 and then you have a few components, coefficients here
 that multiply with the so-called eigenfaces.
 And with the right combination of coefficients,
 you can kind of reconstruct every face.
 We saw that also in particular, we can even do that in 3D
 with the work of the morphable faces from plants and vetter.
 That was this 3D video that I showed you yesterday,
 on Tuesday.
 Any questions with respect to this?
 Then we also saw these limitations.
 In particular, I just said it before,
 there are some-- when you compare a query face,
 for example, or any kind of thing you want to recognize,
 you compare it to the images that you
 store in your database.
 There are some changes you care about,
 and there's some changes you don't care about.
 So random noise, random points on being shaven or not,
 or things like this, details you would not want to care about.
 You would really want to focus on the core characteristics
 of the face.
 For example, also, you might want
 to be able to do the recognition independently of the lighting,
 because you don't care about the lighting,
 you care about the identity.
 So there can be some very significant changes that
 actually are not important.
 That's where we ended last time, is that essentially,
 surprisingly, you can do a better recognition job
 by ignoring the first three components of the eigenfaces,
 of the eigenface decomposition, if you
 have a lot of changing illumination in your database.
 Then we started looking at a principled way to approach this.
 Principled, but still relatively simple, linear method.
 So principled method.
 And of course, you can launch modern methods
 that will just, through gradient descent,
 find a very complex representation of all
 of this data, and then provide it
 with the right loss functions that
 will guide it to learn the right representation.
 Here, we focus in this course, we
 focus on mostly looking at the principled,
 the kind of baseline principled methods to do this.
 So let's say we have a data set like this one.
 We have five different classes that we would
 like to be able to separate, so to recognize and know
 if we get a new data point, does it
 belong to the green or the red or the black,
 et cetera, category.
 So the way we'll do this is we'll
 look separately at the difference within a class,
 and then the difference across classes.
 So in particular, we'll take the average of each class, which
 are the dots you see here, and then we'll
 fit essentially two different variances.
 We'll first fit the variance between the classes.
 So how does one class differ from another class?
 That's a distribution of the different classes.
 In a sense, it kind of roughly fits these five different dots.
 And then separately, we'll look at the within class variation.
 Now, we have five different within class variations.
 What we'll do is, as we did before, actually,
 we'll subtract out the average for the class
 and then assume that within a class,
 they all behave roughly similarly.
 In a sense, we'll fit an ellipse.
 Here we fit five ellipses, or it shows five ellipses.
 What we actually did is for each of them,
 we subtract this point, and then we're
 left with the relative vectors to this average point.
 And those are actually the same for all of the classes.
 So now we have a lot of points that all
 are in the same ellipsoid.
 They all map onto each other.
 And we fit one ellipsoid, one ellipse in 2D,
 but one ellipsoid in any dimensions,
 to all of those data points.
 So essentially, it's the same ellipse just shifted,
 shifted by these translation vectors.
 And now what we want to do conceptually
 is we want to maximize this, but ignore that.
 So in a sense, what we'll do conceptually is maximize that,
 but minimize that variation, like the importance
 of that variation.
 So if a point differs in this direction,
 this is kind of unimportant for us,
 because that's actually a change that we don't care about.
 You had a question?
 [INAUDIBLE]
 So for all five, it's the same, yes.
 [INAUDIBLE]
 This method will, yes, will do the same.
 The assumption is that this is kind of--
 that they're kind of in the same place.
 Let's say with lighting, right?
 The assumption is that the light will kind of do a similar effect
 and behave similarly in this space
 and approximately linearly in this space,
 and that all those classes are close enough together
 that the changes of bright light from this side
 or from that side or so will kind of
 have a similar type of effect.
 Think of it as if we look here, that you'll
 have similar kind of--
 that modeling lighting on one person on the other person
 will kind of essentially just be moving in this dimension,
 essentially, for both of them, and that there
 will be other dimensions in which you move that
 correspond more to identity.
 So the method can only cope with one--
 essentially, we see it here.
 So conceptually, you care--
 you want to maximize this change here, but kind of minimize--
 not care about that change.
 So you kind of, in the end, you end up
 with a kind of division from one by the other.
 In some sense, I will not cover the math here in detail,
 but you can look at this.
 You can look at all of this as you want at home.
 Go for this, but essentially, it will--
 this ends up being essentially a generalized eigenvector
 problem.
 So the normal eigenvector--
 where do I have the cursor here?
 OK, here.
 Normal eigenvector problem just has a matrix here.
 This was the matrix-- this one here.
 So this part doesn't exist for the regular eigenvalue problem.
 But here, essentially, we add, then, this second matrix,
 the ones we don't care about.
 We add it on that side.
 And that actually, then, solving this generalized eigenvalue
 problem is actually the correct way to solve this problem.
 In a way, you can also very easily do this
 if you look at the singular value decomposition,
 because you can easily generate an inverse by just--
 for all the non-zero singular values,
 remember, you can just take a singular value
 and do 1 over that singular value
 to generate the pseudo-inverse and transpose, of course.
 So your left singular vector is becoming your right one,
 and vice versa.
 But then, whatever dimension gets
 stretched through a linear transformation, linear mapping,
 to invert that, if it gets stretched by a factor of 3,
 now you shrink it by a factor of 3 to undo that stretch.
 Think a bit of the same way we inverted filters.
 You can invert or divide by a matrix
 by just, essentially, multiplying
 with the pseudo-inverse of that matrix.
 That's a similar way.
 In that case, instead of the original matrix here,
 you do this matrix with--
 you multiply by the pseudo-inverse of this matrix
 over here, and then it's just a normal singular value problem,
 if you would do it that way.
 Anyways, if you have already the singular value decomposition,
 you don't even need to do the singular value
 decomposition, of course.
 All of those things are equivalent.
 That's also where I don't want to dwell too long into the math
 here, but it's more important to essentially get, really,
 the kind of intuition here that you maximize, essentially,
 this variation while, at the same time,
 ignoring as much as possible that variation.
 And so it's really the ratio.
 So in other words, if you look at those two,
 a variation in this direction is going
 to be important, while a variation in that direction
 is not going to be important.
 So in a sense, what you'll do is really
 take this ellipsoid here and then squeeze it
 along this dimension.
 That's the division by that one, in a sense.
 That's what it actually will correspond to.
 You could verify all of this by yourself if you want.
 But it will essentially take that matrix
 and end up squeezing.
 And it will be the same effect as if you
 had that squeezed matrix, which is
 a division by that ratio, essentially.
 So here's a very simple example where
 we have just two dimensions here, class 1 and class 2.
 We have circles and pluses.
 They are distributed here, here and here,
 in the original space.
 And then we can do two things.
 One is we can do PCA.
 And we do it centered around 0.
 If we do PCA of these crosses and these circles,
 the two together, we do PCA, we see
 that most of the variation is in this direction.
 And we'd end up with that being the one principal component
 in two dimensions.
 And then if we only preserve this one dimension from the two
 original dimensions of this 2D space,
 we project it on the 1D space.
 So we do a principal component analysis.
 We find the dominant principal component.
 If we keep two, of course, we keep everything.
 So we just keep one.
 We say this here is the dominant direction of variation
 in this two-dimensional plot.
 So we project on this line.
 And what do we get?
 Well, we get essentially all the things that initially
 were nicely separable, the pluses on that side,
 the circles on this side.
 They all map into this direction of principal variation.
 They actually all map in between each other.
 So although this is the direction of biggest variation,
 it's not a good one because it's actually also the direction--
 this is not the direction on which you can recognize
 that they are different.
 They both vary a lot together in that direction.
 But they vary-- the change, the difference between the two
 is actually in this other direction.
 So think of what we did on the previous one.
 If we take the average of this one, we get a dot here.
 We get the average of this one.
 We get a dot here.
 Now we do the difference between those two.
 So we model that difference between classes.
 That will give us a vector exactly in this direction.
 In other words, we get this axis here.
 So the Fisher-- this is Fisher linear discriminant.
 So we get that direction.
 So this Fisher faces then, if we will apply these two faces,
 will essentially, instead of choosing this
 as the one dimension you keep out of two,
 you would actually choose this dimension.
 And of course, you can see that if you use this direction
 and now project orthogonally to this direction,
 you actually get a nice separation of both classes.
 Of course, this is a well-chosen perfect example for this.
 But it illustrates the concept.
 So let's get back to this.
 This was the thing.
 So eigenfaces without the three first components,
 Fisher faces.
 So here, essentially, you do even better
 by not just getting rid of the first three components,
 but by actually properly looking at what
 part of the variations come from lighting, which
 I don't care about, or what part doesn't come from identity.
 So what is the variation within--
 for each person, what's the variation,
 the different pictures of that person?
 In other words, here, this is the within class variation
 for this person.
 You see, essentially, that these are all the different ways
 that that person can look like.
 And you say, this change, I explicitly don't care about.
 So please divide it out.
 That's the concept here.
 OK.
 So maybe at this point, many of you say, yes,
 but why somehow was it three?
 Why did you end up taking out--
 why was it a good idea to take out three components there?
 So let me-- this is a bit of a preview
 from what you'll see in graphics in the second half.
 But it's quite relevant here.
 So let me kind of briefly explain that.
 So essentially, the way that lighting gets modeled,
 if I have a light source, for example, the projector there,
 and it projects kind of vertically to my hand here,
 then essentially, it will capture the most amount of light
 on the surface of my hand.
 If I hold-- so if you all look at my hand,
 actually, from-- my hand is mostly so-called diffuse.
 It means that my hand, or diffuse material,
 will-- or actually, also the projector here,
 the surface here on the wall, it kind of
 reflects light uniformly in all directions the same amount.
 So you see as much light there as you see light over here.
 So you will both see as brightly from any direction,
 essentially.
 But if I turn my hand gradually more and more like this,
 then my hand will actually become darker
 from your viewpoint or so.
 So essentially, why?
 Because now, the amount of light--
 so here, it's this width, this broad of a beam of light.
 And then when I turn my hand like this,
 there's only a thin slice of light that still hits this.
 So there's less light on my surface,
 on the surface of my hand.
 But now, it's still independently
 from where you look, as long as you can see my hand,
 it will look as bright.
 But of course, also, as you look,
 you will see at one particular point, you will--
 if you look from there, you look straight on it.
 So you see a broader area.
 If you look from there, you see a thin area.
 So there's difference that way.
 But in the end, the amount of light--
 so how bright a point looks if the light gets reflected
 evenly, so a Lambertian surface.
 What really matters is, what is the angle at which the light
 illuminates the surface?
 So in other words, we have the normal of the surface.
 And what matters is, essentially,
 this cosine here between the angle
 of the normal of the surface and the incoming light direction.
 That angle is what matters.
 So when I put my hand like this, it's essentially,
 this here is the normal.
 And so when that normal is at a big angle
 to the incoming light, like this,
 then I get little light.
 So if the surface is orthogonal, in other words,
 the normal is aligned, then I get the maximum response.
 So in the end, what does it mean?
 If I have normalized vectors, if my surface normal is normalized,
 then I get something simply as this expression here,
 which is, this is how much a--
 let me forget a for now.
 We get this expression here, where actually, it starts there.
 L is the intensity of the light.
 How bright the light is, obviously,
 if the light is twice as bright, it's
 going to be twice as much light on the surface.
 Then we have the product of the light direction
 and the normal of the surface.
 If those are two both normalized,
 this is simply the cosine.
 It's 1 if they're the same, and it's 0 if they're orthogonal.
 And it varies as a cosine in between.
 And then a is actually the surface.
 If this was a--
 if my hand was black, it would reflect very little light.
 If it's light, then it reflects a lot of the light.
 If it's perfectly white, it will reflect 100% of the light.
 So that is the ratio of how much of the light
 actually gets reflected.
 That's what this a, this albedo is.
 So this doesn't really matter.
 This doesn't really matter.
 Those are obviously proportional.
 So the amount of light of how bright something is,
 the amount of light that is actually on the surface there
 is essentially proportional to both of those.
 And then it's just this here.
 Now, if you look at this, this is in three-dimensional space.
 We are in three-dimensional space, so these normals
 and vectors are three-dimensional vectors.
 And so essentially, at every pixel,
 if you now look at an image, at every pixel, what do we get?
 Well, we get the incoming light direction,
 which if the light is far enough,
 it's actually pretty much constant.
 So the vector from the light there,
 the vector that hits my face here or hits my face here,
 it's approximately the same vector.
 What does change is actually the normal of my face.
 So here it's like this, and then here it's like that, et cetera.
 So that's the vector that changes,
 but the other vector is constant.
 So in the end, the light vector is kind of a constant
 for a face, for example, with the light that's far away.
 So this is a constant.
 This is three numbers per pixel, essentially.
 It just describes at that pixel what is normal.
 And so no surprise, as you get this expression,
 you actually get a three-dimensional subspace.
 So here, this really explains why
 this kind of was capturing this pretty well
 to take the first three.
 The first three lighting actually
 really turns out to be the dominant factor.
 If you have a data set like this, of course,
 if the data set is always the same light,
 then this would not be an effect.
 But if you actually capture under a light that
 varies from any-- that can shine from anywhere,
 then three dimensions is not perfect.
 Things of shadows, cast shadows and so on,
 they are not modeled by this.
 The fact that this surface should be illuminated,
 except that the nose actually kept the light away from there,
 that is not modeled with this simple model
 that we just described.
 But for the rest, it's actually properly
 modeled with only three dimensions.
 Because really, you have at every pixel
 you could store, instead of one number, which
 is the brightness of the image, just
 imagine an image that just stores the x-component
 of the normal, an image that stores
 the y-component of the normal, and an image that stores
 the z-component of the normal.
 Now you could change the lighting arbitrarily
 by applying a different linear combination here
 to that image, to these three images.
 Think of this here.
 The x-component, the y-component,
 and the z-component of the normal.
 And then you just apply the light vector here
 with its three components.
 And that would be the correct formulation
 for that simple linear model.
 So no surprise that when you look at this here,
 when you look here, that actually this is not a bad
 model to get rid of most of the light variation.
 So it's not by chance that this somehow just
 happens to work out.
 This actually has some real physics, in a sense,
 behind it.
 OK, let's move on.
 This is another example on a particular database
 at a time for faces, just showing that here's
 the other way around.
 It's not showing the error.
 It's showing the success of the recognition.
 And so you see the Fisher faces gets kind of quickly
 to 90% plus recognition, while the Eigen faces are struggling,
 because they get confused by other changes.
 Here's then a Fisher face to recognize glasses, for example.
 You make two glasses, with glasses, without glasses.
 And then you find the vector that
 connects the average without glasses
 to the average with glasses.
 And now you project along that vector.
 And you see if people are likely to have glasses or not.
 If you look at that vector, the difference vector
 between people with glasses and people without glasses,
 this is the vector.
 Remember, these vectors are actually images.
 So if you take that vector and reorganize it in an image,
 that's what it looks like.
 And now if you multiply this with this,
 and you get a strong response one way or the other,
 positive, negative, it tells you probably with glasses,
 probably without glasses, with a single number,
 in a very simple classifier.
 Actually, also here, the Fisher faces
 essentially gets almost no error.
 And obviously, eigenfaces are just random.
 50% for a binary question is essentially random.
 OK, so that was how to handle-- this
 was all assuming that things were roughly-- for example,
 if we have one important dimension,
 that this was all roughly in a linear kind of subspace,
 that it could be well approximated
 by a linear subspace.
 So what if we have things that look more like this here?
 If our distribution isn't this kind of nice blob,
 round blob or elongated blob, but essentially a blob
 that has most of the stuff in the middle and then a bit less
 to the sides until it has nothing,
 and so it's kind of this nice cluster,
 what if the distribution like here
 is actually something that's circular, for example,
 meaning mostly that if I take the average of this,
 I take the average of this distribution,
 well, I get a point here, right?
 Right in the middle there.
 Clearly, that's a point that's not at all in this distribution.
 So here, doing operations like taking the average as a--
 this is what we did before implicitly.
 I didn't tell you explicitly.
 But we kind of assumed-- may I actually go back here?
 We kind of assumed that when I take the average,
 I actually have a very good example of something
 that's really in the middle of my distribution.
 That's what we assumed here.
 This would be a pretty terrible assumption here.
 So there are situations where you really
 need something more complicated, where
 you do need to take nonlinearities into account,
 not because your space is not exactly an ellipsoid,
 but it's a little bit bent.
 So you'll be able to better model
 if you model that the space is bent and not exactly straight.
 But in some cases like this, it's fundamentally
 a space that is not well captured
 by a linear kind of approximation of it.
 So here's an example of how this could be tackled.
 This is a quite old method.
 But it's kind of nice because it kind of illustrates
 some of those concepts very nicely.
 The setting was as follows.
 They had essentially about, I think, 100 objects.
 You see them here, the object collection.
 They would actually-- for each of them,
 they would look at all possible viewpoints and capture that.
 They would do that in a simple way,
 so against a simple background.
 So you could easily do background segmentation,
 which you've also looked at.
 And so just isolate the appearance of the object,
 and then center that, and then get a nice bonding box
 around it, and then try to recognize it.
 Now, obviously, an image of any of those objects
 would be a very high dimensional space.
 And it's probably a space that actually looks more like this.
 It's not likely to be linearly a kind of nice centered linear
 space or so.
 However, what we'll still do is--
 oops, sorry-- is actually still apply principal component
 analysis, keep the dominant principal components,
 and then essentially go over explicitly
 over the whole range of changes.
 In particular, here the object would always
 be put on a table.
 So it always had the same orientation.
 So the viewpoint change, what you didn't care about
 is, under what angle am I seeing this object?
 So the viewpoint change didn't really matter.
 That's a one dimensional viewpoint change.
 It's just rotating it on the surface.
 It's like, I put this object here,
 and I could rotate it in that orientation.
 That doesn't really matter.
 It's still the same object.
 Obviously, just intuitively, rotating an object 360 degrees,
 if it kind of moves around, after rotating 360 degrees,
 I have to be back in the same place.
 Fundamentally, I kind of, by nature,
 have something like this.
 I have something that, as I rotate the object 360 degrees,
 first I move away from it.
 It gets more and more different.
 If you look at me, so initially it
 gets more and more different.
 But as I keep rotating, when I'm back at the original position,
 of course I'm back at looking exactly the same.
 So it's fundamentally something like this.
 It's fundamentally a circular thing,
 if you do something like this, 360 degree rotation.
 OK.
 So essentially, what was their idea?
 Was you do this principal component analysis.
 For example, here, you project down.
 You keep the three principal components.
 And then within that, you kind of explicitly
 remember how, over the 360 degree rotation, how
 the appearance changes.
 Not in the high dimensional space,
 but in this principal component space.
 In this case, this three dimensional space.
 You could also work in higher dimensional space.
 So they essentially explicitly remember here,
 points along how that image would
 project in this three dimensional space
 for this particular object.
 OK.
 In this case, it's a car.
 So look at the car here.
 This should actually be video.
 Oops, sorry.
 OK.
 What's going on here?
 That was nice.
 OK.
 Let me restart the PowerPoint.
 Let me restart the PowerPoint.
 It's actually automatically restarting.
 Sorry for that.
 OK.
 Copilot wants to give me a summary.
 OK.
 Hopefully it's going to play fine this time.
 OK.
 OK, so you see the little car turning around there.
 It's doing background subtraction,
 and then projecting in this space,
 projecting the little clip of the bounding box
 of where the car is located.
 Vectorize that, projects it in this vector space,
 gets essentially a dot.
 The curve you see here was previously sampled curve.
 And then what you see here now is essentially
 where each of those images lands into this space.
 And as you can see, it's always kind
 of following quite closely this one-dimensional manifold
 in the space.
 There's actually something interesting to it.
 I don't know if anybody notices it.
 Anybody see something that's not necessarily completely obvious?
 [INAUDIBLE]
 OK.
 Well, yes.
 Then you can't find it anymore.
 But if you actually look carefully,
 it's a double curve here, right?
 And then here, it's a little bit further apart, the two curves.
 But over here, it's actually very close.
 Anybody knows what's going on there?
 So it looks like we don't have a single loop.
 But we kind of have a double loop somehow
 for the 360 degrees.
 Anybody knows why that is?
 [INAUDIBLE]
 Exactly.
 That's exactly what it is.
 So here, the car from some viewpoints,
 it looks very, very close.
 But from other viewpoints here, from front and back,
 the front and back is actually a bit different
 for this particular car.
 So there, you still see the difference.
 So there, they are a little bit apart.
 So that's essentially what you see here.
 So here's now the system in action.
 Again, as I told you, it kind of finds this-- it crops it,
 it projects it, and then it finds to which of those--
 it has 100 of those curves in a three-dimensional space,
 and it finds which one of those 100 curves it is closest to.
 OK?
 So the head is running in real time back in '96,
 when computers were not as fast as now.
 So that's pretty cool.
 OK, so now let's actually transition
 to looking at how we can compress images,
 generic images.
 So with this PCA, and principal component analysis,
 or Cajun and Loewe transform, we looked
 at how we could do interesting things
 on specific type of images, like the 100 objects we had before,
 or even more importantly, on faces, for example.
 Because faces are not just a random image, not even
 a random natural image, but they are actually
 a very specific type of image.
 And so if we want to analyze faces, recognize faces,
 we actually want to work in a space that
 corresponds to faces, not in the space of arbitrary images.
 But of course, for compression of images and things
 like that, we'll actually want to be
 able to cope with any natural image that we show it.
 So here's actually a kind of interesting plot,
 or picture, which is take a large set of natural image
 patches, small crops from images, and run SVD on it,
 get the principal component analysis extracted from it.
 If you remember when we did Fourier transform and so on,
 I told you that the magnitude transform will kind of always
 look the same for different types of images.
 It essentially really has low frequencies that are dominating.
 And then as you go to higher frequency,
 they are less and less present, in a sense.
 So if we do PCA, or a singular value decomposition of all
 these-- we take all these patches
 and we open them in a big database next to each other.
 We take that matrix and we do singular value decomposition.
 And then we look at the singular vectors, the eigen,
 which is the same as the eigenvalues
 of the product of the matrix with transpose,
 or the autocorrelation matrix.
 These are all the same.
 So if we do the principal component analysis,
 and we look at the-- for the autocorrelation matrix,
 the biggest eigenvalues.
 Or if we just do singular value decomposition of the matrix,
 we look at the biggest singular values,
 we look at the associated singular vectors,
 or the eigenvectors of the autocorrelation matrix,
 the associated eigenvectors, and rearrange them in pictures.
 So rearrange them in a two-dimensional.
 So in MATLAB, that's reshape.
 Or so just rearrange them in the original picture layout.
 We get something like this.
 So first, you see that here, this is logarithmic scale.
 You see that essentially, indeed, you
 kind of have larger values.
 And then this kind of has a distribution.
 So it's not flat distribution.
 It's actually one that continuously decreases.
 And then the question is, does this indeed somehow
 correspond to having low frequencies
 with the bigger eigenvalues, bigger singular values,
 and high frequencies with the lower singular values?
 And yes, if you look at this, you kind of
 notice this very strongly.
 What do we have here?
 Oops, up here.
 Well, we essentially get a horizontal and a vertical kind
 of cosine or sine.
 Actually, we get both.
 And we know we need to get both.
 We need to get one, and then we get one that complements it.
 So a cosine and a sine, let's say.
 In the other direction here, other orientation,
 we get the same also, roughly, et cetera.
 So actually, what we see here is that the Fourier transform is
 not something so unnatural or this or that.
 We see that we kind of automatically, roughly,
 get it by just taking natural images
 and doing a statistical analysis from it,
 getting a principal component analysis of natural images.
 And we essentially get something that's very similar to the
 Fourier transform coming out of it.
 So when we look at faces, we'll get something
 with faces and variation of faces coming out of this.
 If we just feed it with natural images,
 we actually get something like the Fourier transform
 to just come out of this.
 So that also means that as we want to now represent
 natural images, this might also be a meaningful type of basis
 to also use.
 Here's an image of a JPEG compressed image.
 So we'll talk a little bit about JPEG compression.
 Here, you immediately see some of the artifacts.
 JPEG compression, as we'll explain,
 is actually looking at the image in small 8 by 8 blocks,
 8 by 8 pixel blocks, and encodes them separately,
 mostly fully separately from block to block,
 meaning that it doesn't really have
 anything that will ensure that across a block,
 it's things are continuous.
 So if you spent enough bits, you can choose.
 If you spend enough bits per block,
 you can make sure that it actually
 explains the image very well, and therefore it
 will look smooth.
 If you actually force it to compress very strongly,
 like here, and there's not many bits left to actually explain
 what's going on, then you have a lot of error.
 And then you will see these errors explicitly
 show up at the boundaries between blocks.
 So the JPEG compression-- so first thing
 is what we saw here and before is
 that natural images have more low frequency content
 than high frequency content.
 So that's one thing to remember.
 Secondly, there's also here the Campbell-Robson contrast
 sensitivity curve.
 So let me explain what this image is.
 This image clearly in this dimension
 has an accelerating variation.
 So this is a slow variation that gets faster and faster.
 It gets faster as we go to this side.
 And then there's from here to the top,
 you essentially have a linearly decreasing contrast.
 So in other words, the amplitude of the signal
 at the bottom of the picture is like this.
 And at the top of the image, it's flat.
 And in the middle, it's much smaller, essentially.
 So the amplitude of the variation
 here of the amount of light, it varies linearly to the top.
 OK?
 So what does that mean?
 It means that if as a human, in our human visual system,
 we would have a completely linear response to all of this,
 be equally sensitive to all frequencies,
 then when you look at this, you should somehow
 see that the contrast is kind of equally fading out everywhere.
 Is that what you're seeing?
 So can someone describe kind of what
 you see here in terms of that variation of contrast?
 So where do you--
 so in other words, where do you see the most--
 like, how far can you still see changes?
 Where do you see that kind of the highest up
 that you can still see changes?
 OK, so is it here?
 Who thinks it's here?
 No one?
 OK, so lots of people start seeing it here.
 And then-- OK, people here?
 So you see most variations?
 No?
 You see it most here?
 [INAUDIBLE]
 What?
 [INAUDIBLE]
 Oh, like here somewhere?
 Or here?
 OK, I mean, it varies a bit.
 And it also varies with distance and so on.
 But essentially, you kind of get a pattern,
 let's say, that would maybe kind of look like somewhere it kind
 of goes up here, but then it goes here,
 and then it kind of falls down somewhere here.
 So essentially, in particular, as we
 go to the higher frequencies, we really--
 I mean, actually, this, we might just not see it anymore
 on this screen.
 But up to here somewhere, it really falls off.
 So as we look at the higher frequencies,
 it turns out we are less sensitive,
 or eyes less sensitive to these higher frequencies.
 So that's something we can use for compression.
 Because first, there are less high frequencies
 in the image, in natural images.
 Second, we are less sensitive to them.
 And remember, I told you here, we're encoding block by block.
 So really low frequencies, you don't see low frequencies
 here because it's only an 8 by 8 block.
 So low frequency over the whole image,
 like we're not even talking about capturing those
 at an 8 by 8 block size.
 So the 8 by 8 block is somewhere over here anyways
 that you're talking about.
 And so in this regime, it very quickly fades down
 from seeing something to kind of not seeing the high frequency
 content anymore.
 You see it if it's really high contrast.
 If you have black-white, black-white, black-white
 stripes, you see that.
 But if it's kind of a weak high frequency signal,
 you just don't notice it.
 Here, you still see variations.
 It's just up there, you don't see them.
 So we'll make use of that in JPEG.
 Essentially, what we'll do is-- so we
 take these 8 by 8 blocks.
 You see these are small blocks.
 They could look like these guys here.
 And we'll decompose them using not exactly Fourier transform.
 Remember, Fourier transform, this annoying thing
 that they were complex.
 We needed from one number, we would
 get two numbers, a real and an imaginary part,
 in particular because we needed both the cosine and the sine,
 and then the linear combination of both
 to get shifted versions of them.
 So that's kind of annoying and expensive.
 Also, if you want to build all of this in hardware,
 you need two numbers, et cetera.
 So instead of that, it uses something quite close.
 We use discrete cosine transform.
 So this one, essentially, is strictly real.
 So it's a transform that's strictly real.
 It takes a block here, an 8 by 8 pixel block,
 and decomposes it in a basis that's essentially
 8 by 8 basis elements.
 So that's normal.
 We go from 64 numbers to 64 numbers.
 No surprise.
 So this is normal basis transform.
 We'll go from 8 to 8.
 Sorry, from 64 to 64 numbers in, 64 numbers out.
 That all seems to make sense.
 And so you see here also, it captures here
 the low frequencies.
 And these are also low frequencies.
 Notice these are all cosines here.
 So always one here, one here, one here, et cetera.
 And so it kind of has these patterns here,
 low to high frequency horizontally,
 low to high frequency vertically,
 and then here, high frequency horizontally and vertically.
 So in the end, you get this kind of black and white dot pattern.
 So that's the discrete cosine transform
 closely related to the Fourier transform,
 but strictly real numbers.
 So we use this.
 So we take an 8 by 8 block.
 So what do we do to get the numbers, the transform?
 We take this block, multiply it with each of those patches.
 For each of those, we get one number.
 That's now our transform.
 It's 64 coefficients with respect
 to these basis functions.
 [MUSIC PLAYING]
 OK, and I explain to you the rest after the break.
 OK, so let's continue.
 So essentially, from what we've seen before,
 we can imagine that there's both going
 to be bigger coefficients up there than down there.
 And we actually also care less about the coefficients
 here because we're less sensitive to them.
 By the way, if our eyes are less sensitive
 to these high frequencies, it might also
 have a lot to do with them being less present in natural scenes
 and therefore also less important for us
 to actually pay attention to.
 And this is all kind of consistent.
 So our brain would have learned to notice
 the important patterns, the ones that are often there.
 OK, so essentially, the way that JPEG compression then
 works is from the-- where's the cursor again?
 Oh, there we go.
 OK, so we take every 8-byte block separately.
 We do a discrete cosine transform.
 Then we apply a quantization matrix.
 I think I might have missed some--
 OK, I don't have the quantization matrix here.
 I thought I had it somewhere.
 That's strange.
 Ah, here we go.
 OK, actually, let me go here then.
 OK, this is actually the quantization matrix.
 OK, so this one is actually going to--
 as we look at our decomposition here and our coefficients,
 the way we'll pay less attention to something
 is not by then completely dropping a coefficient
 or so normally, but it's actually
 by saying we're going to spend less bits on it in a sense.
 How do we do that?
 Well, what we do, for example, with this quantization matrix--
 and you can choose different ones.
 There's different ones that are pretty fine and so on.
 But for a particular level of quality,
 you could use different ones to have more or less compression
 and so on.
 For example, for this one, we say, OK, the value here,
 this coefficient, we'll divide it by 3,
 and then we'll represent it.
 This one, we'll divide by 5 and then represent it, et cetera.
 So in a sense, what we're doing is we're doing quantization.
 And then, of course, in the end, we only
 represent it to the closest integer.
 So what do we see?
 Well, this one at the bottom here,
 we'll divide this number, whatever comes out of this.
 It's typically values, as I told you, from 0 to 256--
 0 to 255.
 You spend 8 bits on this.
 But this one, you actually divide by 31.
 So divide it by 10 times as much as this one.
 So this one, instead of 0 to 255, you get 0 to 80 kind of.
 This one, you get 0 to 8 at most.
 So it means that at most, you'll spend 3 bits here.
 Even if you have a perfect black and white alternating pattern,
 you essentially divide it by 30.
 So you get 255 becomes like 8 something.
 And then you essentially just say, OK, you'll spend 111.
 That's the largest number you can represent there.
 So that's essentially how you do quantization.
 So that means that we'll automatically spend
 a lot less bits down here than we spend up here.
 So back here.
 So that's the zigzag scan.
 Oh, the zigzag scan is also this,
 is that actually, as I told you, we
 quantize in a way that's less important to things
 here than to things here.
 Also, there is typically a lot less here.
 You've seen often you can have relatively homogeneous regions.
 If it's regionally homogeneous, it
 means that you have this here, and then the rest is actually
 kind of pretty much 0 or so.
 In other regions, you might go further,
 but then there might not be really this high-res detail.
 So often, you will have 0s from a certain point
 on, from a certain frequency on.
 So the idea is that you start explaining things
 until you get kind of to the place where you can ignore it.
 So at some point, as you zigzag, so you essentially
 describe this coefficient, and this one, this one,
 and keep going this way.
 And at some point, you say, OK, and from here on, it's all 0.
 So I ignore the rest, and I just won't describe the rest.
 I stop here.
 That's why you're going in a zigzag pattern
 from low frequencies to high frequencies.
 OK, so that's a zigzag scan.
 And the only difference, the only thing
 you actually explain and where you connect one block
 to the next is in this here, which is a DC component.
 This kind of has to do with current, discrete current,
 and analog, and discrete and continuous.
 So this is constant, and this is a variable.
 So this is the constant, essentially.
 This one here, this value, that one is encoded separately.
 And that one is actually encoded relative to the previous one.
 So in an image like this one here, so this patch,
 you'll start from looking at a previous patch
 and say, this value is kind of the same as this one.
 Then it's very cheap to encode here.
 And that's actually the most expensive one,
 because it's the ones we quantize the least.
 We only divide by 3 instead of dividing by 30, for example.
 So this one here will take advantage
 from the continuity from one block to the other.
 All the other ones we just encode locally.
 And then we do Huffman coding.
 We'll briefly get to that in a second.
 This part is lossless compression.
 Clearly, this part, actually in particular this part,
 was loss C. The whole quantization table
 is essentially where the loss comes in.
 Without quantization table, we could actually have no loss.
 But a quantization table means that we
 are reducing the number of bits we spend.
 And then, of course, the cutting it off with a zigzag pattern
 means that we'll ignore everything beyond a certain
 stage.
 That also is lossy, of course.
 Lossy means we won't actually be able to exactly invert
 the process and get exactly the same image back.
 We'll have an approximation of the image back.
 So this is like zipping or so, just compressing normally
 in a lossless way the remaining bits that you transmit.
 Then, of course, this you would just undo.
 This one here that you would also there
 have decoded a previous value and now decode
 the current value of the current patch based
 on the previous value for the constant term.
 The other term, you would apply the inverse quantization
 matrix.
 The numbers you divide by 3, you multiply by 3.
 The numbers you divide by 31, you multiply by 31.
 And then you reassemble.
 So you multiply every time with the basis image.
 The coefficient that you've extracted now and remultiplied,
 you multiply with the basis images.
 And we add all of those together to get your patch,
 decoded patch back.
 And you keep doing that for every 8 by 8 patch
 of your image.
 So why did we use a DCT in JPEG?
 Well, it's real numbers.
 So that was what I told you instead
 of discrete of the Fourier transform.
 And you can do very fast implementations of this.
 In particular, in phones and in any chips
 these days that would be used for multimedia,
 you will have hardware blocks, hardware implementations
 of all of these things so that you don't
 have to move this through a processor
 and start fetching instructions and moving things around
 and this and that.
 No, this is all hardware that exactly takes these values in
 and does all of this in a very efficient, very low power way,
 which is why you can actually capture stills, as many still
 images as you want, and videos and so on,
 on a tiny battery in the end.
 So it's small block sizes.
 This is faster.
 You get-- at this level is really
 you capture the correlation between neighboring pixels.
 Larger blocks could give you better compression
 in smooth regions, but they would actually
 be much more costly.
 You'd have to move a whole block in memory
 and multiply with much bigger basis functions,
 also to reconstruct it, et cetera.
 So we discussed this.
 Then entropy coding, I assume you should all know that.
 Just as a quick reminder, if you have
 symbols of different probabilities,
 you can actually-- you should actually
 make sure to spend a number of bits that's
 proportional to the probabilities,
 or in particular to the fractional.
 So essentially, if you have here, with one bit,
 you can express two choices.
 So essentially 50-50 choices are exactly
 right to express with one bit.
 So if you have a character, like z here,
 that has a probability of 0.5, then
 you want to represent it with one--
 with essentially one code, with a one--
 a code of length one.
 If you have here a character y that has 25% chance,
 so one in four, you want to represent it essentially
 with two bits, two bits corresponding
 to four possible characters.
 So four characters of two bit characters,
 there's four characters.
 Therefore, four characters is 1/4 on average per character.
 It's kind of an even distribution.
 You want to be as close as possible to an even
 distribution here, a distribution
 proportional to your probabilities.
 So you want to spend two characters on this one.
 And then here you have 2 times 1/8.
 So that's actually-- you can spend three bits on this,
 expressing something like this.
 So here you would want three bits.
 And so you see that here we can represent that by essentially
 these mapping.
 If you're lucky, it maps perfectly.
 Then you can essentially spend half the encoding,
 that the one goes to the z, and then the zeros goes
 to the rest.
 If you add all of this up, you get 50%.
 So that's OK to have a zero here.
 And then the next one of what's left, half of it
 goes to this one.
 So the one you assign here, the zero
 you assign to the remaining two, which together also form
 half of the remaining probability, et cetera.
 And so this way, that's essentially
 entropy encoding or Huffman codes.
 And so this is essentially optimal encoding.
 And you spend exactly as many bits
 as there's actually bits of information
 in the information theoretical sense in the data.
 So in other words, in the encoding-- where was it?
 Here?
 So here you actually use the insights in how images
 and how to best represent images and the generic correlation
 between pixels, et cetera, et cetera, et cetera.
 This is all very specific to image encoding.
 This part is just very completely generic information
 theoretic, just spending a number of bits
 of what's actually happening in terms
 of reflecting the probabilities of symbols to show up,
 et cetera.
 So it's a mixed combination of lossy and lossless compression.
 Here's an example.
 I think this is the-- I don't know if that's uncompressed,
 but anyways, that's much more highly compressed.
 There's a few more artifacts here.
 The-- we'll get to more comparisons a little bit later
 when we compare to wavelet-based representations.
 Before getting to wavelets, we'll
 talk a little bit about scale spaces, scale representations.
 So essentially, signals, as we've already
 seen in the Fourier transform, signals really
 show up at different resolutions,
 different frequencies.
 Some things show up at high frequencies.
 At high frequencies here, you see lots of small details.
 After a while, as you blur things out,
 let's say with Gaussians and you keep blurring them out
 with Gaussians, you would essentially
 have a lot of these tiny details going up and down and up
 and down, kind of these details, to kind of essentially
 completely disappear at some point.
 They're just not there anymore.
 Particularly if you look at the second derivative here,
 you essentially-- so the zero crossings
 of the second derivative, if you see those as features here,
 you see that you can kind of trace them for a while.
 For example, this one here, it's one of those here.
 It's there for a while.
 At some point, it will just be gone, et cetera.
 So you see that actually the second derivative
 at some point, this one and this one come together,
 and then they disappear.
 So you had a bump, but after a while,
 the bump gets really smoothed out.
 It's just not there anymore.
 Not even a little bit.
 It's just gone.
 There's no bump anymore.
 That's what you kind of see here.
 So there's somehow this kind of scale.
 There's some features and some characteristics
 of the function at the beginning that are there for a while
 as you blur it through, but eventually, they just
 completely disappear as those kind of things
 merge together.
 So in images, it means that often it's actually
 very meaningful--
 and we've already a little bit looked at that earlier--
 to represent the image at all these different scales.
 And you can represent images at different scales.
 And you can, for example, here, you can approximate.
 So you blur with a Gaussian, for example.
 Then you downsample.
 And then you can look at also the difference
 and re-upsample, do the interpolation filter,
 and reconstruct to a prediction.
 And then to the difference between what your low-resolution
 image captured, which you then bring back again
 to high resolution, but it is what is still there
 as information at the low resolution,
 and do the difference with the current resolution.
 And that's essentially kind of the difference
 between those two here is kind of what is explicitly
 at this--
 captured at this scale versus what is still
 captured at lower spaces.
 So these scaled representations, they
 can be useful for finding correspondences, core scales,
 then refine with finer scales, edge tracking,
 find strong edges.
 We've seen edges a few weeks ago.
 Like if we find edges, if we smooth to not get random edges
 but actually only get strong edges,
 we might still actually want to go back
 to the high-resolution image to find exactly where the edge is.
 So we know it's important because it's still present.
 When you look here, it's still present.
 Like this is clearly an important feature
 because it's still present.
 Or this one is still present even in the coarse resolution.
 So it's one of the original dominant features.
 But you might want to not take the position here,
 which would be wherever somewhere here.
 But you might want to trace it back to the full resolution
 image where exactly it's most prominently present.
 Also in particular, also to actually control
 the computational cost of matching things and so on,
 to match it at the resolution where the feature is naturally
 represented in a compact way with convolutions and so on,
 have small convolution filters.
 So here's an example of the CMU face detector.
 This is a very old one.
 It was using already neural networks
 to find faces 20-so years ago.
 So the way to do that was to have a neural network that
 would look at the fixed number of pixels
 and would essentially, in such a 20 by 20 template,
 would find the pattern of a face.
 It would take a template here.
 It would correct the lighting, do histogram equalization.
 So make sure that there's all the type of-- you stretch out
 the contrast so that you have values everywhere,
 so that you have both white pixels and black pixels.
 You stretch out the contrast and then throw that
 in a neural network.
 And neural networks would say face, no face, essentially.
 Binary answer coming out.
 And maybe with a confidence.
 I'm not sure if the strength would represent that.
 But the key is now you get 20 by 20 pixels.
 So how do you apply that to find faces
 at any resolution in the image?
 Well, so you have a fixed resolution filter
 or a fixed resolution network here
 that will tell you on a 20 by 20 pixel
 if there is a face centered in those 20 by 20 pixels or not.
 So you can kind of apply that in, let's say,
 a convolutional type of way.
 So meaning that you're going to apply it consistently
 at every possible pixel location throughout the whole image.
 And then wherever the face would show up, you would find it.
 Except that if the face was too big,
 you would not find it because you'd
 be looking at the nose, for example,
 looking at the eye with your 20 by 20 pixels.
 You wouldn't see anything.
 So the way to do that is not to make another detector that
 finds 50 by 50 pixel faces, but it's actually
 to build up a pyramid here that would essentially
 make the image gradually smaller.
 And so that your 20 by 20 detector
 would find face of any size up from 20.
 So by essentially having an image that's
 from the original image, you would gradually reduce.
 So you would blur a bit and subsample.
 You might blur once and then subsample and then blur again
 and subsample, blur again.
 Build up a pyramid.
 And then essentially apply this 20 by 20 detector
 not only in the high resolution image,
 but essentially all the variants of the resolution.
 That means that the 20 by 20 would be,
 compared to the original image, would always
 cover a bigger and bigger area.
 And that way, you can actually get a very effective detector
 that works at all resolutions.
 And of course, as the original image
 is always the most expensive, processing a full pyramid
 is only maybe two times or a few times
 more expensive than the original high resolution image.
 Because the subsequent images in the pyramid
 are less and less pixels.
 Typically, you might generate an image at every, for example,
 square root of 2 in each dimension.
 So that means you get 256 by--
 or let's say you get 200 by 200 image.
 Then you get a 140 by 140.
 Then you get a 100 by 100, et cetera.
 Anyway, so in the end, this sums up
 to a small multiple of the original number of pixels
 to process.
 So there's two types of pyramids.
 There's the Gaussian pyramid and the Laplacian pyramid.
 The Laplacian is actually more a difference of Gaussian pyramid,
 as we'll see.
 But essentially, the idea is you smooth with--
 the first thing is that a Gaussian multiplied by Gaussian
 is another Gaussian.
 So if you blur by Gaussian and then we blur again by Gaussian,
 we actually also are blurring by a bigger Gaussian.
 And of course, after a while, and we
 got rid of enough of the high frequencies by the Gaussian,
 we can actually subsample without losing further signal,
 as these are low-pass filters.
 So we get to a redundant representation
 relatively quickly.
 So here's kind of an illustration of that.
 As you reduce the resolution here, factor of 2 every time,
 you get--
 this is how it would look like.
 But this is actually the original resolution.
 And then factor of 2 in each dimension,
 so 4 times less pixels, and then again 4 times less pixels
 again.
 So in the end, you get really only a few pixels.
 You see that here the total is much less than a factor of 2
 of the original resolution.
 It's less than a factor of 2 addition to this here.
 And you can process all of the scales at the same time.
 So Gaussian pyramid, again, illustrating here.
 The Laplacian pyramid is one that Laplacian
 is mathematical, like the representation
 of second derivative.
 But in practice, we'll actually reconstruct it
 from a Gaussian pyramid by just doing--
 so this was the Gaussian pyramid.
 What we'll do is--
 oops, sorry.
 What we'll do is we'll just subtract two consecutive layers
 here.
 If we do that, we get something like this here.
 If you actually look carefully, this one
 is the same between the two.
 It should be the same, actually.
 Maybe it's not exactly here.
 But it should actually be the same.
 At some point, it stops, and it's
 the same because there's no next one anymore.
 But all the other ones are just the difference
 between two scales.
 So this one is the difference between this one and this one.
 The difference between those two is this one.
 And then this one is the difference between what was
 here and what was here, et cetera.
 So this is always kind of specific.
 What is specific to this scale?
 Or to this scale that's not in the next scale?
 What's lost in the next scale?
 So Laplacian pyramid, in a sense, the different layers
 capture--
 so these guys, this one here is essentially
 capturing all these frequencies here, the high frequencies
 in all directions.
 That's captured.
 And so that's actually a lot of content.
 The next one is only capturing this here.
 No surprise.
 This was actually a smaller image, right?
 So there's less captured there.
 So there's this one.
 And the third one is here.
 The fourth one is there, et cetera.
 If you want, you can actually do an oriented pyramid.
 This is actually saying I'm going to look at these guys,
 that orientation separately from this orientation, and so on.
 Also here, if we are precise, the Laplacian of a Gaussian
 would be a function like this here.
 So that's like you filter first.
 You do Gaussian to properly filter the image.
 But then you do a Laplacian.
 So there's a second derivative here of that thing.
 It gives you something like this.
 If you do a difference of Gaussian, it's different.
 You see it's not the same exact function.
 But qualitatively, it has the same behavior.
 It has essentially this kind of centered surround behavior.
 So you have a peak in the middle, in this case,
 peak down in the middle, and then a kind of positive
 on the periphery of that.
 And then it goes to 0 really quickly.
 So although mathematically, it would often
 make sense to use a Laplacian, in practice,
 everybody uses the difference of Gaussians.
 Although they say Laplacian pyramid,
 they actually mean difference of Gaussian pyramid,
 because that one is actually very easy and cheap
 to compute, because you anyways already
 have to build your Gaussian pyramid, this one.
 And therefore, you build your Laplacian pyramid,
 also known as difference of Gaussian pyramid.
 You build it by literally doing the difference
 of these Gaussian blurred images.
 So the one on the right is just metronome practical.
 And they're qualitatively the same.
 They get kind of the same behavior in practice.
 So again, this one is really a difference of Gaussian,
 not a Laplacian.
 So you take this one and this one.
 You subtract out, you get this one.
 So you add this one and this one.
 You subtract out, you get this one.
 If you would like to compress, you
 could kind of do this and use this as we did before,
 and kind of quantize at this level here.
 And remember, actually, these different images-- oops.
 If I go back here, these different images
 have very different resolutions.
 So only the first one is a lot of data.
 And you quantize.
 And it's a lot of data, but it's high frequencies.
 And so you don't necessarily have to keep all of this,
 et cetera.
 So all these things were kind of looked at.
 Oriented pyramids, you could kind of look at doing,
 in addition to the frequency filters, also orientation
 filters, and so split things in orientations.
 And then you could do the Laplacian pyramid layers,
 splitted orientation layers, and vice versa,
 and then decide to quantize appropriately along the way.
 OK, so people have looked at this for many, many years.
 And it's used-- all these pyramidal representations
 are used all throughout for reasons of efficiency,
 for different reasons to use a particular filter at all scales,
 find patterns of all scales in the image
 by actually rescaling the image rather than rescaling
 your filters, et cetera.
 But then here, let's look now at specifically
 wavelet transforms.
 Wavelet transforms are interesting in the sense
 that what they really try to explicitly do, in a sense,
 is kind of look both at the scale,
 but also at the location.
 We're in the Fourier transform.
 That's kind of the problem that it was--
 that is global over the whole image.
 So you look for-- you decompose in global patterns.
 In JPEG, we kind of avoided that by going to very local 8
 by 8 blocks and working there.
 So that's kind of a hack, in a sense.
 With wavelet transforms, we'll actually
 look at something that is both local in space,
 but also in the frequency domain.
 And so we want kind of locality in both spaces.
 A pixel was hyperlocal in space.
 And then Fourier was hyperlocal in frequency.
 But they essentially had kind of impact--
 a pixel has impact on all frequencies.
 Remember, the frequency response of a pixel, of a local pulse,
 is a flat thing.
 So it's all frequencies, and vice versa.
 One frequency has a global wave pattern over the whole image.
 With wavelets, we actually want something
 that's both local in space and local--
 so compact in space, compact in frequency also.
 It will actually work, in a sense,
 by doing a two-band filter bank.
 So it decomposes a signal.
 This is frequency here, one dimensional.
 This is the frequency.
 It will decompose it in one, in a low pass and a high pass.
 And then the low pass, it will further decompose inside that
 in a low and a high pass.
 And then again, it takes again the low pass,
 redecomposes that in low pass and high pass, et cetera.
 You can see that this is kind of related to these pyramid
 concepts we described.
 So here's an example of the simplest possible wavelet
 transform.
 It's called a Hart transform.
 And it looks like as follows.
 So it's real and orthogonal.
 And it has transitions at each scale, p,
 and is localized according to q.
 So let me show you.
 These are separable Hart transforms for two dimensional.
 So let's look at this.
 So this is the 8 by 8 Hart basis.
 This is just flat.
 Then here we have one separation,
 so one localization of the transition here.
 And then here we have-- and then here we essentially have--
 so this is 0 here.
 And then this is positive and negative.
 So you have a kind of local pattern
 that's active in half of the thing and the other half is 0.
 So this is localized here.
 And then you have the other one that's localized here.
 The same pattern, but localized there and there.
 And then here you have the higher frequency,
 which is here or here or here or here.
 So explicitly we have it sampled at four locations.
 But also we have different-- so we
 have here the low, low frequency, and then here,
 still low frequency, and then higher frequency,
 and then the highest frequency.
 And notice the highest frequency is the one
 that needs most place in our representation.
 The highest frequency is kind of all of this here.
 It's like 3/4 of the image of the transform
 is actually covering the highest frequency.
 So if we compare the DCT, which we use for JPEG,
 and then here this very simple Hart basis kind of compare it.
 So here we have global patterns with a fixed frequency.
 Here we have both a frequency but also a location
 for a signal.
 Here's the kind of 1D illustration of this.
 These ones are not great as they are.
 They have poor energy compaction.
 But they are much better ones.
 Here is how you can construct in 1D in this case,
 how you can construct this.
 By the way, separable meant that you
 can do products of a 1D wavelet this way times a 1D wavelet
 that way to get the 2D wavelets.
 So here is just a 1D signal.
 We get the full signal comes in here.
 We split it in two parts, low pass and high pass.
 So we do a high pass filter, and then we subsample by two.
 And same thing is we do a low pass filter,
 and we subsample by two.
 This one will keep as is.
 And then we'll re-upsample and filter another way
 with a complementary filter with this one and reassemble.
 The other one will actually further split up.
 We actually do again the same thing.
 We separate the low pass signal in a low low
 and a low high signal.
 The low low high goes through.
 The low low actually we can further split up and so on.
 So if you remember from what we said with Fourier transforms
 and so on, you could say, wait a moment.
 Isn't this a problem?
 I have a high frequency, and I subsample it.
 What should happen if we take a high frequency
 and we subsample it?
 Anybody remembers?
 Aliasing, exactly.
 So you actually see, yes, but it's not a problem.
 Because here, because of this, we
 don't have low frequencies anymore.
 We only have high frequencies.
 And now we kind of move the high frequencies
 to low frequencies.
 By subsampling here, we can only represent low frequencies.
 At least we can actually not unambiguously
 represent low and high frequencies.
 We can't separate those anymore.
 If you only have high frequencies, it's still fine.
 Whatever shows up in our naively low frequency representation
 with limited samples, we know it's actually
 all high frequencies.
 So we're not confused.
 This is definitely not the low frequency signal here.
 So we have a signal, and we know if we just take it naively,
 it looks like a low frequency signal potentially.
 But no problem.
 We know that we can just move it back to the right location
 in the frequency spectrum.
 So there's actually not a problem here.
 But you do keep-- here, it's a little bit
 in the other direction.
 You do keep most of the data here.
 The next one is also, let's say you have 1,000 numbers coming
 in, you get 500 here, you get 250 here, you get 500 here,
 but those again get split in 250 and 250.
 This 250 would get split further, et cetera, et cetera.
 In two dimensions, it works like this.
 You have your whole frequency content here.
 You kind of separate it in the high frequencies
 and the low frequencies.
 The low frequencies get further split into low, low frequencies
 and then all combination of low and high frequencies, et cetera.
 So let's get to actually an image.
 So we start from this image.
 We decompose it through these different filters
 in these different bands.
 This is the low frequency.
 That's easy.
 If we reconstruct that, it looks just
 like a low frequency version of the image.
 The high frequencies look like this here.
 Again, they are kind of subject to aliasing.
 I mean, to this kind of aliasing,
 you kind of-- I think we'll see it more.
 So now what we do is we then take the low frequency signal
 and again decompose it in relative low and high
 frequencies.
 So we get something like this.
 And here, in a sense, you can kind of
 start seeing these things.
 Actually, look at it here.
 This was not here.
 So this is somehow some kind of something
 that looks like aliasing.
 But again, it's not aliasing.
 Or it is aliasing, but it's actually not
 problematic aliasing because we're not confused.
 We know exactly what frequency band this signal came from.
 So we can place it back there when we reconstruct things.
 And so on.
 So you do see kind of larger regions
 of the same frequency content.
 And we know what frequency content it is.
 So again, we're not confused about what the content is.
 So this is kind of a wavelet decomposition of this image.
 And then you can do similar things where you can--
 and we won't go into detail here--
 but you can do similar things in terms of compressing this.
 I think maybe I have a few more details further.
 One thing that's important to know--
 not to derive mathematically or so--
 but one thing that's important to know
 is that this can actually--
 so remember, the perfect filters--
 we looked at perfect filters before.
 We had a problem with perfect filters
 was that the perfect filter was infinitely large.
 And that brought all kind of issues with itself,
 including assumptions of the image being a periodic image,
 et cetera, et cetera, with Fourier transforms.
 The nice thing is here, we can, by choosing, actually,
 a combination of four filters--
 so we want to get to a perfect reconstruction, but not--
 in a sense, we don't do that by doing one filter and then
 a perfect inversion filter kind of thing, like subsample
 and then re-absample or something.
 But we have a combination of four filters.
 So you have a low-pass analysis filter and then
 low-pass reconstruction filter.
 And you have a high-pass analysis filter
 and a high-pass reconstruction filter.
 And the combination of all four together,
 they reconstruct the signal perfectly.
 But just half of them would not or so.
 So the low-pass doesn't need to be a perfect match--
 a perfect inversion.
 It's the combination of these two plus these two together,
 all four together, they, in the end,
 the combination of all four, that actually results
 in a perfect reconstruction.
 This is one example of that.
 And the nice thing about it is you
 can do that with very short, finite filters.
 You see here the frequency.
 The actual size of those filters is 3 and 5,
 in this example, for example.
 There's actually a mechanism.
 This happens to be derived by a Belgian guy.
 There is a mechanism to actually just automatically construct
 those filters.
 So if you construct the analysis filter this way,
 so you have signal coming in here and here,
 the even samples and the odd samples,
 really literally taking one out of two samples one way
 and the other the other way.
 And then you apply, essentially, kind of a multiplier here
 and some vice versa, what comes out of this,
 multiply it here, some there, et cetera, in a chain like this.
 And you get low band and high band coming out of this.
 That's your-- those essentially lead you
 to the two filters one way.
 And then if you exactly reverse everything,
 minuses and so on, you actually get a perfect inverse.
 So this allows you to build this biorthogonal perfect
 reconstruction.
 Biotogonality is-- but so it's not--
 it's biotogonality means the two together do kind
 of a perfect job.
 And so this way you can kind of design these things.
 So this is one example.
 And so by the way, this is from signal processing and so on.
 Z, Z minus 1, and so this corresponds to a delay.
 So Z is a symbol that actually says,
 take the next signal or the next sample in the row
 or the previous sample in the row.
 That's what this actually corresponds to.
 OK?
 So this is kind of briefly what these wavelets are.
 And there's a whole space and whole research field
 around wavelets, which we won't go into.
 But these wavelets are used in JPEG 2000,
 which is, well, the new old standard for wavelets.
 It turns out it's not used that much for a number of reasons.
 It is much better.
 If you look at it, it actually looks a lot
 like what we just saw for standard JPEG.
 So you take the color image.
 You can essentially cut it in tiles, et cetera.
 You certainly decompose it in the white component.
 This is essentially the grayscale component
 of the image, kind of the most important part.
 So this is a grayscale image.
 And then the kind of color components here.
 These are two color components.
 And then for each of those, you do a transformation.
 So from this image, you get to an image like this,
 where here you have the very small low resolution
 version of it, plus then all of the high pass
 kind of different high pass, low pass kind of combinations.
 So that's wavelet transform here.
 Then as before, quantization.
 So a quantization that also kind of works like this.
 You definitely keep this.
 And you keep explaining things as long as it's relevant.
 At some point, you can say, OK, I
 don't have more bits to spend.
 I'll stop representing anything beyond a certain point
 in a similar type of zigzag, a little bit zigzag pattern.
 So very similar to JPEG there.
 Then entropy coding.
 So zipping the remaining bits.
 You assemble the bit stream, because you
 have these color components.
 You have these things.
 All of this, you reassemble that in a bit stream.
 You send that out.
 And then, of course, decompression
 is just the reverse of that, inverting all of this
 and going all the way back to the image.
 You can tile.
 So you can-- really big images, you
 could cut in tiles to not process everything at once.
 So that you might be limited.
 But still, so this is a better compression.
 Actually, here's an example.
 Same size image.
 4 and 1/2 thousand bytes.
 Pretty poor reconstruction here.
 Much better image over here.
 So it can really make a difference.
 Here's another example.
 So both JPEG and JPEG 2000.
 So clearly, those two are much better
 than the equivalent over here.
 You see, in particular, this one is disastrous.
 Because essentially, you're left with,
 beyond the DC component, just the brightness of that patch.
 On average, there's no bits left to explain much.
 This is at essentially 1/8 of a bit per pixel for color.
 So 24.
 The raw image is 24 bits per pixel.
 And so yeah, so here it's really lost.
 Here it still works, kind of.
 With more bits per pixel, so a quarter of a bit per pixel,
 you start getting something reasonable.
 Not as good as JPEG 2000.
 But so if you look at this, this is clearly
 a lot more complicated and a lot harder
 to put in hardware efficiently and so on
 than the simple 4 by 4 blocks.
 And anyways, everybody already had built chips
 and has it in all the--
 everywhere, et cetera, has all of these IP blocks
 already in there that just do the right thing for JPEG
 and this and that.
 That there's a high hurdle to go to something else.
 And so yeah, so this is actually a much better standard.
 It's used in some places.
 It's not used that much in many practical settings.
 What we'll see next week is we'll look at two things.
 We'll start with optical flow on Tuesday.
 So optical flow is, as you kind of could see here a little bit,
 if you look at this, you kind of see visual patterns
 that move around.
 So optical flow is about determining the vectors,
 how things move around.
 So the motion determine how visual patterns move
 from here to here, et cetera.
 First, we look how to compute that,
 how we can determine that computationally and so on,
 mostly on Tuesday.
 And then we'll continue a bit with that on Thursday
 and then look at how we can use that together with actually
 what we saw in particular now with JPEG,
 how we can bring those things together to now also
 efficiently encode videos.
 Because if images were expensive to encode,
 videos are orders of magnitude, kind
 of two orders of magnitude more expensive to encode because you
 get kind of two orders of magnitude kind of images
 per second for making a video.
 So it's many orders of magnitude more
 if you capture videos than if you capture still pictures.
 So we'll see how we can encode.
 It's much more important there even to encode.
 And so we'll see how optical flow combined
 with then single image compression, the two together,
 allow us to get actually reasonably compressed images
 that still look-- videos that still look good.
 And I'm done.
 [APPLAUSE]
 [APPLAUSE]
