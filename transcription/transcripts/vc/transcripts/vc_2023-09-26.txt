
[00:00:00.000 --> 00:00:06.360]   Okay, good morning everyone. So today we'll talk mostly about segmentation.
[00:00:06.360 --> 00:00:11.480]   This week we'll talk about image segmentation. Really a problem of
[00:00:11.480 --> 00:00:16.320]   segmenting, for example, foreground from background. As you see in this example
[00:00:16.320 --> 00:00:22.680]   here, this optical illusion. Actually also interesting is you can actually look
[00:00:22.680 --> 00:00:28.880]   at these two ways, but you can kind of see both at the same time. Your brain will
[00:00:28.880 --> 00:00:35.480]   kind of switch from seeing a vase or seeing two faces looking at each other.
[00:00:35.480 --> 00:00:40.760]   You can kind of see both, but not at the same time. It's kind of interesting
[00:00:40.760 --> 00:00:47.280]   effect of how our brain is interpreting images. Before going there, we'll
[00:00:47.280 --> 00:00:53.480]   finish. So last lecture, I'll skip through this, so this last time, but maybe as a
[00:00:53.480 --> 00:01:01.800]   reminder, so we have different types of resolutions for images. How many pixels
[00:01:01.800 --> 00:01:12.800]   we have in this case covering the same amount of pixels per meter or per
[00:01:12.800 --> 00:01:17.240]   centimeter in the image on the real world, but just different than fields of
[00:01:17.240 --> 00:01:22.640]   view, different number of pixels. Here we have the same image, but sampled with the
[00:01:22.640 --> 00:01:32.480]   different number of samples of pixels on the image from 144 square to in the end
[00:01:32.480 --> 00:01:38.360]   just four square. We also looked at radiometric resolution, so this is not how
[00:01:38.360 --> 00:01:43.120]   many pixels we have in the image to represent a scene, but it's how many bits
[00:01:43.120 --> 00:01:50.400]   do we spend per pixel to express the property, the function value that we
[00:01:50.400 --> 00:01:58.800]   have measured. Going from here eight bits, so 256 values, 0 to 55, which is
[00:01:58.800 --> 00:02:05.360]   actually the standard that you will find, you know, for almost every image
[00:02:05.360 --> 00:02:12.000]   you'll work with. It will be this, but you can actually go down. You can kind of
[00:02:12.000 --> 00:02:20.000]   also notice that this screen doesn't really represent eight bits, doesn't have
[00:02:20.000 --> 00:02:25.520]   eight bits of contrast, in a sense you probably barely see the difference until
[00:02:25.520 --> 00:02:30.560]   we get, you know, roughly here, maybe a little bit there already, but it's probably
[00:02:30.560 --> 00:02:35.000]   not much more than 50 values that you can actually see with the limited
[00:02:35.000 --> 00:02:41.000]   contrast on the screen, given the environment light, given everything else.
[00:02:41.000 --> 00:02:47.400]   Typically on your laptop screen you'll see a bit more contrast actually.
[00:02:47.400 --> 00:02:57.640]   Okay, we had looked at this also. So I said so mostly you have eight bits, so two
[00:02:57.640 --> 00:03:04.360]   to the eight color images, it's for, you know, three channels, R, G and B. You can
[00:03:04.360 --> 00:03:11.400]   have, depending on the sensors, you could have more bits being spent. Also it can be
[00:03:11.400 --> 00:03:18.240]   non-linear. Many cameras will actually use a non-linear warping for
[00:03:18.240 --> 00:03:25.200]   representing things in most image formats, except if you actually, on some cameras,
[00:03:25.200 --> 00:03:30.680]   you will be able to have raw sensor access. Then it turns out the sensor
[00:03:30.680 --> 00:03:35.320]   technology itself is actually linear. At least not the event cameras, they
[00:03:35.320 --> 00:03:43.040]   actually have a logarithmic behavior, but the normal CMOS or CCD sensors are
[00:03:43.040 --> 00:03:48.640]   essentially linear in their behavior, at least over the range that they work.
[00:03:48.640 --> 00:03:54.840]   You can choose a certain exposure, you choose a certain gain. This is an
[00:03:54.840 --> 00:04:00.240]   amplification between how much electricity comes in and then you're,
[00:04:00.240 --> 00:04:10.720]   how much you, you know, you amplify that and then transform it to digital. But
[00:04:10.720 --> 00:04:15.240]   within that range, once you do that, it's actually a linear range. Intrinsically,
[00:04:15.240 --> 00:04:24.920]   it's a linear behavior, but it's often interesting, for example, here to do so
[00:04:24.920 --> 00:04:32.520]   called tone mapping, which is to somehow adjust the image. This image here has,
[00:04:32.520 --> 00:04:38.080]   you know, would have a huge contrast between, you know, dark regions where you
[00:04:38.080 --> 00:04:43.640]   still see some details here and then brightly lit regions. You can actually,
[00:04:43.640 --> 00:04:50.960]   there are techniques that we'll look at somehow preserving, compressing something
[00:04:50.960 --> 00:04:55.560]   that might need many orders of magnitude, so, you know, tens of bits or
[00:04:55.560 --> 00:05:02.920]   definitely way more than eight bits to compress that into preserving details in
[00:05:02.920 --> 00:05:06.600]   all the regions. But then you are actually talking about doing really
[00:05:06.600 --> 00:05:11.800]   nonlinear warps and not only a nonlinear warp that's common to all the pixels,
[00:05:11.800 --> 00:05:15.200]   but actually different warps in different parts of the image so that you
[00:05:15.200 --> 00:05:19.960]   actually do something different in this region that's very dark compared to
[00:05:19.960 --> 00:05:23.920]   regions that have a lot more light, for example. You will actually map them
[00:05:23.920 --> 00:05:30.520]   differently to grayscale tones. Okay. Not something we'll get in detail to, but
[00:05:30.520 --> 00:05:34.440]   something that's, you know, had quite some interesting graphics, for example, and so
[00:05:34.440 --> 00:05:39.920]   on. Nowadays also, not going to talk about that, but more and more TVs actually
[00:05:39.920 --> 00:05:46.240]   have high dynamic range capabilities. This means that instead of only being able
[00:05:46.240 --> 00:05:53.360]   to show maybe eight bits of contrast, they can show more. This typically works
[00:05:53.360 --> 00:06:02.040]   either by having, you know, actually, so a panel like my laptop here, the way it
[00:06:02.040 --> 00:06:07.760]   works is that it actually has a backlight, so there's a light behind the screen
[00:06:07.760 --> 00:06:12.080]   that illuminates everything, and then I have an LCD panel that will switch, that
[00:06:12.080 --> 00:06:18.080]   will kind of modulate how much light gets through. That has an intrinsically
[00:06:18.080 --> 00:06:25.760]   limited contrast. However, there are screens that will have not one common
[00:06:25.760 --> 00:06:30.720]   illumination behind, but will have more localized LEDs behind, so they can
[00:06:30.720 --> 00:06:34.800]   actually have one part of the screen be much brighter than the other part. That
[00:06:34.800 --> 00:06:41.640]   way you can get to high dynamic range, or you can have technology like OLED or
[00:06:41.640 --> 00:06:46.720]   micro LED or other technologies that can actually really have individual light
[00:06:46.720 --> 00:06:51.760]   sources locally per pixel, and then, you know, you have essentially, you can go to
[00:06:51.760 --> 00:06:57.160]   unlimited ranges of contrast, especially with LEDs. Essentially, you get a
[00:06:57.160 --> 00:07:00.640]   particular amount of light, typically by switching it on and off, and you can do
[00:07:00.640 --> 00:07:06.280]   that in the megahertz ranges, giving it all eyes, you know, integrate light over
[00:07:06.280 --> 00:07:12.280]   about 10 milliseconds. The range between 10 milliseconds and the megahertz or
[00:07:12.280 --> 00:07:16.480]   more range that you can actually switch those things on and off, you can
[00:07:16.480 --> 00:07:21.760]   actually create very large amounts of contrast on display technology nowadays.
[00:07:21.760 --> 00:07:27.920]   So you can have HDR TVs, you know, and screens, etc. Okay, but this projector is
[00:07:27.920 --> 00:07:34.560]   definitely not HDR, okay. You barely get 50, so that's like six bits of, six, seven
[00:07:34.560 --> 00:07:44.400]   bits of contrast on this. Okay, image noise. So, you know, images can have,
[00:07:44.400 --> 00:07:50.560]   ideally, you would just measure, you know, exactly the amount at a particular
[00:07:50.560 --> 00:07:57.000]   location in practice for multiple reasons. We've seen actually already earlier
[00:07:57.000 --> 00:08:02.240]   that there were some, sometimes there were some fluctuations on the measurements.
[00:08:02.240 --> 00:08:09.200]   There could be this dark current, for example, there can be other effects that
[00:08:09.200 --> 00:08:12.680]   can create that the actual value you measure is not exactly the one you want,
[00:08:12.680 --> 00:08:16.160]   you know, like the one that was actually physically there, but that there's some
[00:08:16.160 --> 00:08:21.120]   perturbations on your measurement, like actually any measurement device. Okay, so
[00:08:21.120 --> 00:08:26.080]   it will be important to somehow incorporate that and model that in a
[00:08:26.080 --> 00:08:31.600]   certain way. The typical way to model it is actually saying, well, every pixel will
[00:08:31.600 --> 00:08:36.200]   be perturbed, every measurement will be perturbed by some random perturbation,
[00:08:36.200 --> 00:08:44.120]   independent per pixel, zero mean and some distribution and we'll
[00:08:44.120 --> 00:08:48.760]   follow some kind of Gaussian distribution, for example. Okay, so average is zero, but
[00:08:48.760 --> 00:08:52.520]   it kind of, you know, spreads itself out. Small deviations are much more likely than
[00:08:52.520 --> 00:08:58.720]   large deviations and they fit within some kind of sigma of deviation. So that's a
[00:08:58.720 --> 00:09:04.240]   typical scenario is you would measure the actual value plus or minus some kind
[00:09:04.240 --> 00:09:07.800]   of Gaussian distribution that perturbs. So the noise is then modeled as a Gaussian,
[00:09:07.800 --> 00:09:12.840]   additive Gaussian noise. Additive means that you have the value plus minus, you
[00:09:12.840 --> 00:09:21.440]   know, the noise. In some cases, there are other models of noise that are more
[00:09:21.440 --> 00:09:29.960]   appropriate. For example, one challenge with just using additive Gaussian noise on
[00:09:29.960 --> 00:09:34.800]   potentially a very dark image or a situation where you have, you expect only
[00:09:34.800 --> 00:09:41.320]   very, very, very small measurements with some significant amount of noise, for
[00:09:41.320 --> 00:09:46.200]   example, is that you could actually get negative values, right? If you just add a
[00:09:46.200 --> 00:09:51.680]   symmetric function that's both positive and negative, so the Gaussian, then you
[00:09:51.680 --> 00:09:55.840]   could actually end with negative light, which doesn't exist. Yes?
[00:09:55.840 --> 00:09:59.560]   Why is there no square root in the Gaussian distribution?
[00:09:59.560 --> 00:10:03.840]   The pi is usually in a square root.
[00:10:03.840 --> 00:10:13.640]   No, it's, no, no, it's just divided. So it's divided by that, but no, there's no
[00:10:13.640 --> 00:10:27.400]   square root. The sigma, I mean, here is the, no, I mean, I don't think so. I mean, I
[00:10:27.400 --> 00:10:33.520]   need to check, but I don't think so. But maybe there's a typo. I haven't checked it.
[00:10:33.520 --> 00:10:43.200]   But anyway, so it's just a Gaussian formula, whatever. So sometimes, in some
[00:10:43.200 --> 00:10:51.800]   situations, you actually have, you want a non-symmetric noise. So you want a noise
[00:10:51.800 --> 00:10:59.920]   that would, for example, that only is positive, but cannot be negative. So for
[00:10:59.920 --> 00:11:06.480]   example, in those cases, a Poisson noise could be more relevant. So what, what
[00:11:06.480 --> 00:11:12.120]   scenario is that? For example, a scenario like this one, where you're taking an image
[00:11:12.120 --> 00:11:18.240]   at the limit of exposure time, so very limited exposure time, so that in the end,
[00:11:18.240 --> 00:11:22.920]   you really only get a few electrons. So the, the, the key thing is that, a few
[00:11:22.920 --> 00:11:28.440]   photons, the key thing is that in the end, you're looking at a stochastic process,
[00:11:28.440 --> 00:11:34.080]   and light is not a continuous flux at the most, you know, in very short time. It's
[00:11:34.080 --> 00:11:38.720]   actually a set of discrete particles, discrete photons that will actually hit
[00:11:38.720 --> 00:11:44.240]   your sensor, and those will transform in discrete electrons. Okay? That means that
[00:11:44.240 --> 00:11:49.120]   if you're in very dark light, you might or might not have, you know, captured one
[00:11:49.120 --> 00:11:54.600]   photon, or two, or maybe three, or something like this, but you're in this kind of
[00:11:54.600 --> 00:11:58.480]   ranges here, where you essentially have, if you put, if you want to model that
[00:11:58.480 --> 00:12:01.800]   distribution with a Gaussian, you'll actually also predict some negative
[00:12:01.800 --> 00:12:05.040]   photons, or you know, you would have a distribution that would also cover a
[00:12:05.040 --> 00:12:13.480]   negative amount of light. So, so in these cases, you would actually go to Poisson,
[00:12:13.480 --> 00:12:18.000]   which is kind of the Poisson distribution as you see once, so let me
[00:12:18.000 --> 00:12:23.720]   actually explain this distribution here. Lambda, in this distribution, so in this
[00:12:23.720 --> 00:12:36.600]   formula here, lambda represents the expected amount of times a rare event
[00:12:36.600 --> 00:12:42.840]   should happen. Okay? So for example, lambda equals one means that based on my
[00:12:42.840 --> 00:12:48.960]   expectation, I will, I should essentially, on average, for a particular
[00:12:48.960 --> 00:12:56.720]   duration of time, I should see, for example, I should get one photon. Okay? So it
[00:12:56.720 --> 00:13:01.840]   turns out that if you expect one photon, and the distribution follows a Poisson
[00:13:01.840 --> 00:13:11.040]   distribution, then if your expectation is one, the likelihood to have zero is
[00:13:11.040 --> 00:13:16.920]   about, you know, whatever, 37%, that you have no, no photons yet, although on
[00:13:16.920 --> 00:13:22.520]   average, you should get one. You have 37% chance to have no photon, about 37%
[00:13:22.520 --> 00:13:27.720]   chance to have exactly one photon, and then you have about, you know, a bit
[00:13:27.720 --> 00:13:34.680]   below 20% chance to have two photons, about, you know, let's say 6% chance to
[00:13:34.680 --> 00:13:38.960]   have already had three photons, although on average, you should only get one, and
[00:13:38.960 --> 00:13:42.480]   you see it goes down from there, maybe 1% chance to already have accumulated four
[00:13:42.480 --> 00:13:47.560]   photons, and you know, then it gets very negligible after that. Okay? So on average,
[00:13:47.560 --> 00:13:50.920]   I should get one, sometimes I will have zero, sometimes, you know, it's like if
[00:13:50.920 --> 00:13:55.520]   you play with dices, you know that you will not, you know, let's say get double
[00:13:55.520 --> 00:14:00.920]   six exactly every 36 times, right, that you throw the dices, sometimes you'll be
[00:14:00.920 --> 00:14:04.720]   lucky and you'll get a few more early on, sometimes it will take a lot longer
[00:14:04.720 --> 00:14:12.320]   before one shows up. Okay, same thing here. If you go to larger numbers, for
[00:14:12.320 --> 00:14:16.880]   example, you've sampled long enough or you waited long enough, you opened your
[00:14:16.880 --> 00:14:23.000]   exposure long enough, that you would have expected to see 10 photons, okay? Lambda
[00:14:23.000 --> 00:14:29.320]   equals 10, then you see that the distribution you get starts looking a
[00:14:29.320 --> 00:14:33.920]   lot like a Gaussian. Okay, so for large numbers, this just becomes a Gaussian
[00:14:33.920 --> 00:14:38.480]   distribution, okay? But for small numbers, it actually deviates very significantly
[00:14:38.480 --> 00:14:41.880]   from it, right? This is highly asymmetric, it doesn't go negative, so
[00:14:41.880 --> 00:14:45.360]   everything kind of gets here. And you see also here, it is kind of still
[00:14:45.360 --> 00:14:49.840]   asymmetric, it doesn't have the negative thing for expectation of four, for
[00:14:49.840 --> 00:14:54.720]   example, okay? Anyway, so in some regimes, that model up there, the additive
[00:14:54.720 --> 00:14:59.520]   Gaussian noise model is not appropriate. As you see here, these are actually
[00:14:59.520 --> 00:15:05.080]   examples of very low light conditions where, you know, you could get one or
[00:15:05.080 --> 00:15:10.920]   this or that, and so you see that you get very speckled kind of effects here. As
[00:15:10.920 --> 00:15:17.720]   you go further, you see that you start getting into more remote like this here,
[00:15:17.720 --> 00:15:22.160]   where most of the weight is, you know, even here you still have a big distribution,
[00:15:22.160 --> 00:15:28.640]   so a lot of noise. You'll be somewhere around here, right? So lambda equals one
[00:15:28.640 --> 00:15:36.400]   would be somewhere here or here. Sometimes it's on, sometimes it's off, you know. Let's
[00:15:36.400 --> 00:15:39.760]   say lambda equals 10, you have a big distribution, there's a lot of noise.
[00:15:39.760 --> 00:15:45.800]   It might vary, you know, practically between five and 15. That's something you
[00:15:45.800 --> 00:15:52.960]   see maybe here or here. As you go to, you know, you expect a hundred photons or,
[00:15:52.960 --> 00:15:58.400]   you know, a thousand photons, then you get an image here where this distribution
[00:15:58.400 --> 00:16:04.160]   gets a lot more peaked versus the range, okay? And then, you know, you kind of have
[00:16:04.160 --> 00:16:07.640]   a Gaussian, but also you'll have very little noise actually. The ratio of the
[00:16:07.640 --> 00:16:12.560]   variability, you know, instead of a thousand, you expect a thousand, for example,
[00:16:12.560 --> 00:16:16.560]   lambda equals thousand. You expect a thousand photons, okay, maybe you'll only
[00:16:16.560 --> 00:16:25.240]   have 997. Another time you'll have 1015, etc., but this would barely change one
[00:16:25.240 --> 00:16:32.040]   level of your image intensity, right? So, but so in these really low light
[00:16:32.040 --> 00:16:38.240]   conditions, you get very noisy imagery essentially. There are some other
[00:16:38.240 --> 00:16:45.640]   situations. We won't go into detail here, but for example, for MRI images, it turns
[00:16:45.640 --> 00:16:50.440]   out that this model here is more appropriate. So, if you do some special
[00:16:50.440 --> 00:16:54.200]   imaging techniques and so on, you might have to check, you know, what is the right
[00:16:54.200 --> 00:16:57.960]   noise model. In practice, we'll essentially always assume Gaussian noise.
[00:16:57.960 --> 00:17:07.960]   [inaudible]
[00:17:07.960 --> 00:17:14.440]   This is essentially simulated, so you can simulate this process of sampling, you
[00:17:14.440 --> 00:17:19.280]   know, with these different distributions. So, this is essentially, you have, you
[00:17:19.280 --> 00:17:24.000]   have your, you know, the function of like how much light comes in. So, you have, let's
[00:17:24.000 --> 00:17:28.920]   say, an image that's kind of good quality image, and then you can simulate how would
[00:17:28.920 --> 00:17:33.160]   it look like, and you could kind of easily run that, run this yourself. You can
[00:17:33.160 --> 00:17:38.600]   simulate how it would look like by sampling from one of those distributions
[00:17:38.600 --> 00:17:43.800]   here according to different exposures. So, this is, this is synthetic, these are
[00:17:43.800 --> 00:17:48.360]   synthetic images that are drawn from a, you know, and you could play with this
[00:17:48.360 --> 00:17:53.680]   yourself. They are drawn from a Poisson distribution, but they model quite well
[00:17:53.680 --> 00:17:58.400]   this process of essentially only having a few photons show up, so very low
[00:17:58.400 --> 00:18:07.720]   exposures can easily be modeled this way. Okay? There's also some situations where
[00:18:07.720 --> 00:18:14.320]   the noise can be multiplicative. Again, we won't get, we won't encounter those too
[00:18:14.320 --> 00:18:20.680]   much in, in what we do, but some situations, the noise could intrinsically be
[00:18:20.680 --> 00:18:26.160]   proportional to the signal. So, when it's a very low noise, low signal, there might
[00:18:26.160 --> 00:18:29.600]   be very little noise. When it's a big signal, there might be noise that
[00:18:29.600 --> 00:18:33.880]   proportional to it. That would be modeled like with a product, so then it's not
[00:18:33.880 --> 00:18:40.760]   additive, but multiplicative. There's of course quantization errors. Quantization
[00:18:40.760 --> 00:18:47.360]   errors, actually, anybody has an idea of what the distribution of a quantization
[00:18:47.360 --> 00:19:03.320]   error is? Would it be a Gaussian? So, let's say we have, you know, we, so we, we have
[00:19:03.320 --> 00:19:10.440]   quantization in eight, you know, eight bits, so zero to 255 values. What would the
[00:19:10.440 --> 00:19:21.360]   quantization error look like? Anybody has an idea? So, so what is going to happen?
[00:19:21.360 --> 00:19:28.360]   So, you should think of, I, you know, my function could have an arbitrary value, and
[00:19:28.360 --> 00:19:33.200]   let's ignore the saturation, so zero or 255, but I could have an arbitrary value,
[00:19:33.200 --> 00:19:37.800]   and then quantization, what do we do? We will just round it to the closest value.
[00:19:37.800 --> 00:19:44.000]   Okay, so if my initial, let's assume for simplicity that my initial distribution
[00:19:44.000 --> 00:19:50.160]   would be uniform, and now I will essentially do quantization, so after
[00:19:50.160 --> 00:19:55.720]   quantization I just get, you know, one, two, I don't get wrong, I don't get, you
[00:19:55.720 --> 00:20:00.840]   know, anything behind the comma anymore, so I just round it to the closest integer.
[00:20:00.840 --> 00:20:12.120]   So, what could it be? Exactly, right, so you would essentially have this kind of
[00:20:12.120 --> 00:20:15.840]   distribution that, because the rounding is always just within the range of plus or
[00:20:15.840 --> 00:20:19.320]   minus 0.5, that's exactly the distribution you'll get, this kind of a flat
[00:20:19.320 --> 00:20:24.000]   distribution plus or minus a half, okay? So that's what this quantization error
[00:20:24.000 --> 00:20:29.040]   would actually correspond to in terms of distribution, so not a Gaussian, just a
[00:20:29.040 --> 00:20:34.520]   little square, essentially, right, rectangle. Some situations you can also have
[00:20:34.520 --> 00:20:40.160]   salt and pepper noise, you know, so salt and pepper essentially means that on the
[00:20:40.160 --> 00:20:44.280]   image, like you see here, on the image you could have part of the image, some
[00:20:44.280 --> 00:20:49.800]   pixels could for whatever reason just be completely saturated, or vice versa, some
[00:20:49.800 --> 00:20:54.040]   could just be completely black, so something goes wrong and you, you know,
[00:20:54.040 --> 00:21:00.720]   like some effects, for example, could be some pixels have problems and they kind
[00:21:00.720 --> 00:21:04.640]   of flip one way, you know, they're either fully, they don't respond at all or they
[00:21:04.640 --> 00:21:08.360]   saturate right away because something's wrong with them, you can also have
[00:21:08.360 --> 00:21:11.920]   sometimes effects where there's some particles in the, in the air or something
[00:21:11.920 --> 00:21:15.360]   that reflect a lot of light and so suddenly you have, you're trying to take
[00:21:15.360 --> 00:21:19.080]   an image but there's some kind of points that kind of reflect much more light and
[00:21:19.080 --> 00:21:24.840]   so they kind of locally blind the sensor, things like this, so there are
[00:21:24.840 --> 00:21:30.240]   number of situations where this kind of saturated either fully black or fully
[00:21:30.240 --> 00:21:39.720]   white pixels can show up. Then something important is how do we characterize how
[00:21:39.720 --> 00:21:44.800]   much noise there is in the image, okay, as a very simple thing, we'll essentially
[00:21:44.800 --> 00:21:49.400]   describe the signal to noise ratio, so we'll compare the size of the signal
[00:21:49.400 --> 00:21:53.160]   versus the size of the noise and hopefully, you know, there's a lot more
[00:21:53.160 --> 00:21:59.160]   signal than noise, of course, so the, so, and this is a measure, it's an index of
[00:21:59.160 --> 00:22:03.320]   image quality essentially, right, if there's very little noise compared to the
[00:22:03.320 --> 00:22:08.320]   image, the signal in the image, then we have a high quality image, if there's a
[00:22:08.320 --> 00:22:13.840]   lot of noise compared to the actual image signal, it would be pretty bad, right,
[00:22:13.840 --> 00:22:17.600]   so if you look at this here, for example, here clearly there's a lot of noise
[00:22:17.600 --> 00:22:29.320]   compared to the signal, in this case, the ratio is much better. So, so it's signal
[00:22:29.320 --> 00:22:36.920]   divided by noise, when we have a Gaussian distribution, this would be this, the
[00:22:36.920 --> 00:22:45.920]   sigma from the Gaussian distribution, f would be essentially the normalized
[00:22:45.920 --> 00:22:50.520]   amount of signal, so this is the image values that you read off divided by how
[00:22:50.520 --> 00:22:56.720]   many, you know, the size of the image, so it's the average signal strength, okay.
[00:22:56.720 --> 00:23:03.600]   In practice, people instead of using the SNR, use the PSNR, the peak signal to
[00:23:03.600 --> 00:23:11.360]   noise ratio, why is that? It's because, let's say you do video compression or
[00:23:11.360 --> 00:23:16.920]   something like this, let's say somehow you would compress so that you would, you
[00:23:16.920 --> 00:23:23.520]   know, on average, you know, encode things so that you maybe round plus minus five
[00:23:23.520 --> 00:23:28.440]   values, let's say, after compression that you reconstruct the image with within
[00:23:28.440 --> 00:23:32.880]   like plus or minus five, but then suddenly you have part of the sequence is very dark,
[00:23:32.880 --> 00:23:38.680]   you don't see much anyways, but it's very dark, if you do signal to noise ratio
[00:23:38.680 --> 00:23:42.920]   and you have a dark image, then there's almost no signal anymore and then your
[00:23:42.920 --> 00:23:47.160]   noise would suddenly look very big in proportion, okay. So typically people will
[00:23:47.160 --> 00:23:50.480]   say, well, I can represent my image between, I can represent all the values
[00:23:50.480 --> 00:23:55.680]   between 0 and 255, so I will actually use 255, the maximum signal that I can
[00:23:55.680 --> 00:24:00.960]   represent as this, you know, representative of my signal, where my signal
[00:24:00.960 --> 00:24:05.160]   lives between 0 and 255 and I will compare that to the amount of noise, so
[00:24:05.160 --> 00:24:10.520]   you would typically take the maximum signal, so typically 255 and compare that
[00:24:10.520 --> 00:24:15.560]   to the amount of noise, okay. That's the peak signal to noise ratio, that's
[00:24:15.560 --> 00:24:19.200]   typically we'll come back to that when we talk about video compression in the
[00:24:19.200 --> 00:24:27.640]   sixth week of the semester, okay. Okay, recap of all of this, pixels are, you know,
[00:24:27.640 --> 00:24:33.400]   point measurements of the function value that we want to represent in
[00:24:33.400 --> 00:24:41.600]   discretized image. We looked at different types of resolution, we just
[00:24:41.600 --> 00:24:47.600]   covered image noise, so I skipped that, so now let's look very briefly at the
[00:24:47.600 --> 00:24:53.520]   human eye also. These are some, that's a picture of the human eye, this is more of
[00:24:53.520 --> 00:25:01.000]   a model of it, but what's important is essentially, let me get the cursor up here
[00:25:01.000 --> 00:25:06.040]   somewhere, okay, so this is the eye, the light comes in this way here, you have the
[00:25:06.040 --> 00:25:12.200]   cornea, you have the, here you have a lens, then you have the iris, you see the
[00:25:12.200 --> 00:25:17.440]   iris actually partially covers the lens, okay, so it can actually, this can
[00:25:17.440 --> 00:25:22.440]   actually grow or shrink, so you can actually let, you know, use a bigger part
[00:25:22.440 --> 00:25:29.120]   or a smaller part of your eye, or of the lens to capture more or less light, so
[00:25:29.120 --> 00:25:35.400]   essentially what happens there is that if you're, if you're at night in the dark,
[00:25:35.400 --> 00:25:40.480]   you know, you wake up, you go to bathroom, something like that, your pupil will
[00:25:40.480 --> 00:25:45.840]   kind of delay as much as possible, so essentially this iris will a little bit
[00:25:45.840 --> 00:25:49.840]   shrink, will kind of open up so that you get more light in so that you can still
[00:25:49.840 --> 00:25:53.640]   see something, but then if you go outside, or let's say also you're inside, and then
[00:25:53.640 --> 00:25:58.960]   you go outside, you know, your pupil will kind of shrink so that you get less
[00:25:58.960 --> 00:26:03.520]   light and you're not blinded by the sunlight outside, so we have some way to
[00:26:03.520 --> 00:26:08.200]   adjust to the amount of light that way, literally how much light actually gets
[00:26:08.200 --> 00:26:15.080]   inside the eye, the eye also has more of a nonlinear response, a logarithmic
[00:26:15.080 --> 00:26:25.760]   response to light, the sensitive area is back here, and then essentially it varies,
[00:26:25.760 --> 00:26:33.080]   but most, you're essentially most, you can see most details at the fovea, this is an
[00:26:33.080 --> 00:26:37.800]   area of about two degrees of the field, visual field of view, so there you can
[00:26:37.800 --> 00:26:42.680]   actually read and see high resolution, in the rest of the retina you see a lot
[00:26:42.680 --> 00:26:49.680]   less details, and actually at this point here, where the visual nerves kind of go
[00:26:49.680 --> 00:26:54.240]   out, you even have a blind spot, so there's a small part where you don't see
[00:26:54.240 --> 00:26:58.680]   anything, but you won't actually realize that, it's just that as you look around,
[00:26:58.680 --> 00:27:02.960]   you kind of build essentially a kind of, in your head, a kind of panoramic view of
[00:27:02.960 --> 00:27:08.320]   what's around you, as your eyes kind of also has saccades and moves around, but
[00:27:08.320 --> 00:27:12.240]   there's actually one area where if there would be exactly something happening
[00:27:12.240 --> 00:27:16.080]   in that blind spot, you wouldn't actually see it, but as your eye moves around all
[00:27:16.080 --> 00:27:20.280]   the time, you would actually tend to pick it up, you know, at some point anyways.
[00:27:20.280 --> 00:27:25.920]   You can actually also, you see there's muscles here, you can actually pull on
[00:27:25.920 --> 00:27:31.400]   the lens, this works better when you're young than when you're a bit older, so
[00:27:31.400 --> 00:27:35.960]   you can actually adjust the focus, so I can, you know, when I have my hand here,
[00:27:35.960 --> 00:27:40.680]   I can try to focus on my hand, and then everything behind looks blurred, and I
[00:27:40.680 --> 00:27:44.920]   can see details on my hand, if I now look at, you know, at you guys, then
[00:27:44.920 --> 00:27:48.400]   suddenly my hand is completely blurred here, that's because my lens will actually
[00:27:48.400 --> 00:27:52.720]   change shape, and all of your lenses will change shape as you focus at different
[00:27:52.720 --> 00:27:58.360]   distances, so that's that's what these muscles are, so it's actually a quite
[00:27:58.360 --> 00:28:02.360]   complicated system, and of course the whole eye can rotate around with also
[00:28:02.360 --> 00:28:08.400]   muscles pulling on the eye to look at different directions and focus, okay.
[00:28:08.400 --> 00:28:18.520]   Then also in terms of the light, we're not just sensing like, you know, in the whole,
[00:28:18.520 --> 00:28:26.400]   you know, in the whole spectrum of light, we actually only sensitive to a very
[00:28:26.400 --> 00:28:32.960]   small part here of the spectrum, you know, you have radio waves,
[00:28:32.960 --> 00:28:38.520]   microwave, infrared, the visible spectrum, ultraviolet, x-rays, gamma rays, etc.,
[00:28:38.520 --> 00:28:43.480]   right, so we only, and this exponential by the way, like this, or like
[00:28:43.480 --> 00:28:48.320]   logarithmic scale essentially, so we're just sensitive here, now this is also the
[00:28:48.320 --> 00:28:51.520]   part of the light of the sun that actually gets through the atmosphere, so
[00:28:51.520 --> 00:28:55.480]   it's not by, you know, I mean there's a bit more, of course, there's some
[00:28:55.480 --> 00:28:59.680]   infrared, there's some ultraviolet, but this is a particularly useful part of the
[00:28:59.680 --> 00:29:08.760]   spectrum, it goes from red to, you know, violet essentially, over green in
[00:29:08.760 --> 00:29:17.760]   particular, this is how we perceive those colors, those different wavelengths, we
[00:29:17.760 --> 00:29:24.040]   actually don't sample this spectrum exhaustively, we really have essentially
[00:29:24.040 --> 00:29:31.880]   here these three kind of, you know, these different sensors that are
[00:29:31.880 --> 00:29:38.120]   essentially sensitive to slightly different, you know, frequencies of the
[00:29:38.120 --> 00:29:46.720]   light, so essentially, you know, we have blue cones, green cones and red cones that
[00:29:46.720 --> 00:29:52.560]   are essentially more sensitive to red light or to blue light or to green light
[00:29:52.560 --> 00:30:01.160]   and so if we get light coming to us, those different cones will
[00:30:01.160 --> 00:30:05.400]   respond differently based on the color of the light, the frequency of the light
[00:30:05.400 --> 00:30:12.560]   and we'll associate to that a particular color, okay, but what's important and
[00:30:12.560 --> 00:30:17.520]   you'll see that more in detail when you get to colors with markers, the key
[00:30:17.520 --> 00:30:20.440]   thing is that we don't really see every possible color, we can only really
[00:30:20.440 --> 00:30:25.080]   distinguish three different types of color and therefore you can have
[00:30:25.080 --> 00:30:31.600]   different spectra that will actually look the same to us, okay, this is called
[00:30:31.600 --> 00:30:36.720]   metamerism actually, so we might notice the difference between some colors and
[00:30:36.720 --> 00:30:41.120]   others, also people might have slightly shifted sensitivities, meaning that some
[00:30:41.120 --> 00:30:44.920]   people might see difference between two spectra that another person doesn't see
[00:30:44.920 --> 00:30:49.840]   the difference between, because of slightly different sensitivities and so on.
[00:30:49.840 --> 00:30:57.920]   Also the distributions vary a lot here, so you see the fovea here, so lots of cones
[00:30:57.920 --> 00:31:09.360]   here and then, you know, much more of the rods in the periphery and also this blind
[00:31:09.360 --> 00:31:13.520]   spot here, so also an area where you don't see anything, this is not, this is just
[00:31:13.520 --> 00:31:21.200]   one little area in the two-dimensional visual field and also the rods and the
[00:31:21.200 --> 00:31:28.880]   cones have different temporal responses also in a sense, so the rods are
[00:31:28.880 --> 00:31:33.280]   actually faster in their response and sensitive also to less light, so you need
[00:31:33.280 --> 00:31:40.040]   less light to activate them, but in terms of resolution you need the cones
[00:31:40.040 --> 00:31:44.280]   and for color you need the cones, that's also why at night you don't see colors,
[00:31:44.280 --> 00:31:52.880]   it's because then you're actually really using more, so the rods are more
[00:31:52.880 --> 00:31:56.400]   light sensitive, but then you actually can't distinguish colors anymore, so
[00:31:56.400 --> 00:32:01.160]   that's why at night everything looks kind of gray, okay, if you're interested in
[00:32:01.160 --> 00:32:05.680]   this type of things, this is an interesting book where you can see that in
[00:32:05.680 --> 00:32:10.320]   nature there's many many different principles, so we have one principle
[00:32:10.320 --> 00:32:15.760]   for our eye and you know mammals and so on all have the same roughly the same
[00:32:15.760 --> 00:32:20.440]   principle, but different animals can have actually very different mechanisms of
[00:32:20.440 --> 00:32:27.280]   actually perceiving the world from you know compound eyes like insects here
[00:32:27.280 --> 00:32:38.200]   and so on, with or without lenses you know etc etc etc, okay then you know now
[00:32:38.200 --> 00:32:43.400]   back to how do we represent things, well really a color image is really an R, a G
[00:32:43.400 --> 00:32:48.320]   and a B image that you superimpose and on the display you know you would
[00:32:48.320 --> 00:32:52.520]   essentially make that one you know that's be red, green and blue and then mix it
[00:32:52.520 --> 00:32:55.880]   together in an image like this here, you can really see them as independent
[00:32:55.880 --> 00:33:01.760]   images and if you look here you can kind of see that you know the skin here for
[00:33:01.760 --> 00:33:07.680]   example has very little blue in it, not too much green and a lot of red for
[00:33:07.680 --> 00:33:11.120]   example in the skin right, you can kind of see that you see that when it's white
[00:33:11.120 --> 00:33:16.840]   or light gray it's in all the channels, you can see that the green stuff here is
[00:33:16.840 --> 00:33:22.080]   particularly bright over here and kind of dark in the blue and the red etc etc,
[00:33:22.080 --> 00:33:35.040]   okay so how do we perceive color, a few different ways, different mechanisms, so
[00:33:35.040 --> 00:33:42.120]   here's the first one, you can actually use if you want high quality and very
[00:33:42.120 --> 00:33:48.200]   good light sensitivity, you can actually use prisms, there's a combination of
[00:33:48.200 --> 00:33:54.400]   typically two prisms that you know you can, you have light coming in and
[00:33:54.400 --> 00:34:00.000]   essentially based on the angle you know if you have a prism it will separate out
[00:34:00.000 --> 00:34:05.800]   it will kind of bend light differently based on the frequency and in
[00:34:05.800 --> 00:34:12.880]   particular some light passes certain angle, some light might be almost totally
[00:34:12.880 --> 00:34:17.720]   reflected while past you know like on the other side of that angle it might
[00:34:17.720 --> 00:34:24.000]   mostly go through, okay and so different frequencies behave differently and so
[00:34:24.000 --> 00:34:31.400]   essentially if you choose this, the shape of these prisms and also of course the
[00:34:31.400 --> 00:34:39.840]   n, the optical density of those prisms, you choose all of these exactly right,
[00:34:39.840 --> 00:34:47.040]   then you can have all the light, all the red light for example here that comes
[00:34:47.040 --> 00:34:52.840]   through, all of that light will kind of you know more like essentially most of
[00:34:52.840 --> 00:34:56.920]   it will bounce and then here again for internal reflection will essentially land
[00:34:56.920 --> 00:35:00.720]   on a sensor over here that will essentially get all of the red light
[00:35:00.720 --> 00:35:06.200]   will just land on that sensor and then the blue one, the blue light the other
[00:35:06.200 --> 00:35:10.440]   side of the spectrum kind of goes through this one but then reflects on this
[00:35:10.440 --> 00:35:16.160]   one for example and internally reflects here and then ends up on that sensor and
[00:35:16.160 --> 00:35:21.760]   the green light, the light not exactly one frequency right but the part in the
[00:35:21.760 --> 00:35:25.800]   middle of the spectrum which kind of all goes straight through essentially or
[00:35:25.800 --> 00:35:33.440]   most of it, I think I have a video I hope it's okay it doesn't play anymore, so
[00:35:33.440 --> 00:35:36.160]   there was a video where you actually you know you can look at a prism and you can
[00:35:36.160 --> 00:35:41.240]   kind of see how different colors of light will will bend to different sides, the
[00:35:41.240 --> 00:35:46.040]   so the key is the advantage of this mechanism is that you're not throwing a
[00:35:46.040 --> 00:35:51.000]   way light so this can actually if you really want good quality and good
[00:35:51.000 --> 00:35:54.480]   light sensitivity, this is a great mechanism of course you can see that
[00:35:54.480 --> 00:35:59.000]   intrinsically this is like you know physical block of you know two prisms
[00:35:59.000 --> 00:36:03.440]   glued together etc etc you need three sensors so this is clearly going to be
[00:36:03.440 --> 00:36:09.360]   expensive but in the early days there were a lot of video cameras that were
[00:36:09.360 --> 00:36:14.840]   actually three CCD or three CMOS cameras those were essentially you know using
[00:36:14.840 --> 00:36:20.760]   this type of prism inside to get the most light sensitivity possible. The
[00:36:20.760 --> 00:36:25.000]   standard mechanism what you also find in your phone etc is actually a bare
[00:36:25.000 --> 00:36:32.120]   pattern and so so this bare filter so what's the mechanism here well first
[00:36:32.120 --> 00:36:36.880]   you're already sacrificing a little bit of resolution because you're essentially
[00:36:36.880 --> 00:36:40.280]   going to say well this pixel I sense blue color and then this one green color
[00:36:40.280 --> 00:36:50.120]   and this one red color etc and it's going to essentially filter away all of the
[00:36:50.120 --> 00:36:56.360]   light that's not the right color okay so in a very simple simplified situation
[00:36:56.360 --> 00:37:00.800]   what are we doing we essentially throwing away at every location we're
[00:37:00.800 --> 00:37:05.600]   throwing two-thirds of the light away to just keep the light that we actually
[00:37:05.600 --> 00:37:09.520]   want to measure the of the right color okay so you see that intrinsically in
[00:37:09.520 --> 00:37:13.680]   terms of light sensitivity we're three times worse you know in a simplified
[00:37:13.680 --> 00:37:17.560]   setting we're three times worse than this setting here because here we use all
[00:37:17.560 --> 00:37:21.920]   the light well now we need three sensors to do it but we kind of measured all the
[00:37:21.920 --> 00:37:25.720]   light that was coming in where in this case we actually at every location we're
[00:37:25.720 --> 00:37:31.040]   going to filter away most of the light and only let a particular you know limited
[00:37:31.040 --> 00:37:36.360]   spectrum light come through that's a factor of three right there kind of it's
[00:37:36.360 --> 00:37:41.040]   actually even a bit worse because the just the fact that you put a filter there
[00:37:41.040 --> 00:37:45.880]   and so on it absorbs also some of the green red or blue light itself even if
[00:37:45.880 --> 00:37:53.280]   it preserves more of it additional challenge for those things here is let's
[00:37:53.280 --> 00:37:59.320]   say you have a nice black and white higher contrast kind of image you know
[00:37:59.320 --> 00:38:05.600]   let's say you take a picture of this corner here right with your camera like
[00:38:05.600 --> 00:38:11.080]   this RGB bear pattern then strictly speaking this is what your image will
[00:38:11.080 --> 00:38:17.160]   look like okay so what what happens here well I mean in with a simplified
[00:38:17.160 --> 00:38:22.160]   mechanism right simplified mechanism that would for example for the red pixel
[00:38:22.160 --> 00:38:27.000]   here would say well I don't have a green or blue measurement here but hey let me
[00:38:27.000 --> 00:38:34.120]   just take the average of those four green values and place it here and same
[00:38:34.120 --> 00:38:39.160]   average of the blue values and what do you see what you see that this one if the
[00:38:39.160 --> 00:38:44.320]   edge is I'm not sure exactly if the edge was supposed to be here or here I think
[00:38:44.320 --> 00:38:50.760]   it's here anyways this would have red fully sorry it's black so no red because
[00:38:50.760 --> 00:38:59.640]   it's in the dark side so no red it would have you know one green and one blue if
[00:38:59.640 --> 00:39:06.040]   you look at this one it would have you know one green but it would already have
[00:39:06.040 --> 00:39:11.280]   two blues okay and it would have no red etc and so that's why you see
[00:39:11.280 --> 00:39:16.360]   essentially these strange color patterns appear here if you do a simple you know
[00:39:16.360 --> 00:39:21.440]   simple averaging of the neighbors to get the local value and so on you will get
[00:39:21.440 --> 00:39:25.200]   these type of strange artifacts okay so you get essentially some kind of color
[00:39:25.200 --> 00:39:31.280]   aliasing going on because of the discrepancy between the resolution you
[00:39:31.280 --> 00:39:36.000]   try to rebuild the image at and and the rate at which you sample the colors okay
[00:39:36.000 --> 00:39:40.160]   so there's additional challenges now of course there's now modern algorithms we'll
[00:39:40.160 --> 00:39:43.520]   try to use some machine learning and other things some some more advanced
[00:39:43.520 --> 00:39:48.520]   algorithms that will try to you know fix this and recognize hey this is like a
[00:39:48.520 --> 00:39:52.640]   black and white pattern most likely based on what I see here it's most likely
[00:39:52.640 --> 00:39:56.720]   a pattern and so I will kind of restore an image that is more likely than you know
[00:39:56.720 --> 00:40:07.800]   this kind of strange color patterns yes first there very good question let's
[00:40:07.800 --> 00:40:15.720]   actually go here so if you look here at the color sensitivity all up you see
[00:40:15.720 --> 00:40:20.760]   that in the green range if you look at the kind of how all the curves behave
[00:40:20.760 --> 00:40:25.320]   this is where we are most light sensitive you know the green is has sees
[00:40:25.320 --> 00:40:29.880]   this of course very well but also the red is very much overlapping there and
[00:40:29.880 --> 00:40:34.160]   also the rods also have a strong response there so we actually more
[00:40:34.160 --> 00:40:41.120]   sensitive to green we perceive the green light better and so that's the reason to
[00:40:41.120 --> 00:40:46.200]   actually if you have four and you only have three colors to make two greens for
[00:40:46.200 --> 00:41:02.000]   that reason you also had a question here yeah it it I as far as I know it mostly
[00:41:02.000 --> 00:41:11.760]   heats it up but you know that's negligible okay here's an example in
[00:41:11.760 --> 00:41:16.560]   kind of reality you know this this part of the image here you kind of see these
[00:41:16.560 --> 00:41:25.320]   effects of red and you know of bluish and reddish kind of responses here people
[00:41:25.320 --> 00:41:30.280]   have also experimented with adding additional colors so not to have you
[00:41:30.280 --> 00:41:33.280]   know have two greens but then figured out hey maybe I could add something else
[00:41:33.280 --> 00:41:38.400]   there's also some that have you know would actually have a clear no filter at
[00:41:38.400 --> 00:41:43.520]   all here's the sign a sign filter you could have no filter at all people have
[00:41:43.520 --> 00:41:47.200]   experiment you know played around with this but mostly mostly people use the
[00:41:47.200 --> 00:41:52.000]   classical bear pattern that's what you find mostly if you then want even more
[00:41:52.000 --> 00:41:55.840]   flexibility to measure color you know this is a very flexible mechanism more
[00:41:55.840 --> 00:42:01.080]   used in scientific apparatus and so on where you would actually have a sensor
[00:42:01.080 --> 00:42:06.120]   high-quality sensor there's also more settings where you have a static object
[00:42:06.120 --> 00:42:10.360]   or static scene that you want to take pictures of and then you can actually
[00:42:10.360 --> 00:42:15.200]   just rotate different filters through you could install new filters with
[00:42:15.200 --> 00:42:21.720]   exactly the properties you want to measure things here alright so here's
[00:42:21.720 --> 00:42:25.680]   kind of a comparison a little bit of the different advantages as I said this
[00:42:25.680 --> 00:42:30.680]   really for more scientific applications niche essentially nowadays pretty much
[00:42:30.680 --> 00:42:34.000]   everything is this way because you can digitally correct a lot of things and
[00:42:34.000 --> 00:42:38.880]   isn't that and so this is the simplest and cheapest to manufacture in some high
[00:42:38.880 --> 00:42:46.240]   end camera systems you will still find this here and then there was this there's
[00:42:46.240 --> 00:42:53.680]   this very interesting technology that's actually stacking pixels vertically on
[00:42:53.680 --> 00:43:01.600]   top of each other based on the fact that if you look at the full spectrum in
[00:43:01.600 --> 00:43:06.280]   terms of absorption in the material or penetration of photons in the material it
[00:43:06.280 --> 00:43:09.400]   turns out that based on the amount of energy of the photons they will
[00:43:09.400 --> 00:43:14.360]   penetrate on average at different depths in the material and so they figure hey
[00:43:14.360 --> 00:43:19.200]   if if this is the surface if inside the surface we measure
[00:43:19.200 --> 00:43:24.320]   differentially at different locations in depth we can actually perceive color a
[00:43:24.320 --> 00:43:30.400]   little bit that way because you know you see essentially red red will go on
[00:43:30.400 --> 00:43:34.960]   average deeper it will also you know sometimes so this this essentially means
[00:43:34.960 --> 00:43:42.120]   that in the chip let's say in the CMOS chip if this is the surface here red will
[00:43:42.120 --> 00:43:46.800]   kind of also mostly be absorbed here but if you actually have red light you will
[00:43:46.800 --> 00:43:50.520]   see penetration you will still measure something here where if you have blue
[00:43:50.520 --> 00:43:54.640]   light you pretty much will measure nothing here and so by measuring
[00:43:54.640 --> 00:43:58.360]   separately here here and here you can then run algorithms that will reconstruct
[00:43:58.360 --> 00:44:03.640]   like how much red green and blue was in the incoming light at this location so
[00:44:03.640 --> 00:44:06.800]   you don't need a bear pattern anymore you can at one location measure all three
[00:44:06.800 --> 00:44:11.720]   colors by having three different depths of sensing and then you know some
[00:44:11.720 --> 00:44:17.560]   algorithmic stuff on top of that these avoid this kind of color aliasing in
[00:44:17.560 --> 00:44:24.440]   situations where you have essentially color patterns or you have patterns that
[00:44:24.440 --> 00:44:30.640]   are very close to the macrists frequency that we discussed last week this kind of
[00:44:30.640 --> 00:44:35.120]   the limit of where you can sample and when you have kind of effects of kind of
[00:44:35.120 --> 00:44:42.040]   because we sample color in a sense with the bear pattern we sample color at half
[00:44:42.040 --> 00:44:49.200]   the resolution of sampling the light essentially we can have when we really
[00:44:49.200 --> 00:44:53.960]   operating at the limit in terms of texture details this can this can create
[00:44:53.960 --> 00:45:01.640]   funny effects okay so we'll take a break now and then we'll continue with
[00:45:01.640 --> 00:45:11.960]   segmentation okay so now let's continue with segmentation so there's many
[00:45:11.960 --> 00:45:18.320]   different concepts that can help us in segmentation it is often task-specific
[00:45:18.320 --> 00:45:24.520]   it depends on the context what belongs together one of the you know key
[00:45:24.520 --> 00:45:29.680]   principles and this actually comes from psychology so based on human vision
[00:45:29.680 --> 00:45:33.120]   essentially and trying to understand how people tend to group things together and
[00:45:33.120 --> 00:45:40.080]   so on these are the Gestalt phenomena so kind of figure ground segmentation
[00:45:40.080 --> 00:45:46.040]   proximity so if you look in B here you will visually kind of naturally tend to
[00:45:46.040 --> 00:45:51.560]   you know separate out the ones that are a little bit further etc so you see also
[00:45:51.560 --> 00:45:57.120]   similarity here you would tend to have the you know the the disks and the
[00:45:57.120 --> 00:46:04.160]   crosses kind of as see them as two separate groups continuation in D here
[00:46:04.160 --> 00:46:08.960]   you see essentially we tend to group the ones that are along kind of a same path
[00:46:08.960 --> 00:46:16.280]   together and and same here so but you know like this one is not necessarily
[00:46:16.280 --> 00:46:19.440]   associated with this one in a sense like we would tend to associate those and
[00:46:19.440 --> 00:46:23.920]   those guys this one overlaps in both but but we have kind of this continuity also
[00:46:23.920 --> 00:46:31.200]   that we imagine and then we associate also based on that closure in a sense
[00:46:31.200 --> 00:46:36.080]   here you can kind of more or less associate also the in kind of separate
[00:46:36.080 --> 00:46:44.480]   insight from the outside here come on fate if things move if this would
[00:46:44.480 --> 00:46:48.200]   illustrate motion the ones that move in the same direction could be perceived
[00:46:48.200 --> 00:46:51.840]   to be part of the same group where ones that would move in the opposite
[00:46:51.840 --> 00:46:56.040]   direction would immediately stand out and be kind of visually seen as separate
[00:46:56.040 --> 00:47:02.800]   ones symmetries these two guys look a lot more associated with each other than
[00:47:02.800 --> 00:47:12.040]   those two for example etc. here's kind of again a kind of an interesting illusion
[00:47:12.040 --> 00:47:23.360]   you can kind of again see two figures here who doesn't see two figures okay
[00:47:23.360 --> 00:47:29.280]   which which one do you see like for one of the so there is essentially you know
[00:47:29.280 --> 00:47:34.200]   the nose could either be over here or the nose could be over here so this could
[00:47:34.200 --> 00:47:47.920]   be the nose the mouth etc. okay still someone doesn't see it or everybody
[00:47:47.920 --> 00:47:53.880]   see but notice again with your brain you can see one or you can see the other you
[00:47:53.880 --> 00:47:57.880]   can flip from one to the other you can never kind of perceive both at the same
[00:47:57.880 --> 00:48:01.520]   time so your brain does kind of a holistic interpretation of the whole
[00:48:01.520 --> 00:48:05.760]   image and needs a kind of consistent interpretation so it will really look
[00:48:05.760 --> 00:48:12.560]   either it's either seeing one and kind of you know like has that picture or or
[00:48:12.560 --> 00:48:22.840]   sees the other but so in some ways you know you can you can say this here
[00:48:22.840 --> 00:48:28.080]   essentially segmentation ultimate classification problem if you solve this
[00:48:28.080 --> 00:48:32.680]   then all problems in computer vision if you could just have a perfect solution
[00:48:32.680 --> 00:48:36.160]   to every segmentation problem then essentially all computer vision is kind
[00:48:36.160 --> 00:48:42.720]   of solved this really a fundamental problem but it's also very task-specific
[00:48:42.720 --> 00:48:50.120]   so we look now at a bit more narrow classification segmentation problems so
[00:48:50.120 --> 00:48:54.560]   it partitions an image in two regions of interest it's the first stage in many
[00:48:54.560 --> 00:49:01.960]   automatic image analysis systems a complete segmentation of an image I is a
[00:49:01.960 --> 00:49:08.120]   finite a finite set of regions are on to our ends such that if you put all the
[00:49:08.120 --> 00:49:13.440]   regions together the union of all the regions is the whole image and there's
[00:49:13.440 --> 00:49:18.560]   no overlap between regions they're strictly exclusive okay so that's a
[00:49:18.560 --> 00:49:25.360]   complete segmentation definition of complete segmentation here's an example
[00:49:25.360 --> 00:49:32.520]   of an image how should we segment this right so probably something like this
[00:49:32.520 --> 00:49:37.760]   right it's probably you know we don't know what a task is but we can imagine
[00:49:37.760 --> 00:49:44.280]   that's probably something like this okay so how would we do this well we could
[00:49:44.280 --> 00:49:48.280]   kind of you know if you look at this image here essentially the pixels of
[00:49:48.280 --> 00:49:53.120]   interest these regions they all have they all brighter than the background in a
[00:49:53.120 --> 00:49:59.840]   sense so we could kind of look at it as follows kind of reading those images you
[00:49:59.840 --> 00:50:08.000]   can look at it the notice this although it's a gray scale image is actually a
[00:50:08.000 --> 00:50:14.160]   color image here that was loaded in you can look at a histogram which is
[00:50:14.160 --> 00:50:22.920]   actually this thing here so what's the histogram it's simply right so here it's
[00:50:22.920 --> 00:50:30.680]   just that function but the the histogram is actually a distribution of the
[00:50:30.680 --> 00:50:39.720]   intensities okay so we have 255 levels of intensity in a gray scale image and
[00:50:39.720 --> 00:50:46.760]   so we're just counting how many pixels of each color we have okay so so then you
[00:50:46.760 --> 00:50:49.680]   would get for that image you would get something like this here so what do we
[00:50:49.680 --> 00:50:53.600]   see here well we see that there's a lot of very dark pixels no surprise actually
[00:50:53.600 --> 00:50:59.560]   maybe a surprise is that the dark pixels are not just black okay you see they
[00:50:59.560 --> 00:51:05.520]   actually they actually distributed here in a range from let's say 0 to 20 but
[00:51:05.520 --> 00:51:10.320]   definitely 0 to 5 there's lots of I mean of course it's clipped here so it it's
[00:51:10.320 --> 00:51:16.360]   maybe a lot higher but essentially lots of values here 0 1 2 3 4 5 kind of and
[00:51:16.360 --> 00:51:21.800]   then still some values larger you know like still a few thousand pixels that
[00:51:21.800 --> 00:51:27.240]   are a little bit larger than that then kind of nothing and then you know a few
[00:51:27.240 --> 00:51:31.120]   pixels here that are kind of in the gray in the middle of you know they're not
[00:51:31.120 --> 00:51:34.640]   white they're not black but they're kind of in the middle of the of the intensity
[00:51:34.640 --> 00:51:44.880]   range between mostly between 100 and 150 roughly so that's a histogram that's a
[00:51:44.880 --> 00:51:51.760]   histogram of this this picture here and so you could imagine or actually let me
[00:51:51.760 --> 00:51:58.760]   ask you how would you now find you know we wanted to essentially separate these
[00:51:58.760 --> 00:52:05.160]   these pixels right so like we would like to find these regions here so how would
[00:52:05.160 --> 00:52:08.000]   you kind of do that
[00:52:22.240 --> 00:52:28.600]   so essentially we could say well let's say 50 so we could just do a Boolean
[00:52:28.600 --> 00:52:33.960]   kind of logical operation on the image and save it's larger than 50 you know
[00:52:33.960 --> 00:52:39.760]   it's true and otherwise it's false so you just do a comparison with the image
[00:52:39.760 --> 00:52:44.840]   intensity for every pixel you evaluate that function and then you know all the
[00:52:44.840 --> 00:52:49.000]   ones that end up with a one that had essentially larger than 50 above that
[00:52:49.000 --> 00:52:53.560]   threshold are the pixels of interest and the ones with zero are the pixels we
[00:52:53.560 --> 00:52:56.200]   don't care about are the background pixels right so that would be indeed a
[00:52:56.200 --> 00:53:04.040]   simple segmentation for this here now you know let's say we had an image like
[00:53:04.040 --> 00:53:08.920]   this here suddenly the problem gets a little bit more challenging how would we
[00:53:08.920 --> 00:53:14.200]   segment this also we you know like okay this could be one segmentation as an
[00:53:14.200 --> 00:53:20.320]   example but it's not immediately clear really if somebody asked you to segment
[00:53:20.320 --> 00:53:23.400]   that you probably immediately would say but you know like what do you want me to
[00:53:23.400 --> 00:53:28.680]   segment like what you know like at what level you know do you want to segment this
[00:53:28.680 --> 00:53:33.000]   in two three regions do you want to segment this and they want to get like
[00:53:33.000 --> 00:53:36.280]   every separate object somehow every flower separately or just the better
[00:53:36.280 --> 00:53:42.080]   flowers you know all of those could be possible right so the quality of
[00:53:42.080 --> 00:53:46.000]   segmentation depends on what you want to do with it segmentation algorithm will
[00:53:46.000 --> 00:53:50.280]   be chosen and evaluated with the application in mind so another example
[00:53:50.280 --> 00:53:55.840]   you know maybe like this so so segmentation is kind of an ill-posed
[00:53:55.840 --> 00:53:59.760]   problem you like it really depends on what definition now people can still
[00:53:59.760 --> 00:54:05.400]   meaningfully segment in the absence of a question but you know it's not always
[00:54:05.400 --> 00:54:11.840]   clear so here's some early work in trying to address that you know more as a
[00:54:11.840 --> 00:54:17.200]   more as an experimental science essentially just ask people to lots of people
[00:54:17.200 --> 00:54:23.880]   online to build segmentations how they see fit without a question and then you
[00:54:23.880 --> 00:54:27.600]   know some people will segment it like this other people will you know find
[00:54:27.600 --> 00:54:33.560]   additional regions and so on but essentially by getting many many
[00:54:33.560 --> 00:54:38.480]   different people to segment they can essentially build up these kind of
[00:54:38.480 --> 00:54:43.000]   distribution where if it's very dark it means everybody systematically all the
[00:54:43.000 --> 00:54:47.680]   subjects that had to segment the image always systematically put a boundary
[00:54:47.680 --> 00:54:52.640]   here separated through the regions there while this one that's a lot weaker you
[00:54:52.640 --> 00:54:56.720]   know some people did some people didn't it's it's representing that so it's
[00:54:56.720 --> 00:55:01.960]   capturing kind of what would be the right segmentations essentially the
[00:55:01.960 --> 00:55:08.000]   distribution of segmentations in some sense I don't know the same thing
[00:55:08.000 --> 00:55:12.760]   I don't have a slide about it but a few months ago the latest work in this kind
[00:55:12.760 --> 00:55:20.320]   of broader space came from Meta where they essentially proposed a learned
[00:55:20.320 --> 00:55:24.000]   model that would essentially have been learned on somehow these type of things
[00:55:24.000 --> 00:55:31.920]   and then lots of things beyond that they call segment anything and so that one
[00:55:31.920 --> 00:55:36.240]   you know you can kind of provide it any imagery and it will do some reasonable
[00:55:36.240 --> 00:55:41.160]   job based on you know knowing kind of from people and from other inputs what
[00:55:41.160 --> 00:55:45.720]   typically should be done so there's been a lot of progress in that space and
[00:55:45.720 --> 00:55:52.000]   there are quite advanced deep learning models that will be very you know quite
[00:55:52.000 --> 00:55:57.720]   powerful you could have a look at it of course in this lecture we'll start from
[00:55:57.720 --> 00:56:03.600]   the basics we'll look at we'll introduce them the main important concepts
[00:56:03.600 --> 00:56:08.760]   concepts that generalize all the way to these most advanced methods but it's
[00:56:08.760 --> 00:56:13.600]   important to start to really understand the fundamental principles really well
[00:56:13.600 --> 00:56:19.320]   so here's the algorithm that we just discussed earlier for this kind of very
[00:56:19.320 --> 00:56:24.680]   simple segmentation task a simple thresholding also see it as a very
[00:56:24.680 --> 00:56:32.460]   simple decision function but you know other algorithms could have you know
[00:56:32.460 --> 00:56:36.000]   would have much more involved decision functions and so on or combinations of
[00:56:36.000 --> 00:56:40.920]   many decision functions that you try to combine together etc here we look at
[00:56:40.920 --> 00:56:47.760]   just a very single very simple decision function but that has a parameter so it's
[00:56:47.760 --> 00:56:52.560]   a thresholding function right so as we discussed we we have essentially a
[00:56:52.560 --> 00:56:59.880]   binary image here that for every pixel will either have one or zero one if we
[00:56:59.880 --> 00:57:05.420]   are above the threshold zero if we below the threshold so it generates a binary
[00:57:05.420 --> 00:57:09.620]   image so given an image as input a grayscale image as input it will generate
[00:57:09.620 --> 00:57:14.780]   a binary image as output that binary image is essential classification one is
[00:57:14.780 --> 00:57:23.860]   inside zero is outside or one is foreground zero is background and what's
[00:57:23.860 --> 00:57:27.340]   important is of course that we have one parameter in this decision algorithm and
[00:57:27.340 --> 00:57:34.080]   that's the parameter T where do we put the threshold so you've seen for this
[00:57:34.080 --> 00:57:42.320]   example here you know anywhere between here and here we could arbitrarily choose
[00:57:42.320 --> 00:57:45.760]   the threshold we'd have the same solution because there was a very nice
[00:57:45.760 --> 00:57:52.560]   separation but now let's look at a slightly more complicated image still
[00:57:52.560 --> 00:58:00.680]   quite simple here a duck or a geese maybe it's a duck but it could be a geese
[00:58:00.680 --> 00:58:09.120]   anyways so you have this picture here and so it seems easy enough to segment
[00:58:09.120 --> 00:58:16.040]   for us right so we could do same threshold as before threshold equals 50
[00:58:16.040 --> 00:58:22.840]   okay that's not good enough so let's try to find a better threshold right so
[00:58:22.840 --> 00:58:28.120]   this would be the segmentation yellow inside blue outside okay so not 50 let's
[00:58:28.120 --> 00:58:37.080]   try 100 okay a bit better almost there so this now segmentation okay we're almost
[00:58:37.080 --> 00:58:45.360]   there so maybe we can tweak it a bit more 150 okay now we got rid of most of the
[00:58:45.360 --> 00:58:52.640]   stuff that should be outside but if you actually look carefully we're starting to
[00:58:52.640 --> 00:58:59.040]   lose here part of the of this bird okay so hmm it looks like we won't quite be
[00:58:59.040 --> 00:59:03.480]   able to to get this with a simple algorithm will not be good enough to
[00:59:03.480 --> 00:59:09.240]   actually really solve our problem here and then if we go further it gets only
[00:59:09.240 --> 00:59:17.200]   worse right okay so still how do we choose our threshold what would be the
[00:59:17.200 --> 00:59:21.560]   right threshold so of course you could just try the narrow right you have a
[00:59:21.560 --> 00:59:25.440]   this is quite fast to compute you could try it out try different values and
[00:59:25.440 --> 00:59:32.240]   choose what you like most but that's not very principled of course the typical
[00:59:32.240 --> 00:59:37.440]   way to do it is to compare to ground truth this is actually you know what you
[00:59:37.440 --> 00:59:43.480]   do in more advanced setting in its in machine learning is when you have you
[00:59:43.480 --> 00:59:49.600]   have ground truth data you have data with answers provided with not only the
[00:59:49.600 --> 00:59:55.680]   input but also the output given and so there you can tune your algorithm in
[00:59:55.680 --> 00:59:59.040]   this case it's very simple just one threshold but you can tune your
[00:59:59.040 --> 01:00:03.800]   threshold to do the best possible response whatever way you define best
[01:00:03.800 --> 01:00:10.840]   but best possible response and so so we'll kind of look at the ground truth
[01:00:10.840 --> 01:00:17.360]   but in an automated way and we'll get to that a little bit later with so-called
[01:00:17.360 --> 01:00:22.760]   ROC curves the receiver operator characteristics will get to that a
[01:00:22.760 --> 01:00:29.200]   little bit later okay ideally we had a situation like this one here with you
[01:00:29.200 --> 01:00:34.880]   know a distribution of negatives in in this one dimension in this case the
[01:00:34.880 --> 01:00:38.640]   intensity and then we have a threshold this could also be in other dimensions
[01:00:38.640 --> 01:00:45.280]   you could imagine a particular feature for an algorithm a learning algorithm
[01:00:45.280 --> 01:00:51.640]   and in that for that one feature you have all of your so in this case that
[01:00:51.640 --> 01:00:57.320]   features just intensity but it could also be the result of another function you
[01:00:57.320 --> 01:01:02.760]   have a one-dimensional you map your pixels you map your images parts of
[01:01:02.760 --> 01:01:07.000]   your images to this one-dimensional thing and then use if it's if you're lucky
[01:01:07.000 --> 01:01:11.920]   you know you have a perfect separation between the two distributions the one
[01:01:11.920 --> 01:01:16.240]   you know the ones you want to keep so the foreground let's say and the
[01:01:16.240 --> 01:01:19.880]   background if that's the case it's easy that was the one we had in the first
[01:01:19.880 --> 01:01:24.720]   example right we put a threshold at 50 we get a perfect separation right so
[01:01:24.720 --> 01:01:32.000]   this two set distribution a separated it's easy now there are situations where
[01:01:32.000 --> 01:01:37.520]   you might be able to achieve that here's an example this was actually a class
[01:01:37.520 --> 01:01:44.520]   project long ago it's kind of this chroma so-called chroma king so this is
[01:01:44.520 --> 01:01:49.720]   separation of foreground and background but in a in a place where you actually
[01:01:49.720 --> 01:01:53.800]   take the picture with the purpose of separate you know like you instrument the
[01:01:53.800 --> 01:01:59.680]   world in a way to make the problem easy in particular here for color picture you
[01:01:59.680 --> 01:02:05.840]   will actually have a subject and then the background you make it as simple as
[01:02:05.840 --> 01:02:09.440]   possible and as different of the foreground as possible you've seen that
[01:02:09.440 --> 01:02:16.600]   there's not much green or blue in the colors we typically I mean or like human
[01:02:16.600 --> 01:02:23.640]   colors human skin so also for animals and so on there wouldn't be much green for
[01:02:23.640 --> 01:02:28.480]   example so if you instrument the background you can make that segmentation
[01:02:28.480 --> 01:02:38.400]   problem simpler so this is like so-called chroma keying and so you could for
[01:02:38.400 --> 01:02:47.960]   example here you have a color so the result of your decision of keeping a
[01:02:47.960 --> 01:02:53.400]   pixel or not would be for example you have you make the background on purpose
[01:02:53.400 --> 01:02:59.760]   green and then you would look at a difference between is it different in
[01:02:59.760 --> 01:03:06.240]   that is it close to green or is it far from green if it's close to green then
[01:03:06.240 --> 01:03:12.960]   this is this is not satisfied and then you have a zero here but if it's if it's
[01:03:12.960 --> 01:03:16.640]   far from green then you would say okay this is clearly not background so it
[01:03:16.640 --> 01:03:21.360]   must be foreground so if you're above this threshold here then you would switch
[01:03:21.360 --> 01:03:27.080]   this to one so one example would be 20 color values the green color this for
[01:03:27.080 --> 01:03:32.640]   color image now would be 0 to 55 and 0 remember remember R, G and B so red, green
[01:03:32.640 --> 01:03:40.760]   and blue so this would be the kind of perfectly green saturated green zero for
[01:03:40.760 --> 01:03:46.200]   the other colors now there's some problems with this variation is not
[01:03:46.200 --> 01:03:51.680]   necessarily the same in all three channels so meaning that a fixed threshold
[01:03:51.680 --> 01:03:55.240]   that's applied in all directions the same it's not necessarily the right thing to
[01:03:55.240 --> 01:04:01.680]   do also this would generate a hard alpha mask so it would really per pixel
[01:04:01.680 --> 01:04:05.040]   decide its foreground or its background but it cannot be a blend of
[01:04:05.040 --> 01:04:11.800]   foreground and background because of the lens blur or other effects or also
[01:04:11.800 --> 01:04:17.040]   let's say you see you know if you have hair in front of let's say a green
[01:04:17.040 --> 01:04:21.760]   background the hair will kind of be semi-transparent essentially and there
[01:04:21.760 --> 01:04:26.160]   will be a green hue from the what's in the background you know going being
[01:04:26.160 --> 01:04:33.000]   visible through your hair so that's that would be modeled as this alpha channel
[01:04:33.000 --> 01:04:38.600]   not being binary but actually being a continuous number of its 50 percent
[01:04:38.600 --> 01:04:43.600]   foreground or its 30 percent foreground or stuff like that okay but that's
[01:04:43.600 --> 01:04:49.880]   beyond what we'll we'll see here okay so let's see we have something here I don't
[01:04:49.880 --> 01:04:54.200]   display the color cube RGB three-dimensional color cube but just a
[01:04:54.200 --> 01:05:01.880]   two-dimensional red and green so just one slice through that cube so this is the
[01:05:01.880 --> 01:05:10.000]   amount of intensity green intensity this is the amount of red intensity so you
[01:05:10.000 --> 01:05:13.400]   know imagine if you have a face of a person it's going to be high in the red
[01:05:13.400 --> 01:05:17.680]   and low in the green so it's going to be somewhere up there and then the background
[01:05:17.680 --> 01:05:23.360]   of a chroma keying would be pixels that hopefully have a lot of green and and
[01:05:23.360 --> 01:05:33.240]   only green so no red so you know if we have a if everything is this simple it
[01:05:33.240 --> 01:05:36.600]   would be easy but let's say we have something like this here these are kind
[01:05:36.600 --> 01:05:40.920]   of the background pixels we observe and then the question is okay this this
[01:05:40.920 --> 01:05:47.080]   particular pixel with these values is this foreground or background okay you
[01:05:47.080 --> 01:05:51.560]   can see that compared to the average here these points are as far from the
[01:05:51.560 --> 01:05:56.480]   average as this point roughly so you might actually want a model that's a
[01:05:56.480 --> 01:06:03.160]   little bit more refined than a single you know uniform spherical spherical
[01:06:03.160 --> 01:06:07.880]   distribution modeling the background so you might actually want to fit an ellipse
[01:06:07.880 --> 01:06:14.800]   here that somehow better represents the background distribution the background
[01:06:14.800 --> 01:06:19.520]   color distribution so that you can better make a decision between these all
[01:06:19.520 --> 01:06:24.480]   being background pixels and then this one is far enough so that you can
[01:06:24.480 --> 01:06:30.640]   classify it as being a foreground pixel it's essentially the binary decision
[01:06:30.640 --> 01:06:37.000]   would be essentially you know this this ellipse here right so inside the ellipse
[01:06:37.000 --> 01:06:43.000]   you'd say it's foreground outside the ellipse you'd say so sorry outside the
[01:06:43.000 --> 01:06:47.360]   ellipse is foreground and inside the ellipse is background
[01:06:48.200 --> 01:06:56.600]   okay so most here cartoon version of the Gaussian just as a reminder you get
[01:06:56.600 --> 01:07:02.800]   about two thirds within plus or minus one sigma and you get about 95% within
[01:07:02.800 --> 01:07:15.040]   plus or minus two sigma so let's look here at an example here
[01:07:15.040 --> 01:07:27.880]   of these colors so this now is just the blue-green colors here that will we'll
[01:07:27.880 --> 01:07:34.960]   see this sorry so here you have essentially the ratios are red divided by
[01:07:34.960 --> 01:07:38.400]   the full intensity green divided by the full intensity and blue divided by the
[01:07:38.400 --> 01:07:42.960]   full intensity and then it looks like this here okay so it's kind of normalized
[01:07:42.960 --> 01:07:47.160]   and so you see you could kind of nicely segment all of these green pixels
[01:07:47.160 --> 01:07:58.280]   compared to others that's one way that you can do slightly more the you know
[01:07:58.280 --> 01:08:01.320]   here's kind of a challenge with these mixed pixels that were mentioned so
[01:08:01.320 --> 01:08:05.400]   mixed pixels can both be because you have hair for example that's semi-transparent
[01:08:05.400 --> 01:08:10.200]   that you can also see the background through the hair but actually also it
[01:08:10.200 --> 01:08:16.480]   can be temporal motion blur if the hand moves fast then for part of the time you
[01:08:16.480 --> 01:08:19.640]   taking the picture the hand is in the way and there's no light coming through
[01:08:19.640 --> 01:08:23.200]   but then part of the time the hand might be somewhere else and you actually see
[01:08:23.200 --> 01:08:27.600]   background so now it's it could be 50/50 but it's because half the time your hand
[01:08:27.600 --> 01:08:31.680]   was there or part of your hand was overlapping that pixel was visible in
[01:08:31.680 --> 01:08:35.320]   that pixel in the other half you were just looking at background the other half
[01:08:35.320 --> 01:08:39.560]   of the exposure time the time that you started looking and then until you stopped
[01:08:39.560 --> 01:08:45.760]   looking integrating the light at that pixel right so clearly they also
[01:08:45.760 --> 01:08:52.520]   challenges of you know doing this and so again the way to model that would
[01:08:52.520 --> 01:08:59.960]   actually be by having this be non-binary okay so those are a few examples of
[01:08:59.960 --> 01:09:03.720]   kind of a simple where you actually instrument the problem to make the
[01:09:03.720 --> 01:09:12.320]   problem simpler let's let's get back to our to our duck kind of problem here so
[01:09:12.320 --> 01:09:19.960]   we want to you know we want to automate choosing the right threshold the the
[01:09:19.960 --> 01:09:23.840]   right threshold the best possible threshold the best trade-off it is
[01:09:23.840 --> 01:09:30.800]   going to be a trade-off in practice to do that you want to not you know have to
[01:09:30.800 --> 01:09:34.880]   tweak yourself or so you actually want to do a more principled way you actually
[01:09:34.880 --> 01:09:40.880]   want some ground truth you know this is the answer that I would like to have and
[01:09:40.880 --> 01:09:47.200]   now I will tweak my parameters to get as close as possible to this and as close
[01:09:47.200 --> 01:09:51.440]   as possible again is defined in certain ways we'll get to that okay so this is
[01:09:51.440 --> 01:09:57.440]   the ground truth so this is the solution we would like to have and now we'll
[01:09:57.440 --> 01:10:02.600]   essentially try to characterize how well different different parameter settings
[01:10:02.600 --> 01:10:08.720]   do in this and what type of mistakes they make or as really makes with different
[01:10:08.720 --> 01:10:17.000]   settings okay so for that we'll use the ROC curve so the receive operating
[01:10:17.000 --> 01:10:20.880]   characteristic it's a curve that characterize the performance of a binary
[01:10:20.880 --> 01:10:28.080]   classifier and essentially binary classifiers coming many types here is
[01:10:28.080 --> 01:10:31.680]   the question is foreground background that's the one we're interested in but
[01:10:31.680 --> 01:10:36.400]   it can be many other things it can be cancer screening any automated decision
[01:10:36.400 --> 01:10:43.640]   mechanism you will essentially try you know have a binary answer cancer you
[01:10:43.640 --> 01:10:48.680]   know cancer or no cancer pregnant not pregnant is the object there or is it
[01:10:48.680 --> 01:10:53.440]   not there you know foreground background as in our example so we'll continue with
[01:10:53.440 --> 01:11:01.040]   the last one here so essentially we have ground truth that tells us what is
[01:11:01.040 --> 01:11:06.440]   foreground and what is background and then we'll run our algorithm with a
[01:11:06.440 --> 01:11:11.320]   particular setting and now I will also tell us what is things is foreground and
[01:11:11.320 --> 01:11:18.880]   what it thinks is background so we have kind of two times two possibilities okay
[01:11:18.880 --> 01:11:25.560]   we have true positives so that's one that is our algorithm said was positive and
[01:11:25.560 --> 01:11:31.120]   it was actually if we go check the ground truth it is actually positive okay so
[01:11:31.120 --> 01:11:35.640]   if both states positive then that's a true positive so our algorithm predicts
[01:11:35.640 --> 01:11:40.000]   its positive and it was actually the right answer the ground truth the right
[01:11:40.000 --> 01:11:49.360]   answer was positive there's the same with negatives so true negatives so the we
[01:11:49.360 --> 01:11:53.200]   know the answer should be negative and our algorithm actually also predicts
[01:11:53.200 --> 01:12:02.160]   negative but then you have two undesired situations a false positive false
[01:12:02.160 --> 01:12:07.520]   negative so this is one that the true solution should be positive but our
[01:12:07.520 --> 01:12:12.560]   algorithm thought it was negative okay so it made a mistake there same thing a
[01:12:12.560 --> 01:12:16.560]   false positive ground truth say it should be negative but our algorithm thought
[01:12:16.560 --> 01:12:21.720]   it was positive okay so those are so there's four cases two of those are good
[01:12:21.720 --> 01:12:27.360]   and two or those are problematic okay also if we look at those are the
[01:12:27.360 --> 01:12:31.320]   correctly classified ones those are the incorrectly classified ones by our
[01:12:31.320 --> 01:12:35.840]   algorithm compared to ground truth and then this is the total number of
[01:12:35.840 --> 01:12:39.560]   positives that our algorithm generates and these are a total number of negatives
[01:12:39.560 --> 01:12:48.000]   that our algorithm is generated okay so now if we look at you know our example
[01:12:48.000 --> 01:12:53.760]   and I think this is for the 150 threshold we can compare now we can use
[01:12:53.760 --> 01:12:58.960]   the ground truth and compare and classify essentially in a sense we've
[01:12:58.960 --> 01:13:06.200]   not classified the image in four different regions this you know we have
[01:13:06.200 --> 01:13:11.160]   essentially the ground truth is a binary classifier our algorithm is a binary
[01:13:11.160 --> 01:13:14.800]   classifier so we have two binary classifier combines that gives us you
[01:13:14.800 --> 01:13:20.120]   know essentially four bits two bits so four possibilities right so we classify
[01:13:20.120 --> 01:13:25.520]   now the region in these four possibilities the true positives that's
[01:13:25.520 --> 01:13:30.360]   great the false positives that's not so good the true negatives that's all good
[01:13:30.360 --> 01:13:34.160]   and then the false negatives the one that we should have labeled positive but
[01:13:34.160 --> 01:13:42.200]   algorithm label negative okay so what we'll do is the RSE curve here it is
[01:13:42.200 --> 01:13:49.000]   going to plot the true positive fraction versus against the full positive
[01:13:49.000 --> 01:13:58.480]   fraction okay so the true positive is is essentially the sensitivity so the true
[01:13:58.480 --> 01:14:02.520]   positive divided by the total number of positives is like how many of the
[01:14:02.520 --> 01:14:09.120]   positives did we already pick up and then the here the false positive fraction
[01:14:09.120 --> 01:14:18.120]   is the false positive count over the actual negatives okay so if we plot
[01:14:18.120 --> 01:14:24.280]   is how it tends to look something like this okay so these are the false
[01:14:24.280 --> 01:14:36.960]   positives the false positive ratio and this is the true positive ratio so it's
[01:14:36.960 --> 01:14:44.640]   the true positive count divided by the the positives right so typically the
[01:14:44.640 --> 01:14:50.760]   curve looks like this because typically initially remember right if we sweep or
[01:14:50.760 --> 01:14:55.160]   actually this curve let me actually this one more thing I need to explain
[01:14:55.160 --> 01:15:02.600]   actually so this curve corresponds to different threshold values okay so if I
[01:15:02.600 --> 01:15:11.280]   start from you know let's say the positives are you know like I have the
[01:15:11.280 --> 01:15:17.960]   I start from the very bright pixels because the brightest pixels are
[01:15:17.960 --> 01:15:25.000]   definitely on the duck okay so if I do that you know if if they're really
[01:15:25.000 --> 01:15:29.880]   definitely on the duck then it means that initially I will start going straight
[01:15:29.880 --> 01:15:35.240]   up here because the first few pixels that I classify will be true positives
[01:15:35.240 --> 01:15:39.720]   that I classify as positives so that means that I will do really well here
[01:15:39.720 --> 01:15:45.560]   while having no false positives okay so I will start going straight up typically
[01:15:45.560 --> 01:15:51.960]   but then after a while you know once you get to the underbelly of the of the
[01:15:51.960 --> 01:15:57.160]   duck you start getting it's getting more difficult for every you know trying to
[01:15:57.160 --> 01:16:02.040]   get more of the belly of the duck in starts getting more and more of the
[01:16:02.040 --> 01:16:05.880]   false negatives at the false positive story getting me more and more false
[01:16:05.880 --> 01:16:11.480]   positives so initially it's easy typically but then as I go from the
[01:16:11.480 --> 01:16:16.600]   threshold at 255 you know initially threshold 255 means I have you know
[01:16:16.600 --> 01:16:20.880]   everything is negative essentially so that's not good in general but it means
[01:16:20.880 --> 01:16:25.400]   that the first few one that I pick will definitely be good one so I skyrocket
[01:16:25.400 --> 01:16:30.040]   up here but then after a while it gets more difficult and then if I every time
[01:16:30.040 --> 01:16:35.720]   I start including additional positives so I lower my threshold I get more I want
[01:16:35.720 --> 01:16:41.240]   to get all the positives this comes at a cost of including more and more and
[01:16:41.240 --> 01:16:45.160]   proportionately more and more of the negatives also in and getting more and
[01:16:45.160 --> 01:16:49.400]   more false negatives that's when this number starts going up the ratio of the
[01:16:49.400 --> 01:16:56.080]   false negatives that I'm kind of getting in so so typically a curve will start
[01:16:56.080 --> 01:16:59.480]   going straight up then kind of flatten out and then of course eventually as I
[01:16:59.480 --> 01:17:03.560]   lower the threshold all the way to zero eventually I just add everything in and
[01:17:03.560 --> 01:17:08.440]   so then of course I'm at the upper right corner which is well now all the
[01:17:08.440 --> 01:17:12.320]   positives are labeled positive but all the negatives are also labeled positive
[01:17:12.320 --> 01:17:16.000]   because I move my threshold to zero right so in the limit I always end up
[01:17:16.000 --> 01:17:20.560]   there in a sense the interesting part to be is of course somewhere in the upper
[01:17:20.560 --> 01:17:24.840]   left corner that's where I want to be so I want to be somewhere as close as
[01:17:24.840 --> 01:17:31.160]   possible this is the perfect situation right that's where I have all my
[01:17:31.160 --> 01:17:36.560]   positives and nothing of my negatives yet right this is the ratio of the
[01:17:36.560 --> 01:17:41.800]   negatives that are falsely labeled as positive right so if I'm up in the corner
[01:17:41.800 --> 01:17:45.800]   there if I can find a point you know a classifier that's up in the corner there
[01:17:45.800 --> 01:17:51.560]   then I got all my positives I'm a hundred percent of my positives without
[01:17:51.560 --> 01:17:56.960]   having included any negative so that's the perfect classifier notice that's
[01:17:56.960 --> 01:18:00.160]   what we could achieve on the first example where it was really both
[01:18:00.160 --> 01:18:06.240]   distributions were strictly separated in the one-dimensional threshold space
[01:18:09.600 --> 01:18:15.160]   okay so here's some of the key properties of our RC curves they always pass to
[01:18:15.160 --> 01:18:20.880]   0 0 and 1 1 that's you know the extreme thresholds like everything's in or
[01:18:20.880 --> 01:18:26.880]   everything's out that gives you 0 0 or 1 1 okay and that's like the beginning and
[01:18:26.880 --> 01:18:32.760]   the end of each of those curves I kind of already said this what is the RC of the
[01:18:32.760 --> 01:18:35.240]   perfect system
[01:18:35.960 --> 01:18:45.120]   anyone exactly right and the perfect place is of course at that corner that's
[01:18:45.120 --> 01:18:52.920]   where you want to operate another question what if instead of being straight
[01:18:52.920 --> 01:18:58.200]   up and straight to the right you actually have a line that goes diagonal from 0 0
[01:18:58.200 --> 01:19:02.360]   to 1 1 what would that mean
[01:19:03.360 --> 01:19:11.400]   exactly random guesses so you're it doesn't matter like it's you're doing
[01:19:11.400 --> 01:19:14.600]   just as well on the positive as a negative you have no idea what you're
[01:19:14.600 --> 01:19:19.880]   doing the classifiers this random stuff questions can you do even worse let's
[01:19:19.880 --> 01:19:25.560]   say you know the curve so this is kind of random guessing what what if you have a
[01:19:25.560 --> 01:19:28.800]   curve that does something like this here like a terrible classifier does
[01:19:28.800 --> 01:19:39.240]   everything wrong essentially what should you do then exactly right so essentially
[01:19:39.240 --> 01:19:42.120]   you will never have a classifier that's worse than this here because then you
[01:19:42.120 --> 01:19:45.000]   would just flip it around to use it the other way around and say whatever
[01:19:45.000 --> 01:19:48.000]   predicts I'll say the opposite I'll take the opposite right and then you'd
[01:19:48.000 --> 01:19:57.040]   actually have a decent classifier okay okay so if you go back to these
[01:19:57.040 --> 01:20:05.200]   distributions as we sweep as we go around along that curve if we have
[01:20:05.200 --> 01:20:11.760]   actually the the true positives I mean so we have the ground truth here and the
[01:20:11.760 --> 01:20:17.040]   ground truth would have on our gray level here would have the foreground would
[01:20:17.040 --> 01:20:21.040]   for example in this case be red and the background be blue notice here the
[01:20:21.040 --> 01:20:26.160]   background the foreground is actually dark and the background is is lighter in
[01:20:26.160 --> 01:20:33.160]   this example then if we choose a particular threshold here what are we
[01:20:33.160 --> 01:20:36.960]   doing we essentially saying okay all of these ones are correctly classified as
[01:20:36.960 --> 01:20:42.440]   foreground but all of these ones on that side of the threshold the ones that
[01:20:42.440 --> 01:20:47.280]   should have been classified like they're misclassified and vice versa for the
[01:20:47.280 --> 01:20:52.760]   blue curve the background pixels there's these ones here that are incorrectly
[01:20:52.760 --> 01:21:00.440]   classified the other way right so essentially these are true positives
[01:21:00.440 --> 01:21:06.240]   these are true negatives these are false negatives and these are false
[01:21:06.240 --> 01:21:17.440]   positives right or four classes here so how should we modify the like where
[01:21:17.440 --> 01:21:21.920]   should we choose our threshold like given those curves where would you operate
[01:21:21.920 --> 01:21:24.960]   this here
[01:21:32.240 --> 01:21:37.400]   correct let's say absolutely correct but let's say it's kind of the same you
[01:21:37.400 --> 01:21:40.320]   know like you just don't want to make wrong decisions so you want to minimize
[01:21:40.320 --> 01:21:44.840]   the amount of misclassified things like the false positive and false negatives
[01:21:44.840 --> 01:21:55.640]   are equally costly so where where would that be essentially exactly so that
[01:21:55.640 --> 01:22:01.000]   would be at intersection point because the marginal amount of misclassifications
[01:22:01.000 --> 01:22:05.480]   that you would as you slightly move this around is kind of the length of that you
[01:22:05.480 --> 01:22:11.320]   know of that like if you it's like an integral right if you move a small
[01:22:11.320 --> 01:22:16.640]   amount here it's kind of that width times the height that you know you are
[01:22:16.640 --> 01:22:21.960]   flipping from one side to the other and so clearly here at the intersection
[01:22:21.960 --> 01:22:27.800]   point will be exactly where you know you start having less mistakes from one but
[01:22:27.800 --> 01:22:33.720]   more of the other that's where they cross over so that's the crossover point so
[01:22:33.720 --> 01:22:39.000]   this would be the base minimum risk detector would operate there but
[01:22:39.000 --> 01:22:46.600]   exactly as you were saying this is not always the right thing because depending
[01:22:46.600 --> 01:22:50.960]   on the decision a false negative could be much worse than a false positive
[01:22:50.960 --> 01:22:57.760]   think of cancer for example a false positive would just mean that you would
[01:22:57.760 --> 01:23:01.440]   get a second test and then you know would be scared you would get a second
[01:23:01.440 --> 01:23:07.280]   test and then you know it that would then likely that might be a more expensive
[01:23:07.280 --> 01:23:11.280]   test and a more discriminatory test and so on and so probably then that would
[01:23:11.280 --> 01:23:14.680]   just turn out negative and you'd be say you would have been scared for a week and
[01:23:14.680 --> 01:23:17.920]   then you'd be fine and everything would be sorted out and so the cost would only
[01:23:17.920 --> 01:23:24.880]   be the cost of that additional test and a little bit of being scared the false
[01:23:24.880 --> 01:23:30.520]   negative would mean that you have cancer you're labeled as not having cancer you
[01:23:30.520 --> 01:23:34.840]   don't get treatment and you know a little bit later you might die from your
[01:23:34.840 --> 01:23:38.640]   cancer by not having been treated right so that it's very asymmetric in some
[01:23:38.640 --> 01:23:44.360]   situation so indeed in those cases you really have to you know weigh those
[01:23:44.360 --> 01:23:51.160]   different costs in an appropriate way as shown here and that essentially
[01:23:51.160 --> 01:23:58.760]   typical corresponds to choosing like a particular that you want to operate at a
[01:23:58.760 --> 01:24:08.360]   particular orientation of the curve there okay so let's look again here at the
[01:24:08.360 --> 01:24:14.960]   classification was it here so here a few different examples this is the actual
[01:24:14.960 --> 01:24:21.880]   curve or you know like sampled by the few discrete locations here that's the
[01:24:21.880 --> 01:24:26.360]   actual curve you get and so as you can see it's really literally it's trade up
[01:24:26.360 --> 01:24:31.360]   here in this case because you literally would have made no mistakes in the first
[01:24:31.360 --> 01:24:37.560]   you know up to a certain range right in these examples here initially there
[01:24:37.560 --> 01:24:42.440]   would be no mistakes and then only gradually would you kind of start making
[01:24:42.440 --> 01:24:46.880]   misclassifications that's where you start flattening out here so you you're
[01:24:46.880 --> 01:24:52.360]   kind of perfect you get it first 85% of the pixels classified correctly but
[01:24:52.360 --> 01:24:55.240]   everyone's you try to push beyond that that's where you start making more and
[01:24:55.240 --> 01:25:01.760]   more mistakes until very quickly you're kind of making many more mistakes for
[01:25:01.760 --> 01:25:10.120]   very few additional good pixels that you classify in this is like the gray level
[01:25:10.120 --> 01:25:16.160]   histogram and so you can kind of you know see here that there's this is the
[01:25:16.160 --> 01:25:20.720]   range where you probably want to put the threshold but if you actually now
[01:25:20.720 --> 01:25:24.640]   separate this if we have ground truth we can separate in distribution of foreground
[01:25:24.640 --> 01:25:28.520]   and the distribution of background and then you kind of see that essentially
[01:25:28.520 --> 01:25:35.160]   there's this whole region of overlap so essentially as you start sweeping down
[01:25:35.160 --> 01:25:43.040]   here everything is great until here right about 150 and then you start actually
[01:25:43.040 --> 01:25:46.240]   getting a lot of you know to get the additional yellow ones you need to get
[01:25:46.240 --> 01:25:50.240]   in a lot of blue ones and very quickly the last few once you get here here it's
[01:25:50.240 --> 01:25:55.080]   about equal but then once you get here to get those few extra yellow ones the
[01:25:55.080 --> 01:26:01.320]   foreground ones you are starting to really throw in a lot of bad ones okay
[01:26:01.320 --> 01:26:15.840]   okay so there's essentially clear limitations the so you could so limit
[01:26:15.840 --> 01:26:21.160]   so so a single simple feature like in this case how bright is the pixel how
[01:26:21.160 --> 01:26:27.200]   bright is this particular point is intrinsically very limited maybe you know
[01:26:27.200 --> 01:26:32.520]   we could look at a few neighboring pixels either actually just say like well
[01:26:32.520 --> 01:26:37.200]   if all my neighbors are you know look like a duck then maybe I'm also the kind
[01:26:37.200 --> 01:26:43.080]   of pixel so that's like surface coherence to have neighbors kind of if
[01:26:43.080 --> 01:26:47.240]   neighbors are very correlated then you can use that and this is something we'll
[01:26:47.240 --> 01:26:53.320]   look a little bit at you can also do more advanced features not just the
[01:26:53.320 --> 01:26:57.280]   brightness but you could actually and we'll see this later you could actually
[01:26:57.280 --> 01:27:03.160]   you know if you look at the picture of the duck here if there's particular
[01:27:03.160 --> 01:27:09.360]   characteristics for example the duck is not only locally bright but also it's
[01:27:09.360 --> 01:27:14.120]   kind of a whole region is typically bright so you could kind of look for that
[01:27:14.120 --> 01:27:21.600]   you could look at a number of different texture properties that you know the
[01:27:21.600 --> 01:27:25.760]   local appearance in a region looks different on the duck than on the
[01:27:25.760 --> 01:27:30.280]   reflection here of the duck for example you could start bringing multiple
[01:27:30.280 --> 01:27:33.520]   features together and then use a combination of those a non-linear
[01:27:33.520 --> 01:27:37.280]   combination of those then we're starting to get really into more advanced
[01:27:37.280 --> 01:27:40.160]   machine learning algorithms that then have many thresholds and many parameters
[01:27:40.160 --> 01:27:45.560]   that you would all adjust together okay so we'll leave it at that for today and
[01:27:45.560 --> 01:27:50.480]   we'll continue with this on Thursday
[01:27:50.480 --> 01:27:53.640]   (audience applauding)
[01:27:53.640 --> 01:27:55.500]   [APPLAUSE]

