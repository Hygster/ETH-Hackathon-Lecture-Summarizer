
[00:00:00.000 --> 00:00:07.480]   [MUSIC PLAYING]
[00:00:07.480 --> 00:00:10.480]   OK, so good afternoon, everyone.
[00:00:10.480 --> 00:00:16.720]   So today we'll talk about what digital images, both how--
[00:00:16.720 --> 00:00:20.360]   we'll talk about sensors, how to represent images,
[00:00:20.360 --> 00:00:24.240]   about how we can sense images, look
[00:00:24.240 --> 00:00:25.840]   at a bit of different aspects and issues
[00:00:25.840 --> 00:00:28.480]   that you can encounter there.
[00:00:28.480 --> 00:00:33.440]   And then we'll also look at image representation
[00:00:33.440 --> 00:00:35.640]   and all the different aspects and things
[00:00:35.640 --> 00:00:38.520]   that are important there and how we actually represent images.
[00:00:38.520 --> 00:00:45.400]   And we'll touch just a very little bit on how we actually
[00:00:45.400 --> 00:00:48.840]   represent digital images by samples and things like that.
[00:00:48.840 --> 00:00:51.160]   Some of those concepts we'll then revisit much more in depth
[00:00:51.160 --> 00:00:54.240]   in the coming weeks.
[00:00:54.240 --> 00:00:59.640]   But that's essentially the program for today.
[00:00:59.640 --> 00:01:03.960]   I also apologize the slides are not yet online.
[00:01:03.960 --> 00:01:05.960]   I was trying to exchange them or give access
[00:01:05.960 --> 00:01:09.280]   to the assistance to upload the slides.
[00:01:09.280 --> 00:01:13.920]   And there was some issue with draw box quotas and stuff
[00:01:13.920 --> 00:01:15.520]   like that that couldn't--
[00:01:15.520 --> 00:01:17.080]   we haven't yet set it up so.
[00:01:17.080 --> 00:01:21.680]   But in the future, you should get the slides ahead of time
[00:01:21.680 --> 00:01:22.180]   available.
[00:01:22.180 --> 00:01:31.360]   OK, so just a random picture here.
[00:01:31.360 --> 00:01:34.520]   But essentially, if you think of--
[00:01:34.520 --> 00:01:38.600]   take any cell phone, look out at the world,
[00:01:38.600 --> 00:01:39.960]   grab a picture.
[00:01:39.960 --> 00:01:44.920]   If you actually think of what this sensor actually does,
[00:01:44.920 --> 00:01:46.040]   it's actually quite amazing.
[00:01:46.040 --> 00:01:49.000]   So if you just a little bit think through,
[00:01:49.000 --> 00:01:50.800]   there are sensors that will measure, I don't know,
[00:01:50.800 --> 00:01:54.400]   like temperature or other things that will locally give you
[00:01:54.400 --> 00:01:59.240]   one measurement at a time at a rate that's not too fast or so.
[00:01:59.240 --> 00:02:04.000]   If you look here, what do we have?
[00:02:04.000 --> 00:02:09.160]   We have a sensor, essentially a huge grid of sensors
[00:02:09.160 --> 00:02:17.480]   that all can snap a joint kind of observation of this scene
[00:02:17.480 --> 00:02:21.600]   with millions of coordinated measurements
[00:02:21.600 --> 00:02:25.480]   in a very strictly aligned grid.
[00:02:25.480 --> 00:02:28.440]   So very precise kind of measurements next to each other
[00:02:28.440 --> 00:02:30.480]   in slightly different directions,
[00:02:30.480 --> 00:02:34.320]   differing by literally arc seconds or fraction of arc
[00:02:34.320 --> 00:02:39.600]   seconds in angle.
[00:02:39.600 --> 00:02:41.400]   The other thing that's actually really interesting
[00:02:41.400 --> 00:02:45.840]   is to realize that it's not a local measurement.
[00:02:45.840 --> 00:02:48.360]   I mean, it's a local measurement of incoming light.
[00:02:48.360 --> 00:02:50.400]   But in a sense, given the speed of light,
[00:02:50.400 --> 00:02:52.600]   you're essentially observing anything
[00:02:52.600 --> 00:02:55.840]   that happens in any direction that it means light towards
[00:02:55.840 --> 00:02:58.880]   or reflects light towards you up to, in this case,
[00:02:58.880 --> 00:03:00.760]   many, many kilometers away.
[00:03:00.760 --> 00:03:01.960]   All of that in an instance, you're
[00:03:01.960 --> 00:03:04.160]   measuring all of these points, like the light coming in
[00:03:04.160 --> 00:03:05.080]   from all of this.
[00:03:05.080 --> 00:03:06.960]   So if you just think a little bit about that,
[00:03:06.960 --> 00:03:08.520]   it's actually quite amazing.
[00:03:08.520 --> 00:03:12.840]   And we have very few other sensors
[00:03:12.840 --> 00:03:19.200]   that do anything near this type of amount of sensors
[00:03:19.200 --> 00:03:21.600]   and the flexibility to be able to scan,
[00:03:21.600 --> 00:03:25.320]   to measure some property that could come from just
[00:03:25.320 --> 00:03:28.640]   like nearby from a half meter away all the way
[00:03:28.640 --> 00:03:32.080]   to tens or hundreds of kilometers potentially away.
[00:03:32.080 --> 00:03:33.560]   Or if you take pictures of the stars,
[00:03:33.560 --> 00:03:35.600]   it's of course even much further away.
[00:03:35.600 --> 00:03:38.040]   But then you start noticing maybe the speed of light
[00:03:38.040 --> 00:03:41.520]   would actually play some non-trivial effect.
[00:03:41.520 --> 00:03:44.240]   But anyway, so in a sense, really,
[00:03:44.240 --> 00:03:46.000]   I think it's good to pause just a little bit
[00:03:46.000 --> 00:03:48.280]   and kind of realize actually what you get out of the sensor.
[00:03:48.280 --> 00:03:51.360]   Also, if you think then video sensors, think of this,
[00:03:51.360 --> 00:03:54.600]   then also at actually quite sustained rate
[00:03:54.600 --> 00:03:57.440]   of capturing one image after the other
[00:03:57.440 --> 00:04:00.720]   in a very systematic way.
[00:04:00.720 --> 00:04:02.840]   So in a sense, you could say this here.
[00:04:02.840 --> 00:04:03.800]   So these are the cameras.
[00:04:03.800 --> 00:04:06.720]   They're the best sensor ever in many dimensions,
[00:04:06.720 --> 00:04:09.800]   if you look at it.
[00:04:09.800 --> 00:04:12.680]   Of course, that's not fully true.
[00:04:12.680 --> 00:04:14.080]   And here it's just a little--
[00:04:14.080 --> 00:04:15.920]   this is more a gimmick video.
[00:04:15.920 --> 00:04:17.920]   And I'm not sure if the sun will play.
[00:04:17.920 --> 00:04:20.400]   [INAUDIBLE]
[00:04:20.400 --> 00:04:21.760]   Let me bring it over here.
[00:04:21.760 --> 00:04:22.760]   OK.
[00:04:22.760 --> 00:04:23.440]   So our movie.
[00:04:23.440 --> 00:04:24.920]   [BEEPING]
[00:04:24.920 --> 00:04:25.920]   Focus on the drop.
[00:04:25.920 --> 00:04:33.920]   Enhance, then forward, frame by frame.
[00:04:33.920 --> 00:04:34.420]   OK.
[00:04:34.420 --> 00:04:36.120]   Some people imagine even better sensors.
[00:04:36.120 --> 00:04:37.400]   Just before the views blocked, there's
[00:04:37.400 --> 00:04:38.800]   a shape change in Dean's bag.
[00:04:38.800 --> 00:04:40.480]   See the shadow variance?
[00:04:40.480 --> 00:04:41.080]   See?
[00:04:41.080 --> 00:04:42.000]   The shadow's wrong.
[00:04:42.000 --> 00:04:45.680]   Xavits changed the configuration of Dean's packages.
[00:04:45.680 --> 00:04:47.160]   Is it a tape?
[00:04:47.160 --> 00:04:48.160]   It's hard to say for sure.
[00:04:48.160 --> 00:04:48.840]   These things are--
[00:04:48.840 --> 00:04:50.680]   Computer takes around the other side?
[00:04:50.680 --> 00:04:51.920]   It can hypothesize.
[00:04:51.920 --> 00:04:52.640]   Chris?
[00:04:52.640 --> 00:04:53.140]   Yeah?
[00:04:53.140 --> 00:04:56.200]   Can you rotate a 75 degrees around the vertical, please?
[00:04:56.200 --> 00:05:01.960]   What do you think it is?
[00:05:01.960 --> 00:05:03.480]   It looks a lot bigger than the tape.
[00:05:03.480 --> 00:05:05.560]   Xavits had digital compression equipment in his apartment.
[00:05:05.560 --> 00:05:07.400]   He could have downloaded it to anything.
[00:05:07.400 --> 00:05:09.640]   Or maybe the bag twisted in Dean's pants
[00:05:09.640 --> 00:05:11.120]   or something moved in front of light
[00:05:11.120 --> 00:05:12.240]   and then altered the shadow.
[00:05:12.240 --> 00:05:13.040]   Maybe it's nothing.
[00:05:13.040 --> 00:05:14.520]   Maybe it's everything.
[00:05:14.520 --> 00:05:17.040]   Let's get it and find out.
[00:05:17.040 --> 00:05:17.520]   OK.
[00:05:17.520 --> 00:05:18.560]   I'll leave you with that.
[00:05:18.560 --> 00:05:24.520]   Anyway, so we can't do that type of stuff, of course.
[00:05:24.520 --> 00:05:27.240]   Watch everything that happens anywhere with sensors.
[00:05:27.240 --> 00:05:31.440]   But still, so video cameras or digital cameras
[00:05:31.440 --> 00:05:33.560]   are actually quite amazing.
[00:05:33.560 --> 00:05:35.360]   But there's also plenty of problems,
[00:05:35.360 --> 00:05:37.200]   plenty of things that you might not be aware of.
[00:05:37.200 --> 00:05:39.240]   But are happening under the hood.
[00:05:39.240 --> 00:05:42.680]   And that in some circumstances can actually
[00:05:42.680 --> 00:05:43.400]   create challenges.
[00:05:43.400 --> 00:05:47.600]   Here's one example.
[00:05:47.600 --> 00:05:51.000]   There can be issues of transmission interferences.
[00:05:51.000 --> 00:05:53.080]   If you are sending the data-- so the sensor might not
[00:05:53.080 --> 00:05:53.720]   be the problem.
[00:05:53.720 --> 00:05:55.640]   But then if you are transmitting from somewhere
[00:05:55.640 --> 00:05:59.600]   else the data, there might be issues with transmitting.
[00:05:59.600 --> 00:06:04.800]   There are-- you're not necessarily in a position
[00:06:04.800 --> 00:06:07.840]   to always be able to get the actual, all the raw exact
[00:06:07.840 --> 00:06:08.400]   measurements.
[00:06:08.400 --> 00:06:11.000]   You might have to compress your data
[00:06:11.000 --> 00:06:12.200]   to be able to transmit it.
[00:06:12.200 --> 00:06:15.160]   So in many images, you might--
[00:06:15.160 --> 00:06:17.720]   instead of having the actual raw signal,
[00:06:17.720 --> 00:06:19.040]   you might have something like this
[00:06:19.040 --> 00:06:24.040]   where you might kind of notice a certain regular pattern that
[00:06:24.040 --> 00:06:27.400]   probably has nothing to do with the landscape
[00:06:27.400 --> 00:06:28.760]   that you're looking at.
[00:06:28.760 --> 00:06:31.000]   But it's just something that comes from the image.
[00:06:31.000 --> 00:06:34.040]   And image compression in particular.
[00:06:34.040 --> 00:06:36.400]   We'll see this in a few weeks.
[00:06:36.400 --> 00:06:40.360]   Why these all are-- if you would count the pixels here,
[00:06:40.360 --> 00:06:44.040]   I think these are all eight pixels or so, probably.
[00:06:44.040 --> 00:06:45.560]   Eight points by eight points or so.
[00:06:45.560 --> 00:06:47.320]   Or this is kind of a--
[00:06:47.320 --> 00:06:50.200]   you can kind of see a grid pattern showing up here.
[00:06:50.200 --> 00:06:53.360]   And that has to do with how images typically get compressed.
[00:06:53.360 --> 00:07:00.000]   OK.
[00:07:00.000 --> 00:07:01.360]   Let me switch off the sound.
[00:07:01.800 --> 00:07:10.520]   There can be issues with the pixel sensors saturating,
[00:07:10.520 --> 00:07:12.720]   in certain ways.
[00:07:12.720 --> 00:07:17.880]   So there might be a very bright point over here.
[00:07:17.880 --> 00:07:21.440]   And that could overflow neighboring pixels.
[00:07:21.440 --> 00:07:23.400]   The way that it's measured, every sensor is--
[00:07:23.400 --> 00:07:26.040]   these sensors are physical things next to each other.
[00:07:26.040 --> 00:07:30.200]   There can be situations where this can transfer.
[00:07:30.200 --> 00:07:32.760]   And you have unwanted one pixel kind of gets--
[00:07:32.760 --> 00:07:34.600]   doesn't directly get the light, but the neighbor
[00:07:34.600 --> 00:07:36.840]   has a lot of electrons way too many to contain there.
[00:07:36.840 --> 00:07:38.560]   And they spill over to neighbors.
[00:07:38.560 --> 00:07:45.720]   There can be scratches or other issues, sensor noise.
[00:07:45.720 --> 00:07:50.880]   There can be issues of bad contrast.
[00:07:50.880 --> 00:07:53.760]   It can be hard to get the contrast you want in the images
[00:07:53.760 --> 00:07:55.400]   due to a multitude of reasons.
[00:07:58.720 --> 00:08:04.080]   If you have low contrast, just somehow digitally boosting
[00:08:04.080 --> 00:08:06.000]   contrast, or in this case, maybe analog,
[00:08:06.000 --> 00:08:07.680]   in an analog way, boosting the contrast.
[00:08:07.680 --> 00:08:09.360]   But anyways, just boosting the contrast
[00:08:09.360 --> 00:08:13.000]   and trying to take the values you have and pull them apart,
[00:08:13.000 --> 00:08:15.760]   which is what boosting contrast would be,
[00:08:15.760 --> 00:08:17.760]   doesn't necessarily--
[00:08:17.760 --> 00:08:21.760]   it's not necessarily that helpful in getting a better image.
[00:08:21.760 --> 00:08:24.560]   So we'll have to kind of somehow--
[00:08:24.560 --> 00:08:27.280]   there are limits in what we can do in restoring images,
[00:08:27.280 --> 00:08:30.520]   in processing images, and so on.
[00:08:30.520 --> 00:08:32.760]   Resolution can be an issue.
[00:08:32.760 --> 00:08:35.680]   If you take a picture, you might only have this many
[00:08:35.680 --> 00:08:38.160]   measurements, and therefore only see things that
[00:08:38.160 --> 00:08:40.560]   are certain-- up to a certain resolution.
[00:08:40.560 --> 00:08:45.240]   For example, struggling to read here a license plate.
[00:08:45.240 --> 00:08:50.120]   And then you might have heard already of super resolution
[00:08:50.120 --> 00:08:53.600]   methods that somehow manage to go beyond what you can see
[00:08:53.600 --> 00:08:55.800]   in the original image or images.
[00:08:55.800 --> 00:09:03.520]   There's also there are significant limitations
[00:09:03.520 --> 00:09:06.920]   in what you can do.
[00:09:06.920 --> 00:09:11.880]   But here's an example, a little bit showing what can be done.
[00:09:11.880 --> 00:09:14.640]   So let's say you start here from a relatively low resolution,
[00:09:14.640 --> 00:09:18.720]   but a low resolution video, where if you look at it,
[00:09:18.720 --> 00:09:20.880]   most of the pictures actually really picture--
[00:09:20.880 --> 00:09:25.520]   the same picture just with a little bit of displacement.
[00:09:25.520 --> 00:09:27.360]   In a sense, that means that we're now
[00:09:27.360 --> 00:09:29.320]   having many measurements of the same thing that
[00:09:29.320 --> 00:09:31.440]   are kind of redundant measurements.
[00:09:31.440 --> 00:09:34.080]   And that can actually be leveraged,
[00:09:34.080 --> 00:09:38.000]   as you see in this video here.
[00:09:38.000 --> 00:09:41.120]   And what we'll actually see then with in terms of what
[00:09:41.120 --> 00:09:42.400]   principles we can apply.
[00:09:42.400 --> 00:09:47.000]   And we'll understand the math and the mathematical models
[00:09:47.000 --> 00:09:51.160]   underlying this and it allows us to do super resolution,
[00:09:51.160 --> 00:09:53.840]   to restore resolution that's actually not present in any
[00:09:53.840 --> 00:09:57.560]   of the images that we actually have at our disposal.
[00:09:57.560 --> 00:10:00.600]   But by combining them, we can actually
[00:10:00.600 --> 00:10:03.320]   restore some of a higher resolution.
[00:10:03.320 --> 00:10:06.920]   But we'll also see that, yes, you can maybe get a factor of 2
[00:10:06.920 --> 00:10:11.240]   or a factor of 3 with normal images coming
[00:10:11.240 --> 00:10:15.080]   from normal cameras, but that it will not be possible.
[00:10:15.080 --> 00:10:16.640]   That intrinsically, there's no hope
[00:10:16.640 --> 00:10:19.720]   to really get much beyond that because of reasons
[00:10:19.720 --> 00:10:24.600]   that you'll actually learn about in this course.
[00:10:24.600 --> 00:10:31.040]   You see here, for example.
[00:10:31.040 --> 00:10:39.200]   So you see that you can actually really boost the resolution.
[00:10:39.200 --> 00:10:41.680]   And maybe at a high level, what happens
[00:10:41.680 --> 00:10:44.040]   is because we have this slightly displaced--
[00:10:44.040 --> 00:10:48.000]   this was a kind of video that was taking slightly displaced
[00:10:48.000 --> 00:10:49.320]   images.
[00:10:49.320 --> 00:10:53.520]   In a sense, what we do is every of those measurements
[00:10:53.520 --> 00:10:55.040]   is a slightly displaced version.
[00:10:55.040 --> 00:10:57.400]   And so it's as if you were taking measurements
[00:10:57.400 --> 00:10:58.920]   at a high resolution in between.
[00:10:58.920 --> 00:11:02.480]   So an image that's shifted by half pixel
[00:11:02.480 --> 00:11:04.640]   essentially allows you to get a little bit of a view
[00:11:04.640 --> 00:11:07.800]   in between the two pixels of the original image, et cetera.
[00:11:07.800 --> 00:11:09.920]   And so if you get many measurements,
[00:11:09.920 --> 00:11:13.000]   you can start recovering a little bit the resolution,
[00:11:13.000 --> 00:11:17.160]   except that you don't actually--
[00:11:17.160 --> 00:11:19.480]   as we'll see with typical cameras,
[00:11:19.480 --> 00:11:23.240]   you actually don't get a strict point measurement, a point
[00:11:23.240 --> 00:11:27.680]   sample at one exact location, in which case you could actually,
[00:11:27.680 --> 00:11:32.000]   with infinite many images, you could recover a super resolution
[00:11:32.000 --> 00:11:34.400]   image of infinite resolution.
[00:11:34.400 --> 00:11:37.200]   But you actually get a somewhat blurred version
[00:11:37.200 --> 00:11:40.840]   because the size of the pixel actually corresponds
[00:11:40.840 --> 00:11:42.640]   to some type of blurring.
[00:11:42.640 --> 00:11:44.800]   And therefore, this will intrinsically limit
[00:11:44.800 --> 00:11:47.360]   how much resolution we can recuperate
[00:11:47.360 --> 00:11:52.320]   based on issues of how precise the signal is that we can read,
[00:11:52.320 --> 00:11:55.360]   with a limited signal to noise ratio, et cetera, et cetera.
[00:11:55.360 --> 00:11:57.960]   So all of this is probably--
[00:11:57.960 --> 00:11:59.480]   you don't have enough information yet
[00:11:59.480 --> 00:12:02.320]   to be able to really exactly understand what I'm saying now
[00:12:02.320 --> 00:12:04.440]   so that that's normal.
[00:12:04.440 --> 00:12:06.240]   But these are a little bit of different concepts
[00:12:06.240 --> 00:12:08.520]   that will come back in the coming weeks that will allow you
[00:12:08.520 --> 00:12:11.680]   to really understand what are the intrinsic limitations
[00:12:11.680 --> 00:12:12.960]   of super resolution.
[00:12:12.960 --> 00:12:26.600]   Then another effect is effect of motion blur.
[00:12:26.600 --> 00:12:28.440]   You've probably all seen if you take a picture,
[00:12:28.440 --> 00:12:31.280]   either if you move with your camera while you take the picture,
[00:12:31.280 --> 00:12:34.480]   or there's very little light and you're somehow not holding
[00:12:34.480 --> 00:12:36.920]   the camera very still, or you're zooming in very far.
[00:12:36.920 --> 00:12:39.520]   And so the image moves a lot.
[00:12:39.520 --> 00:12:46.800]   But you need a long exposure because there's not much light.
[00:12:46.800 --> 00:12:49.400]   Or you have a very fast moving object like here.
[00:12:49.400 --> 00:12:52.600]   So you have maybe a static camera,
[00:12:52.600 --> 00:12:54.000]   but there's not much light.
[00:12:54.000 --> 00:12:56.680]   The camera needs to accumulate light for a little bit of time.
[00:12:56.680 --> 00:13:00.560]   So the exposure takes a little bit of time.
[00:13:00.560 --> 00:13:03.360]   And you have a fast object that moves past the camera.
[00:13:03.360 --> 00:13:06.120]   Then that gets smeared out, essentially.
[00:13:06.120 --> 00:13:08.560]   Because essentially, what is this effect here?
[00:13:08.560 --> 00:13:11.960]   It's simply the fact that while the camera is getting light
[00:13:11.960 --> 00:13:14.440]   to come into every pixel from every direction
[00:13:14.440 --> 00:13:16.440]   and creates the image that way, it
[00:13:16.440 --> 00:13:18.880]   does that for a finite period of time.
[00:13:18.880 --> 00:13:20.720]   And during that finite period of time,
[00:13:20.720 --> 00:13:23.520]   you actually have observable motion,
[00:13:23.520 --> 00:13:25.640]   meaning that at the beginning you see the car,
[00:13:25.640 --> 00:13:29.320]   and then the car has moved on, and now you see the background.
[00:13:29.320 --> 00:13:31.040]   So the pixels over here, for example.
[00:13:31.040 --> 00:13:33.720]   So those are mixed pixels.
[00:13:33.720 --> 00:13:35.360]   Also the pixels here are essentially
[00:13:35.360 --> 00:13:40.680]   like accumulated from a whole stripe across the back of the car,
[00:13:40.680 --> 00:13:43.680]   because it kind of shifted while you were taking the picture,
[00:13:43.680 --> 00:13:46.480]   while you were accumulating incoming photons
[00:13:46.480 --> 00:13:49.440]   to form the image.
[00:13:49.440 --> 00:13:51.800]   So typically, you see here, you get something
[00:13:51.800 --> 00:13:52.960]   that's very hard to use.
[00:13:52.960 --> 00:13:57.480]   So there are possibilities.
[00:13:57.480 --> 00:13:59.880]   If you know the motion of the vehicle here,
[00:13:59.880 --> 00:14:02.040]   you know that it moved, for example,
[00:14:02.040 --> 00:14:06.320]   that you can approximate this as a simple shift by whatever.
[00:14:06.320 --> 00:14:07.840]   For example, you would have measured this,
[00:14:07.840 --> 00:14:10.800]   and it's like 30 pixel shift during the exposure,
[00:14:10.800 --> 00:14:13.160]   and it's kind of linear motion, so it's kind of shifting
[00:14:13.160 --> 00:14:14.560]   continuously.
[00:14:14.560 --> 00:14:19.440]   You can model that, and we'll see that in the coming weeks.
[00:14:19.440 --> 00:14:22.760]   You can model that, and then you can invert that process.
[00:14:22.760 --> 00:14:25.440]   But as you can see here, it becomes very noisy.
[00:14:25.440 --> 00:14:28.800]   That looked like a better quality image, but it's blurred.
[00:14:28.800 --> 00:14:31.120]   But in terms of the actual values you get,
[00:14:31.120 --> 00:14:33.000]   it's a nice move, et cetera.
[00:14:33.000 --> 00:14:39.440]   Once you do this inversion, you get a very noisy picture.
[00:14:39.440 --> 00:14:41.960]   But it's sharp now, or kind of sharp.
[00:14:41.960 --> 00:14:43.960]   You can actually read the license plate here.
[00:14:43.960 --> 00:14:49.280]   So we'll see that it's actually really challenges
[00:14:49.280 --> 00:14:52.520]   in the inversion that you lose precision,
[00:14:52.520 --> 00:14:56.880]   because we actually have to take compute small values
[00:14:56.880 --> 00:15:00.000]   and then boost them again, because this effect would
[00:15:00.000 --> 00:15:01.960]   actually have reduced what we can see.
[00:15:01.960 --> 00:15:04.160]   And then in version, we boost things
[00:15:04.160 --> 00:15:06.880]   that almost had disappeared.
[00:15:06.880 --> 00:15:09.040]   But that means that they are now much more noisy,
[00:15:09.040 --> 00:15:15.040]   because the noise kind of plays a big role there.
[00:15:15.040 --> 00:15:18.680]   Here's actually a nice-- just as an example,
[00:15:18.680 --> 00:15:23.720]   a nice paper from 15 years ago.
[00:15:23.720 --> 00:15:27.520]   Or so that was attacking this problem,
[00:15:27.520 --> 00:15:29.560]   trying to change the camera.
[00:15:29.560 --> 00:15:31.720]   Not do a posteriori algorithmically
[00:15:31.720 --> 00:15:37.000]   change the-- do some fancy image processing
[00:15:37.000 --> 00:15:40.200]   to get rid of the motion blur.
[00:15:40.200 --> 00:15:43.640]   But what they did was actually try
[00:15:43.640 --> 00:15:49.840]   to do something smart at the level of the physics before--
[00:15:49.840 --> 00:15:56.000]   like on the incoming light that was going to the sensor.
[00:15:56.000 --> 00:15:57.960]   So here they say the two classical ways
[00:15:57.960 --> 00:16:02.840]   to try to take a good picture here of a moving object,
[00:16:02.840 --> 00:16:05.520]   like in this case, a little toy train.
[00:16:05.520 --> 00:16:07.560]   So this was taken in the lab.
[00:16:07.560 --> 00:16:10.280]   Either you take a very short exposure
[00:16:10.280 --> 00:16:13.440]   to get a sharp image, but then of course, it's actually--
[00:16:13.440 --> 00:16:14.560]   there's almost no light.
[00:16:14.560 --> 00:16:15.640]   So it's pretty dark.
[00:16:15.640 --> 00:16:18.200]   And then you try to boost by raising--
[00:16:18.200 --> 00:16:20.920]   by taking every value you find here
[00:16:20.920 --> 00:16:24.640]   and multiply it by 100 or whatever it is to get a proper
[00:16:24.640 --> 00:16:27.400]   image, and you get something like this here.
[00:16:27.400 --> 00:16:30.040]   Pretty bad.
[00:16:30.040 --> 00:16:32.160]   Or you say, OK, I need more time.
[00:16:32.160 --> 00:16:35.120]   I need to accumulate more light, more information.
[00:16:35.120 --> 00:16:37.600]   And I will do essentially this here.
[00:16:37.600 --> 00:16:41.560]   So you see up there the exposure.
[00:16:41.560 --> 00:16:45.720]   This little blip there means that the camera, at that point
[00:16:45.720 --> 00:16:48.240]   in time, switches on, starts accumulating light.
[00:16:48.240 --> 00:16:50.440]   And then very quickly after, it switches off.
[00:16:50.440 --> 00:16:52.400]   And it doesn't take light anymore.
[00:16:52.400 --> 00:16:54.640]   While this one would actually switch on,
[00:16:54.640 --> 00:16:57.480]   so you open the--
[00:16:57.480 --> 00:16:59.920]   let's say you had a plate in front of the lens.
[00:16:59.920 --> 00:17:02.120]   In the first one, you take it away and immediately cover it
[00:17:02.120 --> 00:17:02.600]   again.
[00:17:02.600 --> 00:17:04.880]   In the other one, you let the light come in.
[00:17:04.880 --> 00:17:06.360]   The train is kind of moving.
[00:17:06.360 --> 00:17:08.360]   You wait a bit, then you close again.
[00:17:08.360 --> 00:17:11.800]   So that's this long exposure time.
[00:17:11.800 --> 00:17:16.880]   You get this nice amount of light, clear colors, et cetera,
[00:17:16.880 --> 00:17:18.920]   not much noise.
[00:17:18.920 --> 00:17:22.320]   But if you then need to do all the processing to remove,
[00:17:22.320 --> 00:17:23.360]   you can see two things.
[00:17:23.360 --> 00:17:25.160]   One, it's a very noisy image.
[00:17:25.160 --> 00:17:28.480]   Two, there's this kind of funny artifacts showing up here.
[00:17:28.480 --> 00:17:33.560]   There's some kind of frequency, pattern of a certain frequency
[00:17:33.560 --> 00:17:35.400]   that seems to have encountered a problem
[00:17:35.400 --> 00:17:38.200]   and that we somehow have lost or has disappeared or something.
[00:17:38.200 --> 00:17:45.080]   What they did was based on some of the concepts
[00:17:45.080 --> 00:17:48.160]   that we will see through this lecture,
[00:17:48.160 --> 00:17:50.600]   they figured out that actually we could do better
[00:17:50.600 --> 00:17:55.960]   if instead of just opening, waiting, and closing,
[00:17:55.960 --> 00:17:57.840]   we kind of accumulate the same amount of light.
[00:17:57.840 --> 00:18:00.240]   But what we do is we'll actually be opening and closing
[00:18:00.240 --> 00:18:04.320]   a number of times very fast at a certain irregular rhythm
[00:18:04.320 --> 00:18:05.400]   somehow.
[00:18:05.400 --> 00:18:08.240]   So open, close, but changing the amount of time
[00:18:08.240 --> 00:18:11.560]   they keep it open and closed and so on.
[00:18:11.560 --> 00:18:15.280]   And so they instrumented the camera to do that.
[00:18:15.280 --> 00:18:17.000]   And then what they obtained is this here.
[00:18:17.000 --> 00:18:19.640]   So you see they call it the flutter-chutter.
[00:18:19.640 --> 00:18:21.160]   Instead of having it open all the time,
[00:18:21.160 --> 00:18:23.880]   they actually are only one time very briefly,
[00:18:23.880 --> 00:18:26.200]   they do something in between which is kind of open, close,
[00:18:26.200 --> 00:18:26.720]   open, close.
[00:18:26.720 --> 00:18:28.240]   That's kind of random durations, roughly.
[00:18:28.240 --> 00:18:31.400]   Not really random, but you could kind of do it random
[00:18:31.400 --> 00:18:32.720]   and it probably work.
[00:18:32.720 --> 00:18:37.200]   Similarly, so something chosen in a certain way
[00:18:37.200 --> 00:18:39.480]   so that you don't get this loss here anymore
[00:18:39.480 --> 00:18:43.720]   at a particular given pattern, that you
[00:18:43.720 --> 00:18:48.640]   get the quality of the much better noise reconstruction,
[00:18:48.640 --> 00:18:50.560]   actually all the frequencies--
[00:18:50.560 --> 00:18:52.600]   and we'll see what that means--
[00:18:52.600 --> 00:18:54.960]   are recovered here, including the one that
[00:18:54.960 --> 00:18:57.760]   had fully disappeared here.
[00:18:57.760 --> 00:19:00.000]   And so you get a much better image or so.
[00:19:00.000 --> 00:19:01.680]   So there's also-- just to tell you
[00:19:01.680 --> 00:19:03.200]   that there's actually many situations where
[00:19:03.200 --> 00:19:05.040]   to really get an improvement, you
[00:19:05.040 --> 00:19:07.520]   need to do it before you are in a digital form
[00:19:07.520 --> 00:19:10.400]   where you intrinsically already have some limitations baked
[00:19:10.400 --> 00:19:11.640]   in your data.
[00:19:12.640 --> 00:19:13.140]   OK.
[00:19:13.140 --> 00:19:23.600]   OK, this is just for the exercises as a bit of a preview.
[00:19:23.600 --> 00:19:27.240]   So the exercises will be in Python this year
[00:19:27.240 --> 00:19:29.240]   or have been for a few years.
[00:19:29.240 --> 00:19:31.320]   They used to be in MATLAB before.
[00:19:31.320 --> 00:19:34.600]   Before that, I'm not going to do that here,
[00:19:34.600 --> 00:19:39.040]   but I actually encourage you before the first exercise
[00:19:39.040 --> 00:19:39.520]   session.
[00:19:39.520 --> 00:19:41.840]   So this week, there are no exercises.
[00:19:41.840 --> 00:19:46.280]   Just go through the slides when you get access to them
[00:19:46.280 --> 00:19:48.000]   and just play a bit around, set this up,
[00:19:48.000 --> 00:19:52.640]   kind of loading an image, show it, et cetera.
[00:19:52.640 --> 00:19:59.400]   So you can show the image, you can let it kind of display.
[00:19:59.400 --> 00:20:01.960]   Then also look at just printing the image
[00:20:01.960 --> 00:20:05.080]   or a fraction of the image, like a sub-image or so.
[00:20:05.080 --> 00:20:06.600]   And just look at the numbers.
[00:20:06.600 --> 00:20:09.400]   Just play a bit around with a digital image.
[00:20:09.400 --> 00:20:16.360]   OK, so again, let's get back to what is an image.
[00:20:16.360 --> 00:20:21.400]   An image in general, not yet a digital image.
[00:20:21.400 --> 00:20:24.240]   An image in general, you can really see it typically
[00:20:24.240 --> 00:20:26.960]   as a 2D signal.
[00:20:26.960 --> 00:20:30.000]   It's a function depending on some variable
[00:20:30.000 --> 00:20:32.680]   with physical meaning.
[00:20:32.680 --> 00:20:36.560]   So typically, the variables are x, y coordinates.
[00:20:36.560 --> 00:20:39.040]   Could also be x, y plus time.
[00:20:39.040 --> 00:20:41.160]   You could also have in medical scanning,
[00:20:41.160 --> 00:20:45.880]   you could have x, y, z, and then potentially x, y, z plus time.
[00:20:45.880 --> 00:20:50.080]   So of course, this will lead to varying amounts of data.
[00:20:50.080 --> 00:20:51.840]   Mostly in the lecture, we'll mostly
[00:20:51.840 --> 00:20:55.280]   look at the situation of two variables,
[00:20:55.280 --> 00:21:01.400]   so a two-dimensional space in which for every value,
[00:21:01.400 --> 00:21:06.480]   at the conceptual level, for the analog image--
[00:21:06.480 --> 00:21:09.200]   so the original function that we will then
[00:21:09.200 --> 00:21:11.320]   sample and represent digitally--
[00:21:11.320 --> 00:21:13.200]   but the original signal will consider it
[00:21:13.200 --> 00:21:18.480]   as a continuous function in two dimensions typically
[00:21:18.480 --> 00:21:20.800]   that represents a physical variable.
[00:21:20.800 --> 00:21:26.440]   So typically, this will be the intensity, the light intensity
[00:21:26.440 --> 00:21:28.080]   that hits the sensor.
[00:21:28.080 --> 00:21:33.880]   But these can be other things like temperature, pressure,
[00:21:33.880 --> 00:21:34.560]   et cetera.
[00:21:34.560 --> 00:21:36.920]   Here's a few examples.
[00:21:36.920 --> 00:21:41.320]   Ultrasound, an ultrasound image.
[00:21:41.320 --> 00:21:44.720]   This is actually with essentially light,
[00:21:44.720 --> 00:21:48.560]   but in the far infrared when it actually really corresponds
[00:21:48.560 --> 00:21:49.360]   to temperature.
[00:21:49.360 --> 00:21:54.640]   As we can have cameras that actually directly measure
[00:21:54.640 --> 00:21:56.880]   temperature, for example, of the surfaces
[00:21:56.880 --> 00:21:58.800]   that you look at.
[00:21:58.800 --> 00:22:02.400]   Normal camera image or here, CT, computational tomography,
[00:22:02.400 --> 00:22:04.200]   that's typically 3D volumes.
[00:22:04.200 --> 00:22:05.880]   It's often also measured in slices,
[00:22:05.880 --> 00:22:08.760]   but you often measure many slices.
[00:22:08.760 --> 00:22:11.960]   So there, you have 3D volumes typically.
[00:22:11.960 --> 00:22:14.720]   So those are just a few examples of possible images.
[00:22:14.720 --> 00:22:19.200]   Mostly in this lecture, we'll look at just classical images,
[00:22:19.200 --> 00:22:23.480]   either grayscale, just a single light in general,
[00:22:23.480 --> 00:22:27.000]   or color within three channels, red, green, and blue.
[00:22:27.000 --> 00:22:33.400]   It's also important to understand what we mean by images.
[00:22:33.400 --> 00:22:35.720]   We'll mostly talk about natural images,
[00:22:35.720 --> 00:22:39.520]   so images like this one, for example.
[00:22:39.520 --> 00:22:42.120]   This looks like a random image.
[00:22:42.120 --> 00:22:45.440]   Could be anything like a random image from the web or so.
[00:22:45.440 --> 00:22:48.800]   But actually, this is really not the random image in a strict,
[00:22:48.800 --> 00:22:50.080]   let's say, mathematical sense.
[00:22:50.080 --> 00:22:53.160]   So this would be mathematically a random image.
[00:22:53.160 --> 00:22:56.840]   Just randomly draw every pixel independently
[00:22:56.840 --> 00:23:00.800]   from some Gaussian distribution or from a uniform distribution.
[00:23:00.800 --> 00:23:02.520]   Then you get something like this here
[00:23:02.520 --> 00:23:05.800]   with essentially no correlation between neighboring pixels.
[00:23:05.800 --> 00:23:08.640]   This pixel and the next one are as correlated as this pixel
[00:23:08.640 --> 00:23:10.240]   and a pixel somewhere completely different.
[00:23:10.240 --> 00:23:11.720]   They're all independent.
[00:23:11.720 --> 00:23:14.920]   That you could call a random image.
[00:23:14.920 --> 00:23:17.880]   When we talk about images, essentially all the images
[00:23:17.880 --> 00:23:21.480]   we look at, natural images, they have already
[00:23:21.480 --> 00:23:23.160]   a lot of structure.
[00:23:23.160 --> 00:23:24.720]   We can make a lot of assumptions.
[00:23:24.720 --> 00:23:28.840]   There's a lot of statistics in the random natural image,
[00:23:28.840 --> 00:23:29.920]   an image of the real world.
[00:23:30.920 --> 00:23:34.280]   Maybe a question, why is that?
[00:23:34.280 --> 00:23:37.000]   Why is there already, what is the structure,
[00:23:37.000 --> 00:23:40.600]   what's different in natural images
[00:23:40.600 --> 00:23:43.160]   than something that looks like this here?
[00:23:43.160 --> 00:23:46.760]   >> [inaudible]
[00:23:46.760 --> 00:23:51.520]   >> Yes. So essentially,
[00:23:51.520 --> 00:23:54.040]   these textures, they are essentially expressing
[00:23:54.040 --> 00:23:57.720]   a certain, these patterns are essentially
[00:23:57.720 --> 00:24:00.280]   a certain correlation or if I see this over here,
[00:24:00.280 --> 00:24:03.360]   then there's a good chance if I have this texture going on,
[00:24:03.360 --> 00:24:07.840]   that I see, that implies some statistics on the neighbors
[00:24:07.840 --> 00:24:09.800]   if they follow that pattern.
[00:24:09.800 --> 00:24:12.360]   Also just in general,
[00:24:12.360 --> 00:24:16.120]   you see that essentially if I take a pixel here, for example,
[00:24:16.120 --> 00:24:19.080]   well many of the neighbors have roughly the same value.
[00:24:19.080 --> 00:24:22.240]   So one of the patterns is just constant, for example.
[00:24:22.240 --> 00:24:27.040]   So if you look at natural images at all the different scales,
[00:24:27.040 --> 00:24:30.680]   you see that there's a lot of correlation between neighbors.
[00:24:30.680 --> 00:24:33.400]   Both very close neighbors are very correlated,
[00:24:33.400 --> 00:24:36.200]   further away neighbors are more weakly correlated,
[00:24:36.200 --> 00:24:37.320]   but they still correlated.
[00:24:37.320 --> 00:24:38.680]   So at all the different scales,
[00:24:38.680 --> 00:24:41.160]   you have some level of correlation going on and so on.
[00:24:41.160 --> 00:24:44.240]   These are things that we'll come back to when we'll discuss
[00:24:44.240 --> 00:24:46.120]   about compressing images.
[00:24:46.120 --> 00:24:50.040]   Compressing this image, okay, it's actually impossible.
[00:24:50.040 --> 00:24:55.840]   You cannot compress this image and this is full information,
[00:24:55.840 --> 00:25:01.040]   random, like you are definitely losing signal when, you know,
[00:25:01.040 --> 00:25:03.760]   it will start like you cannot really compress this
[00:25:03.760 --> 00:25:06.480]   because it's every number, you know,
[00:25:06.480 --> 00:25:09.400]   one pixel doesn't, is not correlated with previous pixels.
[00:25:09.400 --> 00:25:11.440]   So knowing one pixel doesn't tell you anything
[00:25:11.440 --> 00:25:13.360]   about the next pixel.
[00:25:13.360 --> 00:25:16.240]   But however, because there's a lot of correlation
[00:25:16.240 --> 00:25:18.600]   in the image here, you know, here you have an orange pixel,
[00:25:18.600 --> 00:25:21.800]   well there's a lot of orange pixels around it, right, et cetera.
[00:25:21.800 --> 00:25:25.360]   So here there's shadow, well many pixels are also
[00:25:25.360 --> 00:25:26.800]   in the shadow next to it.
[00:25:26.800 --> 00:25:30.200]   All these correlations, they will give us an opportunity
[00:25:30.200 --> 00:25:34.040]   to compress images, to leverage that,
[00:25:34.040 --> 00:25:37.280]   to compress images in ways that we barely notice, okay?
[00:25:37.280 --> 00:25:44.800]   Okay, so a bit more formally, images really,
[00:25:44.800 --> 00:25:48.000]   we assume it's kind of a function defined
[00:25:48.000 --> 00:25:51.760]   over an n dimensional space of real numbers,
[00:25:51.760 --> 00:25:54.200]   so it's continuous.
[00:25:54.200 --> 00:25:58.680]   That's, in general, so it's really just a function,
[00:25:58.680 --> 00:26:01.080]   and you know, actually it can be one value,
[00:26:01.080 --> 00:26:03.040]   it can also be multiple values.
[00:26:03.040 --> 00:26:05.680]   As we discussed, if it's RGB color image,
[00:26:05.680 --> 00:26:10.080]   then we'll have three values, for example, at every location.
[00:26:10.080 --> 00:26:12.840]   In the digital form, we'll have to discretize,
[00:26:12.840 --> 00:26:15.360]   and so then we're actually first starting
[00:26:15.360 --> 00:26:16.840]   by making it finite for sure.
[00:26:16.840 --> 00:26:20.320]   I mean, here we didn't explicitly make it finite,
[00:26:20.320 --> 00:26:22.280]   but in general it will actually only talk
[00:26:22.280 --> 00:26:24.800]   about a finite domain typically.
[00:26:24.800 --> 00:26:26.160]   You know, if you have an image,
[00:26:26.160 --> 00:26:29.280]   you have a certain field of view for the image.
[00:26:29.280 --> 00:26:31.840]   So definitely in the digital form,
[00:26:31.840 --> 00:26:34.640]   it definitely has to be a finite set of numbers.
[00:26:34.640 --> 00:26:37.840]   So we have a grid, a regular grid,
[00:26:37.840 --> 00:26:40.840]   in two dimensions, for example, here,
[00:26:40.840 --> 00:26:44.320]   and so we have a countable, actually a discrete set
[00:26:44.320 --> 00:26:45.920]   of values that we'll remember,
[00:26:45.920 --> 00:26:48.440]   that we'll represent the image with.
[00:26:48.440 --> 00:26:51.160]   Okay, so typical case, n equals two,
[00:26:51.160 --> 00:26:54.600]   and we only have positive numbers here.
[00:26:54.600 --> 00:26:56.320]   Here I still have a real number.
[00:26:56.320 --> 00:26:58.600]   We'll actually see that even that we have to simplify.
[00:26:58.600 --> 00:27:00.600]   Now we could put floating numbers for each of those,
[00:27:00.600 --> 00:27:02.880]   which is a pretty good approximation to represent,
[00:27:02.880 --> 00:27:06.200]   you know, a very good approximation of any real number.
[00:27:06.200 --> 00:27:08.680]   We'll actually see that even that will really typically
[00:27:08.680 --> 00:27:12.320]   simplify and just spend a fixed number of bits
[00:27:12.320 --> 00:27:13.960]   and have an integer representation
[00:27:13.960 --> 00:27:17.920]   of the function value that we want to represent.
[00:27:20.720 --> 00:27:21.800]   Then what is a pixel?
[00:27:21.800 --> 00:27:26.800]   So etymology is picture element, okay, pixel.
[00:27:26.800 --> 00:27:32.080]   And then you have close by, you have voxel,
[00:27:32.080 --> 00:27:37.360]   as like the thing in 3D, the equivalent in 3D, et cetera.
[00:27:37.360 --> 00:27:40.040]   But okay, so a pixel, it's a point in that image,
[00:27:40.040 --> 00:27:43.560]   location in the image.
[00:27:43.560 --> 00:27:48.040]   It does, you know, it's,
[00:27:49.760 --> 00:27:51.800]   there can be a misconception that this could be
[00:27:51.800 --> 00:27:54.640]   a little square, that's not what it is.
[00:27:54.640 --> 00:27:56.440]   There's actually, especially in graphics,
[00:27:56.440 --> 00:27:58.440]   it's really important to understand that.
[00:27:58.440 --> 00:28:03.440]   So there was a whole paper dedicated to explaining this.
[00:28:03.440 --> 00:28:07.480]   Right, a pixel is not a little square,
[00:28:07.480 --> 00:28:09.680]   a pixel is not a little square,
[00:28:09.680 --> 00:28:11.360]   a pixel is not a little square.
[00:28:11.360 --> 00:28:14.680]   And by the way, a voxel is also not a little cube, okay.
[00:28:14.680 --> 00:28:17.880]   Why is that?
[00:28:17.880 --> 00:28:22.640]   Because we'll see that it's really,
[00:28:22.640 --> 00:28:25.360]   what the way we model this,
[00:28:25.360 --> 00:28:27.680]   that we go from in terms of signal processing,
[00:28:27.680 --> 00:28:30.560]   in terms of mathematically modeling things.
[00:28:30.560 --> 00:28:34.280]   We will go from a continuous function,
[00:28:34.280 --> 00:28:37.800]   we will make discrete measurements
[00:28:37.800 --> 00:28:39.600]   exactly at a single location.
[00:28:39.600 --> 00:28:41.600]   You know, a pixel is really the measurement
[00:28:41.600 --> 00:28:44.040]   that we make at one exact location,
[00:28:44.040 --> 00:28:45.800]   not over a square or so.
[00:28:46.880 --> 00:28:49.080]   This gets blurred a little bit
[00:28:49.080 --> 00:28:51.440]   when you actually look at cameras,
[00:28:51.440 --> 00:28:54.120]   which we'd like to measure at a single point,
[00:28:54.120 --> 00:28:56.440]   but hey, that doesn't quite work physically.
[00:28:56.440 --> 00:29:00.320]   You need to both in time, you know, have an exposure,
[00:29:00.320 --> 00:29:03.480]   but also in spatial extent,
[00:29:03.480 --> 00:29:06.160]   you can't have a point measurement physically
[00:29:06.160 --> 00:29:07.400]   in your camera.
[00:29:07.400 --> 00:29:09.240]   So you have a little bit of,
[00:29:09.240 --> 00:29:12.360]   you use a bit of space on the sensor array
[00:29:12.360 --> 00:29:16.440]   to integrate light over a certain spatial area.
[00:29:17.080 --> 00:29:21.680]   Still, the way we'll represent things,
[00:29:21.680 --> 00:29:23.840]   mathematically represent things,
[00:29:23.840 --> 00:29:25.680]   we are still really talking about
[00:29:25.680 --> 00:29:27.600]   single point measurements.
[00:29:27.600 --> 00:29:29.200]   But then to model the fact that
[00:29:29.200 --> 00:29:31.920]   that this actually integrated light
[00:29:31.920 --> 00:29:33.480]   over a certain region,
[00:29:33.480 --> 00:29:36.400]   we will actually, we will have a way to model this
[00:29:36.400 --> 00:29:38.560]   with an arbitrary shape,
[00:29:38.560 --> 00:29:40.440]   so it's anyways not always,
[00:29:40.440 --> 00:29:41.960]   it's anyways not exactly a square,
[00:29:41.960 --> 00:29:44.040]   and there are a number of reasons for that.
[00:29:44.040 --> 00:29:47.880]   Even if it looks like on the CCD or the sensor array,
[00:29:47.880 --> 00:29:49.600]   the CCD or CMOS array,
[00:29:49.600 --> 00:29:52.000]   it's approximated by a square.
[00:29:52.000 --> 00:29:54.040]   For mathematical modeling purposes,
[00:29:54.040 --> 00:29:56.520]   we are strictly talking about a point measurement.
[00:29:56.520 --> 00:29:59.800]   But a point measurement potentially of a function
[00:29:59.800 --> 00:30:02.760]   that takes into account the shape of,
[00:30:02.760 --> 00:30:06.320]   you know, not the original light coming in,
[00:30:06.320 --> 00:30:07.400]   but that light,
[00:30:07.400 --> 00:30:09.960]   and you know, again, I'll use a word
[00:30:09.960 --> 00:30:11.560]   that we'll actually only define
[00:30:12.960 --> 00:30:15.120]   two weeks from now or so,
[00:30:15.120 --> 00:30:20.120]   but a light convolved with the shape of your pixel,
[00:30:20.120 --> 00:30:22.600]   with, of your, you know, of your sensor.
[00:30:22.600 --> 00:30:26.640]   So that way we'll still be strictly talking
[00:30:26.640 --> 00:30:28.080]   about single point measurements,
[00:30:28.080 --> 00:30:29.960]   a single measurement,
[00:30:29.960 --> 00:30:31.360]   and not an integral,
[00:30:31.360 --> 00:30:33.760]   but in a sense that integral will be incorporated
[00:30:33.760 --> 00:30:37.200]   in that convolution operation
[00:30:37.200 --> 00:30:40.280]   that represents the shape of the pixel measurement.
[00:30:42.160 --> 00:30:45.400]   So in order to know for the purposes that we,
[00:30:45.400 --> 00:30:46.600]   that we discussed and for,
[00:30:46.600 --> 00:30:49.720]   for strictly being able to mathematically handle all of this,
[00:30:49.720 --> 00:30:52.480]   we're talking about point measurements.
[00:30:52.480 --> 00:30:53.320]   Okay.
[00:30:53.320 --> 00:30:55.320]   So here an example,
[00:30:55.320 --> 00:30:57.920]   so point measurement and then as we do,
[00:30:57.920 --> 00:30:59.920]   so this is then our digital representation,
[00:30:59.920 --> 00:31:03.280]   it's a number of a discrete set of point measurements.
[00:31:03.280 --> 00:31:05.960]   And then as we want to reconstruct the signal,
[00:31:05.960 --> 00:31:10.320]   because, well, this is all good and well
[00:31:10.320 --> 00:31:12.560]   to have a digital representation,
[00:31:12.560 --> 00:31:16.280]   but now if I want to represent that image on my screen,
[00:31:16.280 --> 00:31:19.000]   potentially my screen has a different resolution
[00:31:19.000 --> 00:31:20.720]   than this grid.
[00:31:20.720 --> 00:31:22.040]   So now I actually,
[00:31:22.040 --> 00:31:23.200]   if I want to show it on the screen,
[00:31:23.200 --> 00:31:24.440]   I need to have a function,
[00:31:24.440 --> 00:31:27.680]   a mathematically well defined way to define,
[00:31:27.680 --> 00:31:31.760]   like if now I have on my screen a pixel over here,
[00:31:31.760 --> 00:31:34.640]   what value, what color should I put at that pixel?
[00:31:34.640 --> 00:31:36.280]   Right, that's not necessarily exactly
[00:31:36.280 --> 00:31:38.520]   at one of my sample locations.
[00:31:38.520 --> 00:31:41.200]   So one thing is to take an image,
[00:31:41.200 --> 00:31:43.320]   the two dimensional continuous function
[00:31:43.320 --> 00:31:47.060]   and measure it at a number of discrete locations,
[00:31:47.060 --> 00:31:50.240]   potentially having modified that function
[00:31:50.240 --> 00:31:52.040]   to take the sampling,
[00:31:52.040 --> 00:31:54.400]   the measurement sensor shape into a condense on,
[00:31:54.400 --> 00:31:56.560]   but anyways, so point measurements there,
[00:31:56.560 --> 00:31:59.040]   we store those point measurements on a grid,
[00:31:59.040 --> 00:32:01.200]   but then in the later stage,
[00:32:01.200 --> 00:32:03.600]   we might want to resample it
[00:32:03.600 --> 00:32:07.080]   so that I can now display it on a different grid,
[00:32:07.080 --> 00:32:09.040]   for example, my computer screen,
[00:32:09.040 --> 00:32:12.120]   or I might want to print out the picture
[00:32:12.120 --> 00:32:13.240]   or whatever it is, right,
[00:32:13.240 --> 00:32:15.560]   that's whatever resolution now,
[00:32:15.560 --> 00:32:19.040]   I need to reconstruct or to be able to at any point
[00:32:19.040 --> 00:32:21.240]   in the whole continuous 2D space,
[00:32:21.240 --> 00:32:23.720]   be able to reconstruct mathematically
[00:32:23.720 --> 00:32:27.640]   by combining numbers and computing a function
[00:32:27.640 --> 00:32:30.320]   that takes a number of neighbors into account or so,
[00:32:30.320 --> 00:32:32.920]   or potentially all of my values into account,
[00:32:32.920 --> 00:32:35.200]   I need to be able to now have a function
[00:32:35.200 --> 00:32:38.360]   that reconstructs the value at that particular given location.
[00:32:38.360 --> 00:32:41.440]   Here you see, you could kind of say,
[00:32:41.440 --> 00:32:46.000]   well, for this value here with a reconstruction filter
[00:32:46.000 --> 00:32:48.680]   that takes for this value would kind of apply it,
[00:32:48.680 --> 00:32:54.480]   in a sense, a pixel here would be impacted by this one
[00:32:54.480 --> 00:32:56.840]   or a pixel here would be impact by this one and this one
[00:32:56.840 --> 00:32:58.720]   and actually also those two, et cetera.
[00:32:58.720 --> 00:33:00.440]   So in a sense, is the sphere of influence
[00:33:00.440 --> 00:33:02.760]   of a particular measurement can be typically
[00:33:02.760 --> 00:33:07.760]   for practical reasons will have a finite range
[00:33:07.760 --> 00:33:10.520]   that measurements will be taken into account
[00:33:10.520 --> 00:33:12.000]   to do the reconstruction.
[00:33:12.000 --> 00:33:14.600]   But again, we'll talk a lot more about that later.
[00:33:14.600 --> 00:33:17.480]   Here's for example, cubic reconstruction filter might,
[00:33:17.480 --> 00:33:22.200]   this point might be applied in this whole large region here
[00:33:22.200 --> 00:33:24.520]   would have an influence on our reconstruction.
[00:33:24.520 --> 00:33:28.000]   So this point here would take this measurement,
[00:33:28.000 --> 00:33:29.960]   this measurement, also other measurements,
[00:33:29.960 --> 00:33:31.160]   lots of measurements into account
[00:33:31.160 --> 00:33:34.160]   to actually compute the value at that given location.
[00:33:34.160 --> 00:33:37.600]   Again, this is from that paper, right?
[00:33:37.600 --> 00:33:40.800]   In a sense, notice that also on the reconstruction side
[00:33:40.800 --> 00:33:44.640]   or if you print something, for example,
[00:33:44.640 --> 00:33:47.840]   you might not actually try to exactly print the function
[00:33:47.840 --> 00:33:49.960]   that we're talking about, but just print something
[00:33:49.960 --> 00:33:54.960]   that's also a variant of it, like here with some dittering
[00:33:54.960 --> 00:33:58.440]   or some printing techniques that from close by,
[00:33:58.440 --> 00:34:01.120]   this doesn't look at all like the function you wanna render.
[00:34:01.480 --> 00:34:05.160]   But it turns out that our eyes also are not perfect sampling,
[00:34:05.160 --> 00:34:06.720]   infinite resolution, everything.
[00:34:06.720 --> 00:34:09.480]   If I look at these things from a little bit further away,
[00:34:09.480 --> 00:34:12.680]   would actually, or I will also in a sense do some kind of
[00:34:12.680 --> 00:34:15.480]   blurring in operations that this will actually look okay,
[00:34:15.480 --> 00:34:21.480]   to our eyes, essentially.
[00:34:21.480 --> 00:34:22.600]   It would kind of look like this,
[00:34:22.600 --> 00:34:26.000]   even if that's printed on the paper,
[00:34:26.000 --> 00:34:27.760]   we will see something like this here.
[00:34:29.200 --> 00:34:34.200]   So our eyes also are not strict measurement.
[00:34:34.200 --> 00:34:37.080]   So where do we just come from?
[00:34:37.080 --> 00:34:38.280]   So of course, digital cameras,
[00:34:38.280 --> 00:34:41.560]   and that's what we'll focus on in this part of the lecture.
[00:34:41.560 --> 00:34:44.720]   They can also come from other devices like MRI scanners
[00:34:44.720 --> 00:34:46.080]   or CT scanners.
[00:34:46.080 --> 00:34:47.760]   Of course, also computer graphics,
[00:34:47.760 --> 00:34:51.600]   and that's what you'll spend the second half of this course on,
[00:34:51.600 --> 00:34:55.480]   and also additional types of sensors and so on that,
[00:34:55.480 --> 00:34:57.640]   like a laser range finder could be scanning the world
[00:34:57.640 --> 00:35:00.040]   with a laser, and then some mirrors or other things
[00:35:00.040 --> 00:35:01.840]   that move the laser around so that it scans
[00:35:01.840 --> 00:35:04.200]   a two dimensional pattern, for example.
[00:35:04.200 --> 00:35:10.360]   So as I said, we'll focus on images, camera images.
[00:35:10.360 --> 00:35:13.520]   Here's kind of a, you know,
[00:35:13.520 --> 00:35:15.520]   schematics of a typical camera.
[00:35:15.520 --> 00:35:17.440]   Light comes in.
[00:35:17.440 --> 00:35:22.840]   In a sense, conceptually, we have a pinhole camera.
[00:35:22.840 --> 00:35:25.560]   So conceptually, we have light coming from a certain direction.
[00:35:25.560 --> 00:35:29.000]   It goes through a point, and it projects onto the sensor
[00:35:29.000 --> 00:35:30.200]   array behind the camera.
[00:35:30.200 --> 00:35:33.360]   In practice, if we just had a pinhole camera
[00:35:33.360 --> 00:35:35.840]   with just this very small hole,
[00:35:35.840 --> 00:35:37.360]   there again, we don't get enough light
[00:35:37.360 --> 00:35:39.360]   to actually get a good signal on the back.
[00:35:39.360 --> 00:35:42.200]   So the way we compensate that is we put a lens.
[00:35:42.200 --> 00:35:44.240]   What's the function of the lens is to,
[00:35:44.240 --> 00:35:46.120]   you know, if you have a lens of, let's say, this size,
[00:35:46.120 --> 00:35:47.960]   of course, you know, they're much smaller here,
[00:35:47.960 --> 00:35:50.960]   but they're still like, you know, reasonable size.
[00:35:50.960 --> 00:35:53.200]   Let's say I have a lens like this,
[00:35:53.200 --> 00:35:55.920]   then what happens is that from a particular point in space,
[00:35:55.920 --> 00:35:58.080]   all of the light that comes on the lens
[00:35:58.080 --> 00:36:00.800]   from a particular, you know, point,
[00:36:00.800 --> 00:36:04.400]   ideally will be refocused and all land on a single pixel.
[00:36:04.400 --> 00:36:09.320]   And to make that really work out,
[00:36:09.320 --> 00:36:13.280]   you actually have to choose what distance you're focused on.
[00:36:13.280 --> 00:36:14.840]   But if you're focused on the right distance,
[00:36:14.840 --> 00:36:15.960]   then this would actually really work.
[00:36:15.960 --> 00:36:18.760]   So you would have all the points at a certain distance
[00:36:18.760 --> 00:36:21.480]   would kind of, you know, spread out to the lens here,
[00:36:21.480 --> 00:36:23.880]   and then all those points would really be focused.
[00:36:23.880 --> 00:36:26.880]   For simplicity here, let's say that it's a point
[00:36:26.880 --> 00:36:27.840]   that's really far away.
[00:36:27.840 --> 00:36:32.000]   So all the incoming rays from that point are parallel, right?
[00:36:32.000 --> 00:36:35.240]   So that's when your camera is focused at infinity.
[00:36:35.240 --> 00:36:37.040]   So you have parallel incoming rays,
[00:36:37.040 --> 00:36:40.560]   and then those rays would be focused, not like here,
[00:36:40.560 --> 00:36:43.120]   but would be focused essentially for a particular direction.
[00:36:43.120 --> 00:36:47.040]   For example, all focused exactly over there on that pixel.
[00:36:47.040 --> 00:36:49.680]   And notice exactly it's actually already good enough
[00:36:49.680 --> 00:36:52.560]   if it all falls within the same little pixel area.
[00:36:52.560 --> 00:36:56.280]   So we don't need to have it exactly, exactly focused,
[00:36:56.280 --> 00:36:58.840]   but, you know, let's say within the size of a pixel.
[00:36:58.840 --> 00:37:01.000]   So this depends a bit on the resolution of your camera,
[00:37:01.000 --> 00:37:04.600]   how, you know, the lens and the lens focusing
[00:37:04.600 --> 00:37:06.680]   needs to match also in a sense,
[00:37:06.680 --> 00:37:08.240]   the resolution of your camera.
[00:37:08.240 --> 00:37:09.720]   There's some relationship there.
[00:37:09.720 --> 00:37:11.280]   Or at least the lens should be good enough
[00:37:11.280 --> 00:37:13.680]   to actually allow you to see every pixel independently
[00:37:13.680 --> 00:37:16.760]   and not have a blurry mess on your pixels
[00:37:16.760 --> 00:37:18.680]   and have anyways 20 pixels next to each other,
[00:37:18.680 --> 00:37:20.400]   and then we'll see the same thing, yes.
[00:37:20.400 --> 00:37:21.680]   - [Man] The rays look high away,
[00:37:21.680 --> 00:37:24.000]   and still miss the grid, right?
[00:37:24.000 --> 00:37:25.320]   - The what?
[00:37:25.320 --> 00:37:27.120]   - [Man] The rays with a high wavelength
[00:37:27.120 --> 00:37:30.200]   will still miss the grid then, right?
[00:37:30.200 --> 00:37:32.720]   Could they have a wavelength that's longer
[00:37:32.720 --> 00:37:34.560]   than the grid has size?
[00:37:34.560 --> 00:37:41.000]   - I'm not sure, I think that's actually still,
[00:37:41.000 --> 00:37:44.200]   I mean, if you really get into the fraction issues
[00:37:44.200 --> 00:37:45.480]   and stuff like that on there,
[00:37:45.480 --> 00:37:46.720]   but that's really something different.
[00:37:46.720 --> 00:37:48.640]   That's really not what I'm talking about here.
[00:37:48.640 --> 00:37:51.200]   This is kind of just geometrically here.
[00:37:51.200 --> 00:37:54.600]   We're not going to the point where this becomes an issue.
[00:37:54.600 --> 00:37:58.120]   So I don't know if you're like from the physics department
[00:37:58.120 --> 00:38:00.040]   or something like that, or...
[00:38:00.040 --> 00:38:05.040]   - [Man] Okay, well, so there are certainly ranges where,
[00:38:05.040 --> 00:38:07.280]   there are situations where this becomes an issue.
[00:38:07.280 --> 00:38:09.680]   This is not an issue in the normal ranges
[00:38:09.680 --> 00:38:11.880]   that we'll talk about in this course or something.
[00:38:11.880 --> 00:38:12.720]   - [Man] Remember, this is one of the things
[00:38:12.720 --> 00:38:14.680]   that happens when you stare at the sun,
[00:38:14.680 --> 00:38:16.440]   which is bad for your right to know.
[00:38:16.440 --> 00:38:18.440]   - Well, that's bad in many ways.
[00:38:18.440 --> 00:38:19.280]   So I wouldn't do it.
[00:38:19.280 --> 00:38:22.480]   Anyway, so okay, so we'll assume,
[00:38:22.480 --> 00:38:24.560]   similarly geometrically that, you know,
[00:38:24.560 --> 00:38:27.120]   rays from a certain, that come from points in the scene
[00:38:27.120 --> 00:38:29.520]   that we'll be able to focus them,
[00:38:29.520 --> 00:38:31.000]   you know, on a point here,
[00:38:31.000 --> 00:38:34.080]   on a single sensor element here.
[00:38:34.080 --> 00:38:37.160]   And then we'll have, a program will go more in detail,
[00:38:37.160 --> 00:38:38.920]   but for each of those here,
[00:38:38.920 --> 00:38:41.680]   they will go through an analog digital conversion.
[00:38:41.680 --> 00:38:42.800]   So ADC here.
[00:38:42.800 --> 00:38:47.360]   So that we get from actually just having a bunch of electrons,
[00:38:47.360 --> 00:38:51.160]   we'll actually get to a number representing digital form.
[00:38:51.160 --> 00:38:55.760]   And then we store that in a memory or image array, right?
[00:38:55.760 --> 00:38:58.000]   We'll store this somewhere in memory.
[00:38:58.000 --> 00:39:04.800]   Here's a slightly, you know, more specific example here
[00:39:04.800 --> 00:39:10.160]   of that where you see essentially this pixel array.
[00:39:10.160 --> 00:39:13.240]   You kind of also notice that there's something funny
[00:39:13.240 --> 00:39:14.080]   going on here.
[00:39:14.080 --> 00:39:18.840]   We have these little color filters in front of pixels.
[00:39:18.840 --> 00:39:23.560]   So we have actually two greens, one red and one blue.
[00:39:23.560 --> 00:39:28.120]   This is actually one of the mechanisms
[00:39:28.120 --> 00:39:30.800]   that allows us to generate color images
[00:39:30.800 --> 00:39:34.040]   is that we'll actually, the incoming light,
[00:39:34.040 --> 00:39:35.640]   in front of every pixel,
[00:39:35.640 --> 00:39:37.320]   on front of one will only keep green,
[00:39:37.320 --> 00:39:39.240]   in front of the other will only keep red, et cetera.
[00:39:39.240 --> 00:39:43.720]   And so that way we'll be able to regenerate color pictures.
[00:39:43.720 --> 00:39:44.720]   It's not a perfect mechanism,
[00:39:44.720 --> 00:39:46.880]   it's actually the cheapest and simplest one.
[00:39:46.880 --> 00:39:48.960]   And so it's the one that's used pretty much
[00:39:48.960 --> 00:39:51.200]   in all kind of consumer cameras or so.
[00:39:51.200 --> 00:39:53.800]   But we'll talk about this a little bit later in the lecture.
[00:39:53.800 --> 00:40:00.160]   Very conceptually, to give you some intuition,
[00:40:00.160 --> 00:40:03.960]   do a little bit of how a CCD camera works.
[00:40:03.960 --> 00:40:06.400]   CCD stands for charge couple devices.
[00:40:06.400 --> 00:40:10.720]   And so, imagine that's your, you know,
[00:40:10.720 --> 00:40:12.840]   behind your lens, this is your sensor,
[00:40:12.840 --> 00:40:14.560]   and you have like raindrops falling,
[00:40:14.560 --> 00:40:15.960]   and you know, let's say the image would be
[00:40:15.960 --> 00:40:17.520]   that in some areas there's a lot more rain
[00:40:17.520 --> 00:40:19.240]   than in others and so on.
[00:40:19.240 --> 00:40:23.080]   And so buckets will be filled in different amounts.
[00:40:23.080 --> 00:40:27.800]   And then essentially those buckets to be read out,
[00:40:27.800 --> 00:40:29.760]   they'll actually be moved over here,
[00:40:29.760 --> 00:40:33.960]   fill one of those, so essentially they'll move all one motion
[00:40:33.960 --> 00:40:37.200]   this way, this will empty this one in here.
[00:40:37.200 --> 00:40:41.120]   And then once that happens, this whole line moves this way,
[00:40:41.120 --> 00:40:45.680]   one by one you will have this bucket, but in here,
[00:40:45.680 --> 00:40:48.280]   you measure, so there's a measurement unit,
[00:40:48.280 --> 00:40:50.240]   you measure this is the analog digital conversion,
[00:40:50.240 --> 00:40:52.360]   right, the model they for.
[00:40:52.360 --> 00:40:54.560]   And so you read the whole line out,
[00:40:54.560 --> 00:40:56.680]   you have all the numbers now for that line,
[00:40:56.680 --> 00:40:58.840]   and then you move all of those one more line,
[00:40:58.840 --> 00:41:01.120]   so you copy one more line from the image area
[00:41:01.120 --> 00:41:04.760]   to this line here to read out, and you digitize, et cetera.
[00:41:04.760 --> 00:41:07.600]   So every time you move line by line,
[00:41:07.600 --> 00:41:09.320]   you can already imagine that while you do that,
[00:41:09.320 --> 00:41:11.720]   it keeps raining, so this is actually an issue
[00:41:11.720 --> 00:41:14.600]   that we'll see later is that this can perturb images
[00:41:14.600 --> 00:41:15.440]   a little bit.
[00:41:15.440 --> 00:41:21.280]   So image area can be quite large, not that large in here,
[00:41:21.280 --> 00:41:23.880]   but in digital cameras, they can actually be really
[00:41:23.880 --> 00:41:25.760]   physically this kind of size.
[00:41:25.760 --> 00:41:33.640]   So it's really electric charges that you get in these buckets,
[00:41:33.640 --> 00:41:36.280]   and the charges are essentially really proportional,
[00:41:36.280 --> 00:41:37.800]   and by that I really mean proportional,
[00:41:37.800 --> 00:41:41.040]   so it's actually a linear response.
[00:41:41.040 --> 00:41:44.320]   So for if you have three times as much light,
[00:41:44.320 --> 00:41:47.640]   you will have three times as many electrons popping up
[00:41:47.640 --> 00:41:48.840]   in those buckets.
[00:41:48.840 --> 00:41:54.760]   And then you convert them, so let's say here,
[00:41:54.760 --> 00:41:57.240]   the example is go like this, you go to the
[00:41:57.240 --> 00:42:00.480]   analog digital conversion, this is then the translation
[00:42:00.480 --> 00:42:05.320]   from the electrons to actual digital numbers,
[00:42:05.320 --> 00:42:07.600]   then you read the next one in, oops, sorry,
[00:42:07.600 --> 00:42:10.880]   you read the next one in here, you shift through further,
[00:42:10.880 --> 00:42:12.200]   you read the next one, et cetera,
[00:42:12.200 --> 00:42:14.480]   and that's the way you build up the image.
[00:42:14.480 --> 00:42:19.840]   They can be fixed here that are problematic,
[00:42:19.840 --> 00:42:22.600]   so for example, if you have something that's really
[00:42:22.600 --> 00:42:24.800]   many, many times brighter than one you can actually
[00:42:24.800 --> 00:42:29.800]   contain in a single bucket, you will tend to get
[00:42:29.800 --> 00:42:33.120]   these tricks here because of the fact that this is
[00:42:33.120 --> 00:42:37.240]   set up in a way that you can actually, with current,
[00:42:37.240 --> 00:42:39.920]   you can actually move from one bucket to the other,
[00:42:39.920 --> 00:42:42.520]   it means that they're not fully isolated between
[00:42:42.520 --> 00:42:44.920]   the different sensing elements, because they actually
[00:42:44.920 --> 00:42:47.840]   have to be able to move things from one to the next
[00:42:47.840 --> 00:42:50.160]   to measure it, it means that you have the risk to
[00:42:50.160 --> 00:42:53.240]   if you have a really strong saturation in one location,
[00:42:53.240 --> 00:42:55.560]   and way too many electrons there that they just flood
[00:42:55.560 --> 00:42:58.680]   over everything and the whole, but the whole vertical line.
[00:42:58.680 --> 00:43:00.680]   If you see this, it means that the electrons actually
[00:43:00.680 --> 00:43:03.800]   transferred in the vertical direction, right?
[00:43:03.800 --> 00:43:05.760]   It tells you something about the orientation of your
[00:43:05.760 --> 00:43:07.440]   sensor, in a sense.
[00:43:07.440 --> 00:43:14.600]   You can also have some of these bleeding and smearing,
[00:43:14.600 --> 00:43:19.600]   which would come from the fact that you keep measuring,
[00:43:19.600 --> 00:43:22.520]   so if you have in particular very short exposure times,
[00:43:22.520 --> 00:43:27.280]   so that in the end, the time it takes to read out
[00:43:27.280 --> 00:43:30.520]   the image, which is kind of a fixed time that it takes
[00:43:30.520 --> 00:43:33.920]   to shift through the whole image, that that time is about
[00:43:33.920 --> 00:43:36.520]   of the same order as the time you actually
[00:43:36.520 --> 00:43:39.840]   exposed your image, then that kind of, you know,
[00:43:39.840 --> 00:43:42.760]   think of the rain there, you expose a little bit for it
[00:43:42.760 --> 00:43:44.840]   to get some rain, and then while you move through,
[00:43:44.840 --> 00:43:46.840]   it takes about the same amount of time, so you're gonna have
[00:43:46.840 --> 00:43:48.800]   still the same amount of rain that still falls through
[00:43:48.800 --> 00:43:51.600]   in the wrong buckets, because you are actually moving out
[00:43:51.600 --> 00:43:53.640]   and shifting through those buckets.
[00:43:53.640 --> 00:43:56.360]   That's kind of these lines that you see here,
[00:43:56.360 --> 00:43:59.040]   this brighter, I don't know how much contrast there is,
[00:43:59.040 --> 00:44:01.160]   but you see like brighter stuff over here,
[00:44:01.160 --> 00:44:03.280]   this is due to this type of issues.
[00:44:03.280 --> 00:44:07.520]   Here's something like dark current.
[00:44:07.520 --> 00:44:14.440]   The sensors, they also actually based on temperature,
[00:44:14.440 --> 00:44:20.560]   they will actually generate by themselves without any light,
[00:44:20.560 --> 00:44:23.360]   they will generate some electrons, okay?
[00:44:23.360 --> 00:44:25.040]   And this can actually evolve over time,
[00:44:25.040 --> 00:44:27.680]   so this is here from a satellite, I think,
[00:44:27.680 --> 00:44:30.760]   nine years apart, initially it was like this,
[00:44:30.760 --> 00:44:34.160]   after a long time, it was actually had more of this problem,
[00:44:34.160 --> 00:44:37.760]   and so this resulted in degraded capability
[00:44:37.760 --> 00:44:42.760]   to take high quality images for this satellite here over time.
[00:44:42.760 --> 00:44:51.640]   So the dark current is in CCDs,
[00:44:51.640 --> 00:44:54.200]   it's thermally generated charges,
[00:44:54.200 --> 00:44:56.760]   they give non-zero output, even in darkness,
[00:44:57.760 --> 00:45:01.760]   so this is dark current, and it also is not,
[00:45:01.760 --> 00:45:08.280]   okay, it's not perfectly deterministic.
[00:45:08.280 --> 00:45:11.920]   So maybe just to wrap up before the break,
[00:45:11.920 --> 00:45:15.760]   any idea, any suggestions how we can essentially
[00:45:15.760 --> 00:45:17.400]   reduce this dark current?
[00:45:17.400 --> 00:45:20.360]   Sorry?
[00:45:20.360 --> 00:45:23.080]   What do you mean by that?
[00:45:24.360 --> 00:45:29.360]   Maybe compress the image and then try to decompress it again?
[00:45:29.360 --> 00:45:34.240]   That's not gonna do too much about it.
[00:45:34.240 --> 00:45:36.080]   You had a--
[00:45:36.080 --> 00:45:37.080]   - The camera?
[00:45:37.080 --> 00:45:38.400]   - Sorry?
[00:45:38.400 --> 00:45:40.520]   Cool down the camera, yes, so you read
[00:45:40.520 --> 00:45:42.160]   that it was thermally generated, so yes,
[00:45:42.160 --> 00:45:44.400]   so the cooler it is, the less you will have from it,
[00:45:44.400 --> 00:45:46.040]   and the warmer it is, the worse,
[00:45:46.040 --> 00:45:49.240]   so that's a very good point, yes?
[00:45:49.240 --> 00:45:51.480]   - Professional thing, like if it's less than,
[00:45:51.480 --> 00:45:53.600]   maybe say five electrons, probably,
[00:45:53.600 --> 00:45:55.120]   maybe you have to ignore it, or do you mean
[00:45:55.120 --> 00:45:56.040]   for pressure to--
[00:45:56.040 --> 00:45:59.480]   - Maybe, I mean, yes, but that also means
[00:45:59.480 --> 00:46:02.040]   that then small signals you will also fail to register,
[00:46:02.040 --> 00:46:06.560]   so it's not great, same thing, okay?
[00:46:06.560 --> 00:46:09.920]   So one other thing that can be done is actually,
[00:46:09.920 --> 00:46:14.040]   so it's partially, it fluked to it randomly,
[00:46:14.040 --> 00:46:16.960]   but it's not fully random, right, so it still has,
[00:46:16.960 --> 00:46:21.280]   so therefore, what people are doing is they were taking
[00:46:21.280 --> 00:46:23.560]   dark images, so they would actually close the camera,
[00:46:23.560 --> 00:46:27.040]   make sure there's no light coming in, and take a picture,
[00:46:27.040 --> 00:46:30.320]   and then they could actually subtract that picture,
[00:46:30.320 --> 00:46:32.520]   or they could take a few of those, average those,
[00:46:32.520 --> 00:46:34.840]   and so get an average, so it fluked to it,
[00:46:34.840 --> 00:46:37.040]   but it still fluked to it around some values,
[00:46:37.040 --> 00:46:39.480]   and so they could take a dark image,
[00:46:39.480 --> 00:46:41.400]   and essentially subtract that from every image,
[00:46:41.400 --> 00:46:42.640]   and that's also a way to cope with it,
[00:46:42.640 --> 00:46:45.480]   but indeed, the cleanest way is to actually,
[00:46:45.480 --> 00:46:47.920]   really cool down your sensor, and actually,
[00:46:47.920 --> 00:46:52.160]   high quality in observatories and so on,
[00:46:52.160 --> 00:46:54.720]   where they really want the best possible images,
[00:46:54.720 --> 00:46:58.200]   they would actually really cool down actively their sensors
[00:46:58.200 --> 00:46:59.800]   to get the best possible thing.
[00:46:59.800 --> 00:47:02.440]   It also means that a satellite in space, in outer space,
[00:47:02.440 --> 00:47:04.240]   as long as it's not in direct sunlight and so on,
[00:47:04.240 --> 00:47:06.520]   it's in pretty good shape already in general,
[00:47:06.520 --> 00:47:11.160]   because it could actually be, it would be very cold out there.
[00:47:11.160 --> 00:47:14.240]   Okay, so we'll have to break now, and resume after that.
[00:47:15.320 --> 00:47:17.280]   Okay, so we'll continue.
[00:47:17.280 --> 00:47:23.440]   Maybe also hear something that's kind of interesting,
[00:47:23.440 --> 00:47:30.400]   is this is now about wavelength,
[00:47:30.400 --> 00:47:34.600]   proper wavelengths of the light,
[00:47:34.600 --> 00:47:38.000]   which ranges visible to the human eye,
[00:47:38.000 --> 00:47:43.000]   is about here, about 500 to 700 or so,
[00:47:45.000 --> 00:47:47.600]   so that the eyes can kind of see the light well.
[00:47:47.600 --> 00:47:51.640]   Within that we'll see, we have like three different ways to,
[00:47:51.640 --> 00:47:53.760]   we can perceive three different shades here,
[00:47:53.760 --> 00:47:55.080]   red, green and blue kind of.
[00:47:55.080 --> 00:47:59.520]   CCDs actually perceive light in a much larger area,
[00:47:59.520 --> 00:48:03.320]   and actually also can be actually very, very effective
[00:48:03.320 --> 00:48:07.600]   in terms of efficiency of picking up even all the way
[00:48:07.600 --> 00:48:12.600]   to single photons, so the efficiency of conversion
[00:48:13.520 --> 00:48:15.880]   photons to electrons is very high.
[00:48:15.880 --> 00:48:22.440]   Anyway, so also now that more and more sensors
[00:48:22.440 --> 00:48:26.520]   actually not CCDs, this was more in the past,
[00:48:26.520 --> 00:48:29.840]   now it's mostly all CMOS sensors,
[00:48:29.840 --> 00:48:31.560]   it's actually the same technology,
[00:48:31.560 --> 00:48:34.720]   or the same intrinsically same sensor element,
[00:48:34.720 --> 00:48:37.280]   so essentially photo diodes that will convert
[00:48:37.280 --> 00:48:42.200]   incoming photons into electrons.
[00:48:43.120 --> 00:48:48.120]   With the CMOS, each photo sensor has its own amplifier,
[00:48:48.120 --> 00:48:51.960]   here also there is a need to reduce,
[00:48:51.960 --> 00:48:56.280]   to subtracting a black image, also in the sense of,
[00:48:56.280 --> 00:48:59.880]   so there is a pattern of noise that's fixed,
[00:48:59.880 --> 00:49:01.840]   that you can actually remove.
[00:49:01.840 --> 00:49:08.560]   In the past, it also had lower sensitivity,
[00:49:10.080 --> 00:49:13.640]   because around every pixel you need a bit of electronics,
[00:49:13.640 --> 00:49:15.800]   and so in particular the higher the resolution you made,
[00:49:15.800 --> 00:49:19.320]   the more space this was taking on the sensor area.
[00:49:19.320 --> 00:49:23.320]   I think now as far as I understand,
[00:49:23.320 --> 00:49:25.520]   they actually, they change the process around,
[00:49:25.520 --> 00:49:30.520]   and they actually do it in a way that they,
[00:49:30.520 --> 00:49:33.840]   but all the electronics on one side,
[00:49:33.840 --> 00:49:35.840]   and then actually flip it,
[00:49:35.840 --> 00:49:37.680]   put stuff on top and then flip it around,
[00:49:37.680 --> 00:49:41.320]   and then shave away everything up to the sensor,
[00:49:41.320 --> 00:49:44.760]   and that way actually can have a close 200% fill rate.
[00:49:44.760 --> 00:49:48.400]   So you don't need space next to the pixel anymore,
[00:49:48.400 --> 00:49:51.960]   to handle all of the rest,
[00:49:51.960 --> 00:49:53.920]   but you can actually have that simply,
[00:49:53.920 --> 00:49:59.640]   by flipping the, literally flipping the chip around,
[00:49:59.640 --> 00:50:00.600]   and then scraping around,
[00:50:00.600 --> 00:50:02.520]   and having the light sense the area
[00:50:02.520 --> 00:50:04.600]   really from the other side of the chip,
[00:50:04.600 --> 00:50:08.640]   they can actually avoid that particular problem.
[00:50:08.640 --> 00:50:13.560]   And of course, because this standard CMOS technology,
[00:50:13.560 --> 00:50:15.240]   you can actually put on the same chip,
[00:50:15.240 --> 00:50:17.440]   you could actually put lots of other
[00:50:17.440 --> 00:50:19.780]   digital electronics, essentially.
[00:50:19.780 --> 00:50:23.160]   And by the way, this is already a wild back,
[00:50:23.160 --> 00:50:26.840]   but essentially CMOS chips can go to very high resolutions.
[00:50:26.840 --> 00:50:33.640]   Here's a little bit of a comparison.
[00:50:34.640 --> 00:50:38.080]   In particular, this is cheaper,
[00:50:38.080 --> 00:50:41.640]   because it doesn't need its own chip processing facility,
[00:50:41.640 --> 00:50:43.840]   that is made custom for CCDs,
[00:50:43.840 --> 00:50:48.520]   because they had kind of different electronics.
[00:50:48.520 --> 00:50:51.160]   It's just standard lines.
[00:50:51.160 --> 00:50:57.040]   It's lower power, because this transfer actually of charges,
[00:50:57.040 --> 00:50:58.760]   to read out the image,
[00:50:58.760 --> 00:51:00.840]   you have to trend and get a certain voltage
[00:51:00.840 --> 00:51:02.000]   over the whole line.
[00:51:02.000 --> 00:51:05.000]   Those kind of issues disappear, so it's lower power.
[00:51:05.000 --> 00:51:06.040]   It's a little bit less sensitive,
[00:51:06.040 --> 00:51:10.160]   but you have a lot more flexibility,
[00:51:10.160 --> 00:51:12.340]   essentially, with this technology.
[00:51:12.340 --> 00:51:16.880]   Also, potentially, if you do per pixel amplification,
[00:51:16.880 --> 00:51:20.720]   you can better handle high dynamic range images.
[00:51:20.720 --> 00:51:24.720]   We'll discuss later, kind of really,
[00:51:24.720 --> 00:51:26.360]   what the issue is with range,
[00:51:26.360 --> 00:51:28.600]   but essentially, because you can only have,
[00:51:28.600 --> 00:51:30.040]   if you have one common,
[00:51:31.960 --> 00:51:33.680]   amplification of the whole image,
[00:51:33.680 --> 00:51:36.840]   and you have, let's say, only eight bits
[00:51:36.840 --> 00:51:37.920]   to represent your image,
[00:51:37.920 --> 00:51:39.840]   then you're stuck, essentially,
[00:51:39.840 --> 00:51:43.560]   either having this be white and seeing something here,
[00:51:43.560 --> 00:51:48.560]   or vice versa, having contrast here,
[00:51:48.560 --> 00:51:50.680]   but then all of this will just be black,
[00:51:50.680 --> 00:51:52.320]   will be zero value, essentially.
[00:51:52.320 --> 00:51:56.600]   There, you can actually do better.
[00:51:56.600 --> 00:51:59.960]   They also issue with CMOS sensors.
[00:51:59.960 --> 00:52:00.960]   Here's kind of one example
[00:52:00.960 --> 00:52:02.640]   of what you get with CMOS sensors.
[00:52:02.640 --> 00:52:07.280]   This is actually because CMOS sensors, typically,
[00:52:07.280 --> 00:52:10.840]   it's not strictly needed, but most CMOS sensors,
[00:52:10.840 --> 00:52:16.520]   where is the cursor, here we go, have rolling shutter.
[00:52:16.520 --> 00:52:18.800]   Okay, so what is rolling shutter?
[00:52:18.800 --> 00:52:20.280]   It essentially means that,
[00:52:20.280 --> 00:52:24.960]   to keep the, to in particular,
[00:52:24.960 --> 00:52:27.360]   make the chip really cheap and everything,
[00:52:27.360 --> 00:52:29.520]   the way it works is that,
[00:52:29.520 --> 00:52:33.040]   they're actually never stopping to integrate on the chip.
[00:52:33.040 --> 00:52:35.480]   So the chip is actually always photosensitive,
[00:52:35.480 --> 00:52:39.080]   so there's no shutter in front.
[00:52:39.080 --> 00:52:40.640]   It always has light coming in.
[00:52:40.640 --> 00:52:44.240]   The light always gets transformed into incoming light,
[00:52:44.240 --> 00:52:47.400]   gets transformed from, you know, into electrons.
[00:52:47.400 --> 00:52:49.520]   The only thing that happens is that every once in a while,
[00:52:49.520 --> 00:52:51.920]   you say, okay, flush everything you have now,
[00:52:51.920 --> 00:52:54.040]   so you clear the line,
[00:52:54.040 --> 00:52:55.520]   so you flush all the electrons that are there,
[00:52:55.520 --> 00:52:57.080]   you reset the line,
[00:52:57.080 --> 00:52:59.800]   and then you restart accumulating from that point on.
[00:52:59.800 --> 00:53:01.360]   And then at another point in time,
[00:53:01.360 --> 00:53:04.520]   you can actually read out the line.
[00:53:04.520 --> 00:53:06.840]   Okay, and so this is also organized in lines,
[00:53:06.840 --> 00:53:08.800]   essentially, in scan lines.
[00:53:08.800 --> 00:53:10.560]   For example, as you see here,
[00:53:10.560 --> 00:53:12.800]   essentially you have lines,
[00:53:12.800 --> 00:53:15.400]   and different things happen at different times,
[00:53:15.400 --> 00:53:18.200]   and that's why along the line, everything is fine,
[00:53:18.200 --> 00:53:20.640]   but across lines, you see strange distortions.
[00:53:20.640 --> 00:53:22.960]   This is because actually every line is read out
[00:53:22.960 --> 00:53:24.760]   at a different point in time.
[00:53:24.760 --> 00:53:27.640]   So if you build electronics,
[00:53:27.640 --> 00:53:29.760]   it is much simpler to build electronics
[00:53:29.760 --> 00:53:32.000]   that just reads out systematically
[00:53:32.000 --> 00:53:35.080]   at a sustained readout rate,
[00:53:35.080 --> 00:53:37.000]   meaning that, you know, for example,
[00:53:37.000 --> 00:53:40.040]   if you want to do a camera that reads
[00:53:40.040 --> 00:53:43.400]   30 images per second, right, 30 frames per second,
[00:53:43.400 --> 00:53:47.960]   then, you know, you shouldn't over dimension
[00:53:47.960 --> 00:53:49.440]   your readout system.
[00:53:49.440 --> 00:53:52.000]   You can take a 30th of a second to read out your image,
[00:53:52.000 --> 00:53:53.360]   right, that's gonna be the,
[00:53:53.360 --> 00:53:56.080]   that you need to have that bandwidth
[00:53:56.080 --> 00:53:59.040]   to read out at 30 images per second,
[00:53:59.040 --> 00:54:02.360]   but ideally you read really just at that fixed rate
[00:54:02.360 --> 00:54:04.640]   to achieve 30 images per second,
[00:54:04.640 --> 00:54:06.000]   you don't have to read the whole image
[00:54:06.000 --> 00:54:07.440]   in a very quick burst,
[00:54:07.440 --> 00:54:10.240]   and then, you know, wait for like,
[00:54:10.240 --> 00:54:12.800]   you know, what is it like,
[00:54:12.800 --> 00:54:14.400]   30 milliseconds, for example,
[00:54:14.400 --> 00:54:16.960]   you know, read out for three milliseconds,
[00:54:16.960 --> 00:54:20.000]   and then wait 30 milliseconds until the next frame is ready.
[00:54:20.000 --> 00:54:22.000]   The way you actually, that these chips do it
[00:54:22.000 --> 00:54:24.040]   is they just read out, you know,
[00:54:24.040 --> 00:54:26.280]   at their rhythm, line per line,
[00:54:26.280 --> 00:54:27.760]   until they're at the bottom of the image,
[00:54:27.760 --> 00:54:28.840]   and then they go back to the top,
[00:54:28.840 --> 00:54:30.280]   and they start reading out again.
[00:54:30.280 --> 00:54:33.160]   And then how do they do to have the right amount of exposure?
[00:54:33.160 --> 00:54:35.400]   Well, they just reset the line,
[00:54:35.400 --> 00:54:38.040]   you know, the amount of time exposure they want
[00:54:38.040 --> 00:54:40.840]   before they are going to read out the line.
[00:54:40.840 --> 00:54:43.720]   So essentially, as you read out the line,
[00:54:43.720 --> 00:54:46.200]   there's like, in front of it, there's a reset going on,
[00:54:46.200 --> 00:54:48.840]   and so you reset this line, you read out this line,
[00:54:48.840 --> 00:54:50.720]   and then, you know, you move down,
[00:54:50.720 --> 00:54:53.200]   and this amount of time between the two,
[00:54:53.200 --> 00:54:55.720]   you know, will mean that every line is exposed
[00:54:55.720 --> 00:54:57.120]   by that amount of time, essentially,
[00:54:57.120 --> 00:54:58.920]   it takes to travel from here to here, right,
[00:54:58.920 --> 00:55:01.280]   over that distance, essentially.
[00:55:01.280 --> 00:55:02.920]   Yeah, in that way, you can have a system
[00:55:02.920 --> 00:55:05.160]   that just very regularly just reads out pixels
[00:55:05.160 --> 00:55:07.800]   at a fixed constant bandwidth,
[00:55:07.800 --> 00:55:10.440]   you know, rate of reading pixels, essentially.
[00:55:10.440 --> 00:55:14.120]   So that's very cheap, makes the electronics simple,
[00:55:14.120 --> 00:55:16.720]   et cetera, et cetera, all very regular processing,
[00:55:16.720 --> 00:55:19.120]   but, you know, if you move quickly with your camera
[00:55:19.120 --> 00:55:20.720]   while this process is happening,
[00:55:20.720 --> 00:55:23.680]   then this is kind of what it can look like, okay?
[00:55:23.680 --> 00:55:27.560]   Especially with things, you know, here, the helicopter,
[00:55:27.560 --> 00:55:29.360]   you see a nice picture of the helicopter, or less,
[00:55:29.360 --> 00:55:32.200]   but the things that move really fast, you know,
[00:55:32.200 --> 00:55:35.320]   essentially have significant motion between scan lines,
[00:55:35.320 --> 00:55:38.600]   or, you know, across part of the time it takes
[00:55:38.600 --> 00:55:41.200]   to read out this part of the image,
[00:55:41.200 --> 00:55:43.760]   you already have a lot of motion of the,
[00:55:43.760 --> 00:55:47.720]   of this part of the helicopter here,
[00:55:47.720 --> 00:55:51.880]   then it's, you know, you get funny effects here.
[00:55:51.880 --> 00:55:55.760]   And also here, when you have a lightning,
[00:55:55.760 --> 00:55:58.040]   this can also create funny effects.
[00:55:58.040 --> 00:56:01.160]   I think I should have something to,
[00:56:01.160 --> 00:56:02.480]   an example to show here.
[00:56:02.480 --> 00:56:09.000]   Okay, I think this is actually nice here.
[00:56:09.000 --> 00:56:09.840]   - I'd like to show you around
[00:56:09.840 --> 00:56:11.600]   our new plugin, Rowling Shutter.
[00:56:11.600 --> 00:56:14.180]   (upbeat music)
[00:56:14.180 --> 00:56:16.760]   (upbeat music)
[00:56:16.760 --> 00:56:21.620]   - It's designed to help correct skewing distortion
[00:56:21.620 --> 00:56:25.220]   that Seamoth chip in some modern digital cameras can create.
[00:56:25.220 --> 00:56:27.140]   - The problem arises, because there's a slight delay
[00:56:27.140 --> 00:56:30.220]   between the camera recording top and the bottom of the picture.
[00:56:30.220 --> 00:56:32.220]   This time difference means that movement in the scene
[00:56:32.220 --> 00:56:33.380]   can end up distorted.
[00:56:33.380 --> 00:56:35.940]   You can see the trouble here in a shot
[00:56:35.940 --> 00:56:38.380]   where the camera is panning sideways.
[00:56:38.380 --> 00:56:40.620]   Not only can it look hot, but it makes shots
[00:56:40.620 --> 00:56:42.460]   difficult or time consuming to track.
[00:56:43.500 --> 00:56:45.940]   Our plugin, available for after effects and new,
[00:56:45.940 --> 00:56:47.460]   attempts to correct for this distortion
[00:56:47.460 --> 00:56:49.300]   by working out motion in the scene
[00:56:49.300 --> 00:56:52.300]   and sliding objects back into their proper place.
[00:56:52.300 --> 00:56:53.940]   Being able to correct for this not only helps
[00:56:53.940 --> 00:56:55.820]   the look of the shot, but it can also allow you
[00:56:55.820 --> 00:56:58.620]   to extract the camera from a previously untrackable shot.
[00:56:58.620 --> 00:57:03.620]   Unlike some other global solutions,
[00:57:03.620 --> 00:57:05.300]   our technology allows us to look at each
[00:57:05.300 --> 00:57:07.700]   of the moving objects in the scene individually
[00:57:07.700 --> 00:57:09.500]   and move them back to the right place
[00:57:09.500 --> 00:57:12.900]   rather than applying the motion blur here.
[00:57:12.900 --> 00:57:14.980]   This stops any objects that weren't actually moving
[00:57:14.980 --> 00:57:16.900]   relative to the camera being distorted
[00:57:16.900 --> 00:57:18.380]   by the correction process.
[00:57:18.380 --> 00:57:24.100]   The main piece of information you need
[00:57:24.100 --> 00:57:26.180]   in order to use the plugin is how much lag
[00:57:26.180 --> 00:57:28.300]   there is in your camera.
[00:57:28.300 --> 00:57:31.900]   The easiest way to work this out is to film a little test shot
[00:57:31.900 --> 00:57:33.620]   where you pan your camera around the scene
[00:57:33.620 --> 00:57:35.660]   with some quite strong vertical edges,
[00:57:35.660 --> 00:57:36.900]   maybe a wall or a door.
[00:57:36.900 --> 00:57:40.580]   You simply apply the plugin and adjust the--
[00:57:40.580 --> 00:57:43.020]   Okay, anyways, that's good enough.
[00:57:43.020 --> 00:57:46.420]   But that gives you an idea of those kind of problems.
[00:57:46.420 --> 00:57:48.220]   You can actually have fun with this.
[00:57:48.220 --> 00:57:52.460]   If you wanna give it a try with your phone,
[00:57:52.460 --> 00:57:53.700]   then you can try it, I don't know exactly
[00:57:53.700 --> 00:57:55.380]   in what orientation.
[00:57:55.380 --> 00:57:58.060]   You can actually try both, take a video like this
[00:57:58.060 --> 00:57:59.820]   and a video like that as you are in the tram
[00:57:59.820 --> 00:58:01.940]   and you watch out of the window
[00:58:01.940 --> 00:58:04.860]   and just record a video as the tram moves by
[00:58:04.860 --> 00:58:08.260]   and look at the video and you'll see essentially things
[00:58:08.260 --> 00:58:12.020]   at different angles as you do that.
[00:58:12.020 --> 00:58:14.060]   - Is there already such a correction
[00:58:14.060 --> 00:58:16.900]   in when the mobile phone takes the video
[00:58:16.900 --> 00:58:19.300]   because of some pre-built model or something?
[00:58:19.300 --> 00:58:22.340]   - Not that I know, actually, no.
[00:58:22.340 --> 00:58:23.700]   Not that I know.
[00:58:23.700 --> 00:58:25.980]   There might be, you know, they keep putting new things
[00:58:25.980 --> 00:58:27.980]   in all the time, but I don't think so.
[00:58:27.980 --> 00:58:32.620]   What's actually interesting is that
[00:58:32.620 --> 00:58:35.660]   because things that are close to you
[00:58:37.420 --> 00:58:40.980]   visually move faster than things that are further away,
[00:58:40.980 --> 00:58:43.180]   you will actually, as you film like that,
[00:58:43.180 --> 00:58:44.820]   and if you have the right orientation
[00:58:44.820 --> 00:58:49.820]   so that essentially your readout is kind of vertical.
[00:58:49.820 --> 00:58:53.500]   So try both, but one of the two should definitely do that.
[00:58:53.500 --> 00:58:56.900]   If you record a video and you watch at a still frame then,
[00:58:56.900 --> 00:58:59.140]   you will actually see that things closer
[00:58:59.140 --> 00:59:02.980]   will be more tilted than things further away, for example.
[00:59:02.980 --> 00:59:05.340]   Because the things close by move fast
[00:59:05.340 --> 00:59:07.700]   while the things far away are much more stable
[00:59:07.700 --> 00:59:10.700]   as you kind of, you know, just go like this in the tram,
[00:59:10.700 --> 00:59:12.940]   past the facades or past the things.
[00:59:12.940 --> 00:59:15.780]   Anyway, something to homework if you want,
[00:59:15.780 --> 00:59:17.140]   or on the way homework.
[00:59:17.140 --> 00:59:22.820]   Okay, so here's another type of camera.
[00:59:22.820 --> 00:59:26.620]   So we mostly will look at frame-based cameras,
[00:59:26.620 --> 00:59:30.300]   but I also wanted to briefly show you here the DVS camera,
[00:59:30.300 --> 00:59:32.700]   which is an event-based camera,
[00:59:32.700 --> 00:59:34.900]   which is actually, and I think I mentioned it,
[00:59:35.900 --> 00:59:39.540]   on Tuesday is a human-inspired,
[00:59:39.540 --> 00:59:43.500]   human-eye-inspired camera.
[00:59:43.500 --> 00:59:45.740]   It has some of the principle of the human eye,
[00:59:45.740 --> 00:59:49.620]   only some of them, but it's actually quite interesting
[00:59:49.620 --> 00:59:52.620]   and has a number of advantages, but also disadvantages,
[00:59:52.620 --> 00:59:54.660]   but can do some very interesting things.
[00:59:54.660 --> 00:59:56.740]   - Standard camera transmits full frames
[00:59:56.740 --> 00:59:58.780]   at a fixed frame rate.
[00:59:58.780 --> 01:00:02.220]   By contrast, the DVS has smart pixels.
[01:00:02.220 --> 01:00:05.060]   Its pixels are all independent of each other
[01:00:05.060 --> 01:00:06.580]   and only transmit information
[01:00:06.580 --> 01:00:09.820]   if they detect a change of brightness in the scene.
[01:00:09.820 --> 01:00:12.500]   We call these bits of information events.
[01:00:12.500 --> 01:00:15.380]   Since these events are generated asynchronously,
[01:00:15.380 --> 01:00:17.900]   the output of a DVS is a sequence of events
[01:00:17.900 --> 01:00:20.060]   instead of full frames.
[01:00:20.060 --> 01:00:22.780]   To visualize this principle, in this animation,
[01:00:22.780 --> 01:00:26.220]   we show the output of a camera and that of a DVS
[01:00:26.220 --> 01:00:28.340]   when they are both looking at a black dot
[01:00:28.340 --> 01:00:30.060]   on a rotating disk.
[01:00:30.060 --> 01:00:32.140]   As we can observe, for the DVS,
[01:00:32.140 --> 01:00:34.940]   the events form a spiral in space and time.
[01:00:34.940 --> 01:00:37.140]   When the disk stops rotating,
[01:00:37.140 --> 01:00:39.180]   no events are generated at all.
[01:00:39.180 --> 01:00:41.420]   However, the standard camera continues
[01:00:41.420 --> 01:00:44.340]   to wastefully send full image frames.
[01:00:44.340 --> 01:00:45.860]   When we speed up the dot,
[01:00:45.860 --> 01:00:49.660]   the images of the standard camera suffer from motion blur
[01:00:49.660 --> 01:00:52.940]   while the spiral of events is still clearly visible.
[01:00:52.940 --> 01:00:56.980]   This is due to the very high temporal resolution of the DVS,
[01:00:56.980 --> 01:00:59.100]   which is in the order of microseconds.
[01:01:00.020 --> 01:01:02.260]   When we keep the DVS steady,
[01:01:02.260 --> 01:01:05.300]   only motion in the scene generates events.
[01:01:05.300 --> 01:01:07.900]   However, when we start moving the DVS,
[01:01:07.900 --> 01:01:09.860]   gradients in the scene become visible.
[01:01:09.860 --> 01:01:12.340]   In the following experiment,
[01:01:12.340 --> 01:01:14.940]   we mounted the DVS on a standard camera
[01:01:14.940 --> 01:01:17.340]   on a quadruple-optor and performed flips.
[01:01:17.340 --> 01:01:23.180]   When we replay the flips in slow motion,
[01:01:23.180 --> 01:01:25.300]   we can clearly see motion blur effects
[01:01:25.300 --> 01:01:27.180]   on the standard camera.
[01:01:27.180 --> 01:01:30.500]   However, if we render the DVS output appropriately,
[01:01:30.500 --> 01:01:32.820]   we can still see sharp lines.
[01:01:32.820 --> 01:01:34.380]   For these DVS renderings,
[01:01:34.380 --> 01:01:36.900]   we accumulate all the events over a time interval
[01:01:36.900 --> 01:01:39.100]   of delta T in one image.
[01:01:39.100 --> 01:01:41.100]   By choosing this delta T small,
[01:01:41.100 --> 01:01:42.900]   we can render slow motion video.
[01:01:42.900 --> 01:01:46.300]   The high temporal resolution of the DVS
[01:01:46.300 --> 01:01:48.380]   allows us to track the quadruple-optor
[01:01:48.380 --> 01:01:50.820]   during flips with rotational speeds
[01:01:50.820 --> 01:01:53.860]   of up to 1,200 degrees per second.
[01:01:55.100 --> 01:01:59.100]   We believe that a DVS is the most promising onboard sensor
[01:01:59.100 --> 01:02:01.180]   for enabling highly aggressive maneuvers
[01:02:01.180 --> 01:02:02.460]   with flying robots.
[01:02:02.460 --> 01:02:04.780]   - Right, and so you know,
[01:02:04.780 --> 01:02:05.900]   like David is doing,
[01:02:05.900 --> 01:02:10.140]   competing with human capabilities
[01:02:10.140 --> 01:02:12.940]   in terms of robot racing and so on,
[01:02:12.940 --> 01:02:14.540]   including some of the technology.
[01:02:14.540 --> 01:02:20.620]   Okay, so now let's get to our first look at sampling.
[01:02:20.620 --> 01:02:24.340]   This is more cartoon look at it.
[01:02:24.340 --> 01:02:27.140]   We'll get much more in detail at this
[01:02:27.140 --> 01:02:28.140]   further in the lecture,
[01:02:28.140 --> 01:02:30.620]   but to already give you a preview
[01:02:30.620 --> 01:02:33.180]   or some intuition and some kind of basic idea
[01:02:33.180 --> 01:02:35.180]   of what are the important concepts here.
[01:02:35.180 --> 01:02:37.900]   So here we look at, in this case,
[01:02:37.900 --> 01:02:41.180]   just a 1D function, so this curve, right?
[01:02:41.180 --> 01:02:43.860]   We will sample it in 1D,
[01:02:43.860 --> 01:02:47.700]   so what we're really talking about is at discrete locations,
[01:02:47.700 --> 01:02:49.260]   these locations here,
[01:02:49.260 --> 01:02:53.020]   we're going to measure the value of the function.
[01:02:53.020 --> 01:02:55.020]   Again, we're not integrating over certain areas,
[01:02:55.020 --> 01:02:57.740]   this is that we just really measure at that one location.
[01:02:57.740 --> 01:03:00.620]   So that's what we call sampling.
[01:03:00.620 --> 01:03:06.100]   So here's an example, for example, some audio signal.
[01:03:06.100 --> 01:03:09.180]   This would be another signal with an increasing frequency.
[01:03:09.180 --> 01:03:12.580]   So kind of type of sound, right?
[01:03:12.580 --> 01:03:16.060]   That's what it would look like.
[01:03:16.060 --> 01:03:20.540]   Here's this is actually,
[01:03:20.540 --> 01:03:23.060]   and we'll see the actual example later
[01:03:23.060 --> 01:03:24.500]   where this was cut out from.
[01:03:24.500 --> 01:03:25.900]   There's also 1D signal,
[01:03:25.900 --> 01:03:29.420]   it's actually one line from a 2D image.
[01:03:29.420 --> 01:03:34.420]   So we can also sample two-dimensional signals this way,
[01:03:34.420 --> 01:03:37.500]   in this case it's one line cut out of a 2D signal,
[01:03:37.500 --> 01:03:39.340]   and as we sample it,
[01:03:39.340 --> 01:03:41.420]   we also have this continuous function there,
[01:03:41.420 --> 01:03:45.060]   and now we make measurements at a set of discrete locations,
[01:03:45.060 --> 01:03:46.380]   we make a point measurement,
[01:03:46.380 --> 01:03:48.140]   and we write down the value essentially
[01:03:48.140 --> 01:03:49.540]   at each of those locations.
[01:03:50.540 --> 01:03:51.540]   So that's sampling.
[01:03:51.540 --> 01:03:54.980]   So it's pretty straightforward, right?
[01:03:54.980 --> 01:03:57.540]   Just write down the function's value at all these points.
[01:03:57.540 --> 01:04:02.500]   And then, so let's say this was audio or something like that,
[01:04:02.500 --> 01:04:04.500]   then we have now a digital audio,
[01:04:04.500 --> 01:04:05.900]   like what you have on DVD,
[01:04:05.900 --> 01:04:08.540]   or what you will stream from Spotify or whatever.
[01:04:08.540 --> 01:04:12.820]   And then of course, as you get it back to your phone or whatever,
[01:04:12.820 --> 01:04:16.220]   you actually want to regenerate a continuous signal
[01:04:16.220 --> 01:04:18.940]   that you can send into your ears,
[01:04:18.940 --> 01:04:21.020]   and actually appreciate the music, for example.
[01:04:21.020 --> 01:04:22.300]   Or as I said before,
[01:04:22.300 --> 01:04:24.540]   if you want to display the image on the screen,
[01:04:24.540 --> 01:04:27.260]   you have to essentially be able to remeasure
[01:04:27.260 --> 01:04:30.180]   at potentially not those exact locations that you're given,
[01:04:30.180 --> 01:04:34.740]   but at any location you would need for your screen or for your display.
[01:04:34.740 --> 01:04:40.860]   So there's really always in tandem when being able to discretize,
[01:04:40.860 --> 01:04:44.940]   we also need to be able to go from the analog continuous world
[01:04:44.940 --> 01:04:46.380]   to a discrete representation,
[01:04:46.380 --> 01:04:50.300]   we also need to be able to go back from the discrete representation
[01:04:50.300 --> 01:04:52.420]   to a continuous representation.
[01:04:52.420 --> 01:04:54.460]   That is actually called reconstruction.
[01:04:54.460 --> 01:05:00.060]   And so, in a sense, it corresponds to, as you see up there,
[01:05:00.060 --> 01:05:02.780]   guessing what the function did in between.
[01:05:02.780 --> 01:05:07.500]   Okay, so with no more information,
[01:05:07.500 --> 01:05:09.580]   if you remember the random image,
[01:05:09.580 --> 01:05:13.380]   if this was coming from a random signal,
[01:05:13.380 --> 01:05:16.340]   then all bets are open in between,
[01:05:16.340 --> 01:05:20.780]   those few measurements we have say nothing about the other points,
[01:05:20.780 --> 01:05:23.220]   if they're all random and uncorrelated.
[01:05:23.220 --> 01:05:25.460]   So we actually have to make assumptions
[01:05:25.460 --> 01:05:29.420]   of what this function actually can be doing in between measurements,
[01:05:29.420 --> 01:05:32.780]   or in other words, we need to have a certain level of correlation
[01:05:32.780 --> 01:05:34.420]   between the measurements we took
[01:05:34.420 --> 01:05:38.220]   and assume that the function in between does something reasonable.
[01:05:38.220 --> 01:05:39.940]   It behaves in a certain way.
[01:05:39.940 --> 01:05:43.300]   You see me hintwaving now, literally.
[01:05:45.260 --> 01:05:48.780]   We will actually very strictly formalize
[01:05:48.780 --> 01:05:53.020]   what the assumptions are that we make in between
[01:05:53.020 --> 01:05:55.780]   to allow for actually a perfect reconstruction.
[01:05:55.780 --> 01:05:57.300]   So under some conditions,
[01:05:57.300 --> 01:06:00.900]   which have to do with how quickly the signal can change,
[01:06:00.900 --> 01:06:03.900]   and this will be translated into frequencies and so on,
[01:06:03.900 --> 01:06:07.980]   we will actually be able to say, as long as this is satisfied,
[01:06:07.980 --> 01:06:09.780]   these constraints are satisfied,
[01:06:09.780 --> 01:06:12.260]   we will be able to go exactly from that signal
[01:06:12.260 --> 01:06:14.180]   back to the original signal.
[01:06:14.180 --> 01:06:16.340]   So the one that we started from, essentially.
[01:06:16.340 --> 01:06:18.540]   This is under some assumptions,
[01:06:18.540 --> 01:06:22.500]   which in practice we can not exactly satisfy in this and that,
[01:06:22.500 --> 01:06:27.180]   but it will still work quite well.
[01:06:27.180 --> 01:06:32.180]   So here's the example with the sound,
[01:06:32.180 --> 01:06:35.380]   microphone, continuous waveform.
[01:06:35.380 --> 01:06:39.060]   This gets translated to digital,
[01:06:39.060 --> 01:06:43.460]   sampled into a discrete set of numbers that are digital,
[01:06:43.460 --> 01:06:45.660]   and represented with finite number of bits
[01:06:45.660 --> 01:06:50.180]   that get stored on a disk or something else,
[01:06:50.180 --> 01:06:52.460]   in store in memory, or transmitted,
[01:06:52.460 --> 01:06:55.420]   and then gets back to a digital analog converter.
[01:06:55.420 --> 01:06:59.620]   Reconstruction happens, you get a continuous signal,
[01:06:59.620 --> 01:07:03.380]   and then you can listen to the music.
[01:07:03.380 --> 01:07:10.020]   Okay, so let's look at a seamless possible waveform,
[01:07:10.020 --> 01:07:14.420]   a sinusoid, just rhythmically going up and down
[01:07:14.420 --> 01:07:15.980]   at a certain rhythm.
[01:07:15.980 --> 01:07:21.100]   If we sample this, for example like this,
[01:07:21.100 --> 01:07:22.860]   so at each of those locations,
[01:07:22.860 --> 01:07:25.700]   we measure the value of the function, for example here.
[01:07:25.700 --> 01:07:30.100]   So we sample the function,
[01:07:30.100 --> 01:07:31.860]   the only thing we're left with is this.
[01:07:31.860 --> 01:07:34.340]   So we forget the function, we don't have the function anymore,
[01:07:34.340 --> 01:07:36.540]   it's passed, we measured things, it's gone,
[01:07:36.540 --> 01:07:37.860]   the signal is gone.
[01:07:37.860 --> 01:07:39.420]   This is what we have from the function.
[01:07:40.140 --> 01:07:42.140]   So now we should reconstruct.
[01:07:42.140 --> 01:07:46.300]   Now looking at these measurements going like this,
[01:07:46.300 --> 01:07:50.140]   you can probably reconstruct this out of it, right?
[01:07:50.140 --> 01:07:51.220]   That'd be quite easy.
[01:07:51.220 --> 01:07:54.940]   But what happens if we sample at a different rate?
[01:07:54.940 --> 01:07:57.980]   For example here, I measure here,
[01:07:57.980 --> 01:08:00.660]   and then I measure here, and then I measure here,
[01:08:00.660 --> 01:08:04.940]   and then here, and here, and here, here, and here.
[01:08:04.940 --> 01:08:08.500]   Okay, can we do the same trick?
[01:08:08.500 --> 01:08:13.500]   Oh, now if we actually just look at those data points,
[01:08:13.500 --> 01:08:15.940]   there's a much simpler explanation,
[01:08:15.940 --> 01:08:17.900]   which is this much slower moving.
[01:08:17.900 --> 01:08:22.300]   So this is clearly something funny going on here, right?
[01:08:22.300 --> 01:08:25.980]   We had this perfect sine wave,
[01:08:25.980 --> 01:08:30.980]   and we sampled it at a discrete rate,
[01:08:30.980 --> 01:08:32.860]   okay, maybe a bit too slow,
[01:08:32.860 --> 01:08:37.860]   but it's kind of funny that essentially this sine wave
[01:08:38.140 --> 01:08:39.740]   that we sampled kind of looks,
[01:08:39.740 --> 01:08:42.540]   if we look just at the samples,
[01:08:42.540 --> 01:08:45.820]   the original sine wave or this much slower sine wave
[01:08:45.820 --> 01:08:47.700]   look essentially identical.
[01:08:47.700 --> 01:08:50.380]   If we only look at those particular points here, right?
[01:08:50.380 --> 01:08:52.860]   They are indistinguishable, they are identical.
[01:08:52.860 --> 01:08:57.420]   So that's what you see there, right?
[01:08:57.420 --> 01:09:02.780]   So if we don't sample fast enough, this could happen, right?
[01:09:02.780 --> 01:09:05.540]   We could, you know, we might have this signal
[01:09:06.580 --> 01:09:11.580]   that actually happens, but we sampled too slowly,
[01:09:11.580 --> 01:09:14.900]   and now when we just see these points,
[01:09:14.900 --> 01:09:16.380]   of course, what would we reconstruct?
[01:09:16.380 --> 01:09:17.740]   Well, we'll probably just think like,
[01:09:17.740 --> 01:09:21.780]   oh, this is slow moving signal that just moves,
[01:09:21.780 --> 01:09:24.100]   and so we'd probably reconstruct the dark curve here
[01:09:24.100 --> 01:09:26.180]   instead of the original one, right?
[01:09:26.180 --> 01:09:27.980]   So this is going to be a fundamental problem
[01:09:27.980 --> 01:09:30.620]   that we'll have to care,
[01:09:30.620 --> 01:09:32.980]   and you see already with a simplest possible waveform,
[01:09:32.980 --> 01:09:37.860]   like a sine wave, we fundamentally have this problem
[01:09:37.860 --> 01:09:39.500]   that shows up here.
[01:09:39.500 --> 01:09:42.220]   This is a problem that we'll actually call aliasing.
[01:09:42.220 --> 01:09:44.780]   Aliasing from essentially, you know,
[01:09:44.780 --> 01:09:47.340]   they are indistinguishable from each other,
[01:09:47.340 --> 01:09:49.340]   they can be confused with each other.
[01:09:49.340 --> 01:09:53.460]   So this is essentially, you can see these as signals
[01:09:53.460 --> 01:09:56.020]   traveling in disguise as other frequencies.
[01:09:56.020 --> 01:09:58.980]   Notice that before, the signal that we thought
[01:09:58.980 --> 01:10:01.620]   we were sampling, maybe it was actually the fastest signal
[01:10:02.740 --> 01:10:05.180]   that one also looks the same.
[01:10:05.180 --> 01:10:07.500]   So there's not only those two that can be confused,
[01:10:07.500 --> 01:10:11.500]   there's a whole family of different sine waves,
[01:10:11.500 --> 01:10:15.060]   exact sine waves that all go exactly through the same points.
[01:10:15.060 --> 01:10:20.860]   Here's another signal, so again, like a sine wave,
[01:10:20.860 --> 01:10:22.300]   but accelerating sine wave.
[01:10:22.300 --> 01:10:25.860]   If we take a picture of this,
[01:10:25.860 --> 01:10:29.940]   you know, so you would look at it this way here,
[01:10:29.940 --> 01:10:32.580]   you generate this, you can quickly generate this yourself,
[01:10:33.260 --> 01:10:37.100]   and so you essentially just display that discrete locations,
[01:10:37.100 --> 01:10:38.180]   this is essentially the same,
[01:10:38.180 --> 01:10:40.820]   this is sampling essentially, right, that you do.
[01:10:40.820 --> 01:10:44.300]   You just display at these exact locations,
[01:10:44.300 --> 01:10:46.700]   you sample this function,
[01:10:46.700 --> 01:10:49.380]   you read out the function value of this function.
[01:10:49.380 --> 01:10:53.540]   You know, essentially, this all behaves properly
[01:10:53.540 --> 01:10:55.580]   as you would kind of expect,
[01:10:55.580 --> 01:10:58.620]   but then as you get here, something funny happens,
[01:10:58.620 --> 01:11:02.340]   you see that you know it was a high frequency over there,
[01:11:02.340 --> 01:11:04.100]   but what you see is actually a smooth,
[01:11:04.100 --> 01:11:07.020]   wide continuous signal.
[01:11:07.020 --> 01:11:09.420]   So clearly that effect that we had theoretically before,
[01:11:09.420 --> 01:11:11.620]   like it really shows up here.
[01:11:11.620 --> 01:11:14.180]   So we have this aliasing going on.
[01:11:14.180 --> 01:11:16.180]   So it's that kind of behavior here.
[01:11:16.180 --> 01:11:18.420]   Not enough samples, so what we see there
[01:11:18.420 --> 01:11:19.980]   is essentially something like this here.
[01:11:19.980 --> 01:11:21.660]   There's actually a fast wave,
[01:11:21.660 --> 01:11:24.540]   but it's kind of in sync with a kind of slow pattern,
[01:11:24.540 --> 01:11:26.820]   and so when we just look at it,
[01:11:26.820 --> 01:11:28.460]   we see the slow pattern.
[01:11:31.060 --> 01:11:32.780]   Okay, so of course, we're talking about
[01:11:32.780 --> 01:11:34.180]   two dimensional images.
[01:11:34.180 --> 01:11:36.180]   What we really do is not one d functions,
[01:11:36.180 --> 01:11:39.140]   but two d functions, so two d continuous functions,
[01:11:39.140 --> 01:11:41.820]   and then we'll have essentially something here,
[01:11:41.820 --> 01:11:44.820]   like what a fact here would have a bit of nails or so,
[01:11:44.820 --> 01:11:47.580]   or essentially a two dimensional array
[01:11:47.580 --> 01:11:49.860]   of those sample points, right?
[01:11:49.860 --> 01:11:53.220]   Here's for example an image.
[01:11:53.220 --> 01:11:56.860]   You know, this image here,
[01:11:56.860 --> 01:11:59.300]   which we'll use as an example
[01:11:59.300 --> 01:12:02.020]   throughout some of this lecture.
[01:12:02.020 --> 01:12:03.820]   So this image here, this is actually
[01:12:03.820 --> 01:12:04.980]   the two dimensional function,
[01:12:04.980 --> 01:12:06.540]   or the two dimensional set of samples,
[01:12:06.540 --> 01:12:08.300]   what it looks like.
[01:12:08.300 --> 01:12:10.180]   Again, so this image is a bunch of numbers.
[01:12:10.180 --> 01:12:12.340]   This is here, you know, the numbers shown as like
[01:12:12.340 --> 01:12:14.660]   the height of the point for each of those numbers.
[01:12:14.660 --> 01:12:26.620]   Then like, here's a very simple reconstruction algorithm.
[01:12:28.660 --> 01:12:30.380]   Right, so we haven't yet seen
[01:12:30.380 --> 01:12:31.540]   how do you actually reconstruct,
[01:12:31.540 --> 01:12:33.140]   so we hand over it, it's like,
[01:12:33.140 --> 01:12:34.500]   oh, you know, we get these measurements,
[01:12:34.500 --> 01:12:37.540]   well, we draw a smooth curve through it or so.
[01:12:37.540 --> 01:12:39.620]   Okay, so here's actually a proper
[01:12:39.620 --> 01:12:42.340]   simple mathematical reconstruction formula.
[01:12:42.340 --> 01:12:45.540]   It's called bilinear interpolation.
[01:12:45.540 --> 01:12:47.780]   We just assume that, you know, like, let's say,
[01:12:47.780 --> 01:12:49.700]   in one dimension, we at this point and this point,
[01:12:49.700 --> 01:12:51.980]   we say, okay, along the way, between the two points,
[01:12:51.980 --> 01:12:53.660]   we assume linear behavior.
[01:12:53.660 --> 01:12:55.220]   So if you ask me what's the value here,
[01:12:55.220 --> 01:12:56.700]   I take this and this,
[01:12:56.700 --> 01:12:58.260]   I measure how far from both it is,
[01:12:58.260 --> 01:12:59.700]   and then I do the linear combination
[01:12:59.700 --> 01:13:03.180]   that will make it land on the line between the two.
[01:13:03.180 --> 01:13:05.500]   That's essentially this formula here,
[01:13:05.500 --> 01:13:09.260]   and then we apply it both in the X and in the Y direction.
[01:13:09.260 --> 01:13:12.740]   And so, you know, you see here the linear factors
[01:13:12.740 --> 01:13:14.380]   for the different points,
[01:13:14.380 --> 01:13:19.380]   and we'll get to define this value here.
[01:13:19.380 --> 01:13:24.100]   If this is A, and so A is smaller than one, of course,
[01:13:24.100 --> 01:13:26.500]   so the distance between two grid points
[01:13:26.500 --> 01:13:28.620]   is assumed to be one here, right?
[01:13:28.620 --> 01:13:31.220]   If we have a value in between,
[01:13:31.220 --> 01:13:35.860]   you know, we'll apply to each of the weights here,
[01:13:35.860 --> 01:13:38.500]   to each of those corners,
[01:13:38.500 --> 01:13:41.540]   the values at each of the corner locations,
[01:13:41.540 --> 01:13:44.420]   we apply these weights in front here,
[01:13:44.420 --> 01:13:46.340]   so these linear combinations of weights,
[01:13:46.340 --> 01:13:50.400]   to each of those, and sum that up,
[01:13:50.400 --> 01:13:52.460]   and obtain essentially this equivalent.
[01:13:52.460 --> 01:13:54.180]   If you only do this in one D,
[01:13:55.020 --> 01:14:00.020]   so in one D, so let's say forget these two terms here,
[01:14:00.020 --> 01:14:04.420]   so forget this, forget this,
[01:14:04.420 --> 01:14:05.860]   this and just look at this and this,
[01:14:05.860 --> 01:14:08.620]   so you have one minus A times this one,
[01:14:08.620 --> 01:14:10.100]   plus A times this one,
[01:14:10.100 --> 01:14:13.820]   as long as, let's say this was constant,
[01:14:13.820 --> 01:14:15.060]   it's the same value,
[01:14:15.060 --> 01:14:18.540]   then of course one minus A plus A, it's just one,
[01:14:18.540 --> 01:14:21.740]   so this in general will just, you know,
[01:14:21.740 --> 01:14:23.500]   it's really just a linear combination
[01:14:23.500 --> 01:14:26.580]   between a line structure between the two,
[01:14:26.580 --> 01:14:29.740]   in two D it's a bit more complicated of course,
[01:14:29.740 --> 01:14:31.540]   but this is essentially why it's called
[01:14:31.540 --> 01:14:32.780]   bilinear interpolation,
[01:14:32.780 --> 01:14:34.180]   it's kind of linear in one direction,
[01:14:34.180 --> 01:14:35.180]   linear in the other.
[01:14:35.180 --> 01:14:38.340]   Okay, now of course, as the four points
[01:14:38.340 --> 01:14:39.700]   don't have to be on a plane,
[01:14:39.700 --> 01:14:42.880]   it's not linear in two dimensions,
[01:14:42.880 --> 01:14:46.180]   but that's why it's called bilinear,
[01:14:46.180 --> 01:14:47.580]   so per dimensions linear,
[01:14:47.580 --> 01:14:48.980]   globally it actually does,
[01:14:48.980 --> 01:14:53.020]   can do a distorted, a non-linear surface.
[01:14:53.780 --> 01:14:58.780]   So this is one of the simplest algorithms,
[01:14:58.780 --> 01:15:00.340]   actually the simplest is just look at,
[01:15:00.340 --> 01:15:03.580]   you know, the simplest algorithm that we'll also see
[01:15:03.580 --> 01:15:05.300]   is like, okay, if this point,
[01:15:05.300 --> 01:15:07.500]   just look at which of the four it's closest to,
[01:15:07.500 --> 01:15:09.380]   if it's closest to this one,
[01:15:09.380 --> 01:15:11.180]   then just actually take this value,
[01:15:11.180 --> 01:15:13.860]   okay, that's the simplest reconstruction algorithm,
[01:15:13.860 --> 01:15:18.860]   that's the one that we actually had here in a sense.
[01:15:18.860 --> 01:15:20.300]   If you look at this image,
[01:15:20.300 --> 01:15:22.940]   you see we have like these constant values here,
[01:15:22.940 --> 01:15:25.100]   that's because for all the values in that range,
[01:15:25.100 --> 01:15:28.380]   we just copied the value from the central point, okay?
[01:15:28.380 --> 01:15:29.780]   So that's the simplest reconstruction
[01:15:29.780 --> 01:15:31.780]   is actually just copy the closest value,
[01:15:31.780 --> 01:15:35.500]   but that's really not a very good reconstruction algorithm,
[01:15:35.500 --> 01:15:38.260]   this is actually a quite decent reconstruction algorithm.
[01:15:38.260 --> 01:15:41.380]   We'll actually see how decent a little bit later,
[01:15:41.380 --> 01:15:44.540]   we'll look at, and again with,
[01:15:44.540 --> 01:15:46.180]   once we get to Fourier transforms
[01:15:46.180 --> 01:15:47.900]   and this understanding and so on,
[01:15:47.900 --> 01:15:50.980]   we'll actually see what different reconstruction algorithms
[01:15:50.980 --> 01:15:55.260]   actually have as an effect on the reconstruction
[01:15:55.260 --> 01:15:56.860]   and how good they actually are.
[01:15:56.860 --> 01:16:00.940]   Okay, so simple reconstruction here.
[01:16:00.940 --> 01:16:03.740]   Now we get to a really important concept
[01:16:03.740 --> 01:16:05.820]   and we'll revisit this later,
[01:16:05.820 --> 01:16:11.820]   but we'll define the Nyquist frequency
[01:16:11.820 --> 01:16:15.500]   or the Nyquist channel and sampling theorem
[01:16:15.500 --> 01:16:18.020]   as half the sampling frequency
[01:16:18.020 --> 01:16:20.660]   of a discrete signal processing system.
[01:16:21.660 --> 01:16:25.020]   The signal's max frequency bandwidth
[01:16:25.020 --> 01:16:28.100]   must be smaller than this, okay?
[01:16:28.100 --> 01:16:31.300]   It must be smaller in the sense that we can,
[01:16:31.300 --> 01:16:35.620]   if it is smaller, so with frequencies,
[01:16:35.620 --> 01:16:38.300]   we mean like the sine waves, for example,
[01:16:38.300 --> 01:16:39.420]   they have a certain frequency,
[01:16:39.420 --> 01:16:40.860]   like how long it takes to go back,
[01:16:40.860 --> 01:16:43.620]   the inverse of the wavelength,
[01:16:43.620 --> 01:16:46.260]   how long it actually takes to go through,
[01:16:46.260 --> 01:16:48.020]   this is not the wavelength of the light
[01:16:48.020 --> 01:16:48.860]   that we're talking about here,
[01:16:48.860 --> 01:16:50.700]   we're talking about the wavelength
[01:16:50.700 --> 01:16:52.100]   of the pattern in the image.
[01:16:52.100 --> 01:16:56.380]   It's like how quickly the thing goes up and down
[01:16:56.380 --> 01:17:00.100]   for what we'll talk about is we'll talk about
[01:17:00.100 --> 01:17:04.860]   representing a signal or decomposing a signal
[01:17:04.860 --> 01:17:07.660]   in just a linear combination of a whole bunch of signs
[01:17:07.660 --> 01:17:11.860]   and cosine functions and just adding all of those up
[01:17:11.860 --> 01:17:14.660]   to form, to represent the signal.
[01:17:14.660 --> 01:17:17.760]   And when we do that, we essentially,
[01:17:18.760 --> 01:17:23.760]   we essentially want to say how fast signals
[01:17:23.760 --> 01:17:30.320]   can we add to or can we still have present
[01:17:30.320 --> 01:17:32.640]   in our signal that we will sample.
[01:17:32.640 --> 01:17:37.440]   And you can kind of guess, given the previous things here,
[01:17:37.440 --> 01:17:39.640]   these kind of problems, we see that we kind of
[01:17:39.640 --> 01:17:42.680]   have to put samples close enough
[01:17:42.680 --> 01:17:47.320]   so that we don't miss like a major behavior
[01:17:47.320 --> 01:17:48.160]   in the function.
[01:17:48.160 --> 01:17:52.160]   In particular, if we look at the sine and cosine functions,
[01:17:52.160 --> 01:17:55.880]   as what this Nyquist frequency is saying,
[01:17:55.880 --> 01:18:00.560]   we need to sample at least twice per wavelength.
[01:18:00.560 --> 01:18:03.800]   So essentially here, the wavelength is like a full cycle.
[01:18:03.800 --> 01:18:06.840]   You start here, you go up, you go down, and you come back.
[01:18:06.840 --> 01:18:09.360]   And then again, you go up, down, and back.
[01:18:09.360 --> 01:18:10.560]   This is the wavelength.
[01:18:10.560 --> 01:18:14.920]   Within this, we need to make at least two measurements.
[01:18:14.920 --> 01:18:19.200]   That's what this is saying here, the sampling theorem.
[01:18:19.200 --> 01:18:22.920]   It says that if for all of the signals we have in there,
[01:18:22.920 --> 01:18:26.640]   they're superposition of all these alternating signals
[01:18:26.640 --> 01:18:30.160]   that patterns of all these things mixed together,
[01:18:30.160 --> 01:18:33.280]   as long as this is satisfied, we will be fine
[01:18:33.280 --> 01:18:36.040]   by just sampling at the rate given by this.
[01:18:36.040 --> 01:18:38.160]   So as long as we sample at least twice
[01:18:38.160 --> 01:18:43.120]   for the fastest changing thing, we sample twice as fast
[01:18:43.120 --> 01:18:45.400]   so that we really see it actually go up and down,
[01:18:45.400 --> 01:18:49.240]   essentially, that we can be confused with another signal.
[01:18:49.240 --> 01:18:51.880]   Then we'll be fine.
[01:18:51.880 --> 01:18:56.400]   We'll actually also see what happens
[01:18:56.400 --> 01:18:59.160]   and how to treat things when this is not satisfied
[01:18:59.160 --> 01:19:00.400]   later in the lecture.
[01:19:00.400 --> 01:19:04.680]   For now, we'll just assume that we can somehow
[01:19:04.680 --> 01:19:06.680]   take care that it will be smaller.
[01:19:06.680 --> 01:19:09.080]   If that's the case, then we'll be in a position
[01:19:09.080 --> 01:19:12.120]   to actually be able to do also a perfect reconstruction
[01:19:12.120 --> 01:19:13.360]   if we do the right things.
[01:19:13.360 --> 01:19:19.280]   Okay, so we'll cover all of this in a lot more detail
[01:19:19.280 --> 01:19:22.440]   in the coming weeks, but this is already some
[01:19:22.440 --> 01:19:24.280]   of the really key fundamental concepts,
[01:19:24.280 --> 01:19:25.720]   and so we'll spend more time on this,
[01:19:25.720 --> 01:19:29.000]   but it will be, you don't need to exactly understand
[01:19:29.000 --> 01:19:31.480]   all of this now, but it will be really important
[01:19:31.480 --> 01:19:33.560]   that at the end of the lecture, you really, really understand
[01:19:33.560 --> 01:19:36.040]   this because it's really some of the fundamental concepts
[01:19:36.040 --> 01:19:37.400]   that we see in this lecture.
[01:19:39.880 --> 01:19:43.800]   Sampling can be done at, doesn't have to be strictly regular.
[01:19:43.800 --> 01:19:46.680]   Theoretically, we could sample like this here,
[01:19:46.680 --> 01:19:49.800]   hexagonal or even random non-uniform.
[01:19:49.800 --> 01:19:53.080]   There are some applications for this.
[01:19:53.080 --> 01:19:57.000]   This can make sense in some situations.
[01:19:57.000 --> 01:19:59.400]   If you're talking about building a sensor,
[01:19:59.400 --> 01:20:01.880]   you want to do something regular and fixed,
[01:20:01.880 --> 01:20:03.640]   but in computer graphics, you could, for example,
[01:20:03.640 --> 01:20:07.920]   do some non-uniform samples and avoid some of these
[01:20:07.920 --> 01:20:09.960]   fixed patterns to show up or stuff like that,
[01:20:09.960 --> 01:20:12.280]   but so these things can make sense.
[01:20:12.280 --> 01:20:17.080]   You can build also different geometries of sensors and so on.
[01:20:17.080 --> 01:20:20.120]   In practice, especially in this part of the lecture,
[01:20:20.120 --> 01:20:21.920]   we'll always assume that we are sampling
[01:20:21.920 --> 01:20:23.760]   with a normal regular grid,
[01:20:23.760 --> 01:20:27.400]   a two-dimensional regular sampling grid.
[01:20:27.400 --> 01:20:30.240]   But other things are possible.
[01:20:30.240 --> 01:20:31.320]   This is one example here.
[01:20:31.320 --> 01:20:35.040]   People have experimented with doing something like this.
[01:20:35.040 --> 01:20:37.680]   Similar to the DVS camera that was trying
[01:20:37.680 --> 01:20:41.520]   to model how the eyes doesn't take frame
[01:20:41.520 --> 01:20:42.920]   and another frame and another frame,
[01:20:42.920 --> 01:20:45.560]   but actually has a very different way to sample
[01:20:45.560 --> 01:20:48.880]   and handle the time dimension in particular.
[01:20:48.880 --> 01:20:51.640]   Also, the spatial dimension is handled very different
[01:20:51.640 --> 01:20:52.480]   in human eye.
[01:20:52.480 --> 01:20:56.320]   It's not a uniform grid in two dimensions.
[01:20:56.320 --> 01:20:59.240]   It's actually something that looks much more like this.
[01:20:59.240 --> 01:21:02.560]   You have high resolution in about two degrees
[01:21:02.560 --> 01:21:04.160]   of your field of view.
[01:21:04.160 --> 01:21:06.960]   So if you're reading, your eye will actually scan the letters
[01:21:06.960 --> 01:21:09.200]   with this kind of two-degree small window
[01:21:09.200 --> 01:21:12.160]   that you can actually see high resolution.
[01:21:12.160 --> 01:21:13.720]   And in the rest, you don't actually see it.
[01:21:13.720 --> 01:21:16.720]   You have the impression, you know, the whole world is,
[01:21:16.720 --> 01:21:19.120]   you have an image of the whole world sharp,
[01:21:19.120 --> 01:21:20.600]   but that's actually not true.
[01:21:20.600 --> 01:21:24.000]   You really have to look locally to see like detail
[01:21:24.000 --> 01:21:27.040]   with your fovea, with the place where the sensors
[01:21:27.040 --> 01:21:29.960]   are very densely present in your eye.
[01:21:29.960 --> 01:21:33.520]   And in other areas, you actually have very coarse representation.
[01:21:33.520 --> 01:21:36.440]   So you could build sensors like this.
[01:21:36.440 --> 01:21:38.800]   You know, people have done that.
[01:21:38.800 --> 01:21:43.040]   This was actually a real sensor built like that.
[01:21:43.040 --> 01:21:46.160]   In practice, it's just not very practical.
[01:21:46.160 --> 01:21:48.440]   I mean, so it's just not very practical.
[01:21:48.440 --> 01:21:52.280]   And there's a lot of challenges with this in many dimensions.
[01:21:52.280 --> 01:21:54.280]   And so this doesn't really exist.
[01:21:54.280 --> 01:21:56.880]   Like nobody uses it in practice, essentially.
[01:21:56.880 --> 01:22:00.200]   Okay.
[01:22:00.200 --> 01:22:05.160]   So we talked a lot about discretizing
[01:22:05.160 --> 01:22:10.080]   in discretizing the domain in the two-dimensional domain,
[01:22:10.080 --> 01:22:12.600]   for example, on which the function is defined.
[01:22:12.600 --> 01:22:18.000]   Now we'll talk about also discretizing
[01:22:18.000 --> 01:22:19.720]   in the actual function space,
[01:22:19.720 --> 01:22:23.120]   like in the actual value that we measure.
[01:22:23.120 --> 01:22:25.960]   That one actually also has to be discretized.
[01:22:25.960 --> 01:22:26.920]   Okay.
[01:22:26.920 --> 01:22:30.200]   Typically, we'll use an integer representation.
[01:22:30.200 --> 01:22:35.040]   And so this, so deciding that representation
[01:22:35.040 --> 01:22:37.560]   is actually called quantization.
[01:22:37.560 --> 01:22:38.400]   We'll round it off.
[01:22:38.400 --> 01:22:40.760]   So we'll define like a unit,
[01:22:40.760 --> 01:22:44.000]   and then we'll represent the value as,
[01:22:44.000 --> 01:22:46.920]   you know, how many times this unit do we have.
[01:22:46.920 --> 01:22:51.920]   So we'll round off to the closest number we can represent.
[01:22:51.920 --> 01:22:54.600]   This is lossy.
[01:22:54.600 --> 01:22:57.840]   I told you before that under some conditions,
[01:22:57.840 --> 01:23:01.080]   we could perfectly reconstruct the signal, right?
[01:23:01.080 --> 01:23:04.400]   That as long as the signal wasn't varying too fast,
[01:23:04.400 --> 01:23:08.000]   the Nyquist theorem, the Nyquist frequency,
[01:23:08.000 --> 01:23:09.760]   as long as that was satisfied that, you know,
[01:23:09.760 --> 01:23:11.320]   and for now it's still a bit of hand waving,
[01:23:11.320 --> 01:23:14.840]   but as long as the signal wasn't changing too quickly,
[01:23:14.840 --> 01:23:16.600]   as long as that is strictly satisfied,
[01:23:16.600 --> 01:23:19.360]   then we'll see what it means to be strictly satisfied
[01:23:19.360 --> 01:23:20.800]   in the coming lectures.
[01:23:20.800 --> 01:23:22.880]   As long as that property was satisfied,
[01:23:22.880 --> 01:23:26.360]   then we could actually,
[01:23:26.360 --> 01:23:28.840]   there was a way to get to a perfect reconstruction
[01:23:28.840 --> 01:23:30.120]   of the original signal.
[01:23:30.120 --> 01:23:32.680]   So we could exactly invert the process of sampling.
[01:23:32.680 --> 01:23:35.640]   We could exactly go back to the original signal.
[01:23:35.640 --> 01:23:37.440]   As soon as we say, well, this is the value,
[01:23:37.440 --> 01:23:39.400]   but I can't represent this value exactly,
[01:23:39.400 --> 01:23:40.640]   I have to clip it to here,
[01:23:40.640 --> 01:23:43.840]   or, you know, like round it to a nearby value,
[01:23:43.840 --> 01:23:45.400]   okay, then we lost this property
[01:23:45.400 --> 01:23:47.560]   of being able to exactly reconstruct the signal.
[01:23:47.560 --> 01:23:50.280]   Now we'll still be able to do a good approximation,
[01:23:50.280 --> 01:23:52.040]   depending also on how many bits we spent,
[01:23:52.040 --> 01:23:54.800]   and so how much we are making rounding errors there,
[01:23:54.800 --> 01:23:59.560]   everywhere, but we do lose that exactness kind of property.
[01:24:00.920 --> 01:24:04.600]   Okay, so here you see after the quantization,
[01:24:04.600 --> 01:24:08.040]   the original signal cannot be reconstructed exactly anymore.
[01:24:08.040 --> 01:24:12.720]   It can of course be reconstructed, but not exactly.
[01:24:12.720 --> 01:24:18.440]   Okay, and so typically the way we'll choose this,
[01:24:18.440 --> 01:24:22.240]   because we work in a computer with binary representations
[01:24:22.240 --> 01:24:26.840]   and so on, the number of, the finite number that we'll use
[01:24:26.840 --> 01:24:29.360]   will typically be a power of two, right,
[01:24:29.360 --> 01:24:32.800]   because we'll spend a number of bits per pixel
[01:24:32.800 --> 01:24:34.640]   to represent the value.
[01:24:34.640 --> 01:24:38.040]   Typically, I don't know if it's indicated here,
[01:24:38.040 --> 01:24:43.640]   but typically this would be in most systems,
[01:24:43.640 --> 01:24:46.480]   it's eight bits, is the standard.
[01:24:46.480 --> 01:24:50.640]   So 256 values going from zero to 255.
[01:24:50.640 --> 01:24:53.160]   If you have a color image, it's zero to 255 for red,
[01:24:53.160 --> 01:24:57.600]   for green and for blue, so that gives you two to the 24th,
[01:24:57.600 --> 01:25:02.600]   which is what, 16 million roughly colors, for example.
[01:25:02.600 --> 01:25:06.720]   Right, so quantization works like this.
[01:25:06.720 --> 01:25:09.480]   This was our function, we have our sample points.
[01:25:09.480 --> 01:25:11.920]   We'll actually see later why there's some errors here.
[01:25:11.920 --> 01:25:19.520]   Let's say these are, we spend two bits in this case,
[01:25:19.520 --> 01:25:23.600]   so we'll have four values, zero, zero, zero, one, one, zero,
[01:25:23.600 --> 01:25:27.200]   one, one, those are our four numbers that we can represent.
[01:25:27.200 --> 01:25:28.920]   So we have to clip it to the closest one
[01:25:28.920 --> 01:25:30.920]   or round it to the closest one.
[01:25:30.920 --> 01:25:33.160]   So this one goes to zero, this one goes to one,
[01:25:33.160 --> 01:25:37.360]   to two and to three, et cetera.
[01:25:37.360 --> 01:25:39.200]   And so you see you have significant errors,
[01:25:39.200 --> 01:25:42.480]   but that's essentially what you would have to do, right?
[01:25:42.480 --> 01:25:43.480]   Of course, hopefully you can spend
[01:25:43.480 --> 01:25:44.840]   more than two bits per pixel.
[01:25:44.840 --> 01:25:48.560]   It's also important to discern
[01:25:48.560 --> 01:25:51.040]   between a few different types of resolution.
[01:25:51.040 --> 01:25:53.200]   So we have the image resolution,
[01:25:53.200 --> 01:25:55.680]   that's simply the size of the image,
[01:25:55.680 --> 01:25:57.600]   how many pixels by how many pixels.
[01:25:57.600 --> 01:26:00.040]   But you can also have something
[01:26:00.040 --> 01:26:02.640]   that's also related to that,
[01:26:02.640 --> 01:26:05.440]   which is the geometric resolution.
[01:26:05.440 --> 01:26:08.920]   That's more in, let's say,
[01:26:08.920 --> 01:26:13.040]   if you're in a remote sensing situation, for example,
[01:26:13.040 --> 01:26:15.680]   or in a more specific situation,
[01:26:15.680 --> 01:26:19.800]   it's essentially not how many pixels do you have in total,
[01:26:19.800 --> 01:26:22.840]   but it's how many pixels do you have per unit
[01:26:22.840 --> 01:26:24.360]   in the real world.
[01:26:24.360 --> 01:26:25.720]   So if you're looking at,
[01:26:25.720 --> 01:26:30.040]   from the air, you're taking pictures of the ground,
[01:26:30.040 --> 01:26:32.160]   how big is a pixel on the ground, actually,
[01:26:32.160 --> 01:26:34.640]   the projection of a pixel on the ground, yes?
[01:26:34.640 --> 01:26:36.960]   - Does HDR just use more bits per pixel?
[01:26:36.960 --> 01:26:37.800]   - Sorry?
[01:26:37.800 --> 01:26:40.160]   - Does HDR just use more bits per pixel?
[01:26:40.160 --> 01:26:46.920]   - Yes, so high dynamic range images, so HDR,
[01:26:46.920 --> 01:26:47.840]   uses,
[01:26:47.840 --> 01:26:51.440]   well, actually there,
[01:26:51.440 --> 01:26:54.680]   so HDR would, I think the formats there,
[01:26:54.680 --> 01:26:58.760]   typically would actually use a floating point representation,
[01:26:58.760 --> 01:27:01.640]   but it depends, but,
[01:27:01.640 --> 01:27:03.640]   I mean, you can have different versions,
[01:27:03.640 --> 01:27:06.840]   but ideally, you could really go to floating point,
[01:27:06.840 --> 01:27:10.960]   you can also just have 10, 12, 16 bits or so per pixel,
[01:27:10.960 --> 01:27:13.560]   or per value, so yes.
[01:27:13.560 --> 01:27:15.800]   Different things are possible there,
[01:27:15.800 --> 01:27:17.160]   and different formats exist.
[01:27:19.680 --> 01:27:20.760]   Okay, so,
[01:27:20.760 --> 01:27:26.800]   so you can have, if you're, or,
[01:27:26.800 --> 01:27:28.400]   let's say actually you would be in biology,
[01:27:28.400 --> 01:27:30.200]   and you would look at through a microscope
[01:27:30.200 --> 01:27:33.440]   and take images also, you might care about,
[01:27:33.440 --> 01:27:36.560]   how many millimeters, or fractional millimeter,
[01:27:36.560 --> 01:27:38.800]   and nanometers or so is every pixel.
[01:27:38.800 --> 01:27:41.400]   So really, to have that link directly
[01:27:41.400 --> 01:27:43.520]   to a dimension in space,
[01:27:43.520 --> 01:27:45.520]   so that's this geometric resolution,
[01:27:45.520 --> 01:27:47.560]   and then we have the radiometric resolution,
[01:27:47.560 --> 01:27:50.160]   which is how many bits per pixel do we spend.
[01:27:50.160 --> 01:27:55.360]   So here's the image resolution,
[01:27:55.360 --> 01:27:56.880]   so you see that the geometric resolution
[01:27:56.880 --> 01:27:58.080]   actually doesn't change,
[01:27:58.080 --> 01:27:59.880]   we just crop the image differently.
[01:27:59.880 --> 01:28:05.240]   The, here the geometric resolution is we vary actually
[01:28:05.240 --> 01:28:06.080]   for the same picture,
[01:28:06.080 --> 01:28:08.800]   we vary how many pixels we have in the image,
[01:28:08.800 --> 01:28:11.880]   and it gets coarser and coarser, as you see here.
[01:28:11.880 --> 01:28:15.560]   Okay, and then we have the radiometric resolution,
[01:28:15.560 --> 01:28:18.320]   where now we have the same amount of pixels,
[01:28:18.320 --> 01:28:22.720]   but we spend, you know, from here eight bits per pixel,
[01:28:22.720 --> 01:28:26.400]   we go all the way down to only one bit per pixel,
[01:28:26.400 --> 01:28:28.080]   which just means really, you know,
[01:28:28.080 --> 01:28:29.640]   is this brighter than a value,
[01:28:29.640 --> 01:28:32.000]   or darker than a value in the end, right?
[01:28:32.000 --> 01:28:33.480]   So you just have two levels now.
[01:28:33.480 --> 01:28:35.440]   Okay.
[01:28:35.440 --> 01:28:42.560]   So, yeah, so maybe a question here.
[01:28:43.520 --> 01:28:46.840]   So what's the disadvantage of a low sampling resolution?
[01:28:46.840 --> 01:28:57.640]   Oh, yes?
[01:28:57.640 --> 01:29:05.760]   Yes, so we'll have aliasing,
[01:29:05.760 --> 01:29:07.280]   we'll have problems like that.
[01:29:07.280 --> 01:29:09.840]   So we could say, okay,
[01:29:09.840 --> 01:29:13.200]   we'll always use a very high sampling resolution,
[01:29:13.200 --> 01:29:14.800]   what's the disadvantage of that?
[01:29:14.800 --> 01:29:19.640]   Assuming higher storage and computation costs,
[01:29:19.640 --> 01:29:21.600]   when it comes to storage or computing,
[01:29:21.600 --> 01:29:22.760]   we have to convert it to...
[01:29:22.760 --> 01:29:25.320]   Exactly, right, so we'll kind of be forced
[01:29:25.320 --> 01:29:27.280]   to do a trade-off there somewhere.
[01:29:27.280 --> 01:29:29.080]   And so we'll have to kind of deal with the issue,
[01:29:29.080 --> 01:29:31.080]   we don't have an easy way out, essentially.
[01:29:31.080 --> 01:29:34.360]   And so as you already see here,
[01:29:34.360 --> 01:29:36.760]   there's, you know, we'll kind of,
[01:29:36.760 --> 01:29:39.840]   this hints that we might compress or not the images,
[01:29:39.840 --> 01:29:42.600]   so because these are actually properties of compression,
[01:29:42.600 --> 01:29:46.120]   lossless and lossy image representations,
[01:29:46.120 --> 01:29:48.200]   but essentially lossy means
[01:29:48.200 --> 01:29:49.960]   that we'll do some kind of compression,
[01:29:49.960 --> 01:29:51.200]   we'll try to spend less bits
[01:29:51.200 --> 01:29:54.600]   than we would strictly just, you know,
[01:29:54.600 --> 01:29:56.960]   naively represent the image.
[01:29:56.960 --> 01:30:00.400]   Do you guys know some formats for pictures?
[01:30:00.400 --> 01:30:05.400]   Yes, exactly.
[01:30:05.400 --> 01:30:07.080]   So there's many different formats,
[01:30:07.080 --> 01:30:10.880]   and indeed JPEG, and then there's also variants of JPEG,
[01:30:10.880 --> 01:30:14.440]   but essentially JPEG is the classical kind
[01:30:14.440 --> 01:30:17.920]   of lossy compression, and it's actually one we'll see
[01:30:17.920 --> 01:30:20.600]   in detail in, I think the fourth week,
[01:30:20.600 --> 01:30:25.440]   if I'm fourth or fifth week, we'll discuss in detail.
[01:30:25.440 --> 01:30:28.360]   Also JPEG 2000, which, although it says 2000,
[01:30:28.360 --> 01:30:30.240]   is still not very adopted yet,
[01:30:30.240 --> 01:30:33.560]   and doesn't seem like it will be picked up so much,
[01:30:33.560 --> 01:30:36.680]   but has some very nice properties.
[01:30:36.680 --> 01:30:41.240]   But yes, so essentially representing raw images
[01:30:41.240 --> 01:30:44.560]   with full format can become quite expensive,
[01:30:44.560 --> 01:30:47.960]   and so many images actually all represented
[01:30:47.960 --> 01:30:51.600]   in compressed formats, especially because our eyes
[01:30:51.600 --> 01:30:54.240]   actually really, there's a lot of stuff you can get away with,
[01:30:54.240 --> 01:30:55.840]   and we wouldn't really notice.
[01:30:55.840 --> 01:30:58.920]   So therefore there's not a high cost of,
[01:30:58.920 --> 01:31:02.480]   you can compress by a significant amount,
[01:31:02.480 --> 01:31:04.400]   say easily a factor of five,
[01:31:04.400 --> 01:31:05.800]   and really not notice anything,
[01:31:05.800 --> 01:31:09.000]   even without using too complicated compression mechanisms.
[01:31:09.000 --> 01:31:11.680]   Because also by the way in compression,
[01:31:11.680 --> 01:31:14.640]   there also the computational power would matter.
[01:31:14.640 --> 01:31:16.720]   You could do very, very fancy compression,
[01:31:16.720 --> 01:31:21.320]   but that could actually be very costly,
[01:31:21.320 --> 01:31:24.040]   in both in compression and also in decompression.
[01:31:24.040 --> 01:31:25.920]   So reading your image from this can,
[01:31:25.920 --> 01:31:27.960]   putting it on the screen, you don't wanna spend
[01:31:27.960 --> 01:31:30.200]   too much effort on having to do computations
[01:31:30.200 --> 01:31:31.360]   to make that happen.
[01:31:33.000 --> 01:31:36.240]   So we'll see also that's why JPEG has a few trade-offs
[01:31:36.240 --> 01:31:37.640]   that is done the way it is.
[01:31:37.640 --> 01:31:42.080]   Actually also, not only computations,
[01:31:42.080 --> 01:31:43.440]   there's also actually the question,
[01:31:43.440 --> 01:31:46.160]   and in particular with video, that's also the case.
[01:31:46.160 --> 01:31:49.200]   A big part is also not only the compute cost,
[01:31:49.200 --> 01:31:50.960]   but actually can you build hardware
[01:31:50.960 --> 01:31:53.040]   that can do this very efficiently?
[01:31:53.040 --> 01:31:57.360]   Because if your phone had to just run a compression,
[01:31:57.360 --> 01:32:00.120]   standard video compression algorithm on the CPU
[01:32:00.120 --> 01:32:03.960]   of the phone, like on the standard CPU,
[01:32:03.960 --> 01:32:06.760]   run some stuff, you would never be able
[01:32:06.760 --> 01:32:09.840]   to actually record videos and store them on your phone.
[01:32:09.840 --> 01:32:11.600]   So you can only do that because there's actually
[01:32:11.600 --> 01:32:13.800]   custom hardware that does just that,
[01:32:13.800 --> 01:32:15.760]   that is specialized to do these operations
[01:32:15.760 --> 01:32:17.640]   that you need for video compression.
[01:32:17.640 --> 01:32:19.320]   That also means that you need something
[01:32:19.320 --> 01:32:21.640]   that's actually efficient,
[01:32:21.640 --> 01:32:23.640]   that can actually efficiently be mapped to hardware.
[01:32:23.640 --> 01:32:25.160]   And not a very complicated algorithm
[01:32:25.160 --> 01:32:26.360]   that's gonna look all over the image
[01:32:26.360 --> 01:32:28.200]   and figure things out, right?
[01:32:28.200 --> 01:32:29.760]   Okay, so we'll leave it at that.
[01:32:30.440 --> 01:32:32.400]   For today, I'll see you next week.
[01:32:32.400 --> 01:32:35.560]   (audience applauding)
[01:32:35.560 --> 01:32:37.720]   [APPLAUSE]

