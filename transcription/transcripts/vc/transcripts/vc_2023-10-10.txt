
[00:00:00.000 --> 00:00:09.520]   Okay, good morning. So today we'll talk about the Fourier transform. It's already announced
[00:00:09.520 --> 00:00:18.720]   this earlier, this is a really important part of the lecture. So let's actually start with
[00:00:18.720 --> 00:00:31.920]   a little video. You know, just, I don't know actually if I have the sound. But you know,
[00:00:31.920 --> 00:00:42.320]   just to, okay I guess there's no sound, I think. Okay, well unfortunately no sound.
[00:00:42.320 --> 00:00:51.120]   Anyway, so you might recognize this from the Blade Runner movie, the first one. Anyways,
[00:00:51.120 --> 00:00:58.400]   it's the sequence where he's kind of step by step zooming in on, you know, a photograph,
[00:00:58.400 --> 00:01:26.520]   scan photograph. Actually maybe there's sound. Actually there is sound. So you see these often
[00:01:26.520 --> 00:01:32.280]   in movies, this kind of zooming in and zooming in and zooming in more and even in a sense
[00:01:32.280 --> 00:01:39.600]   changing viewpoint a little bit, things like this. So this, of course there's limits. And
[00:01:39.600 --> 00:01:53.440]   so as we look at this, the goal of these lectures and in general of this course, one of the
[00:01:53.440 --> 00:02:00.760]   things is for you to really understand what those limits are, what makes sense, what's
[00:02:00.760 --> 00:02:27.960]   feasible, how far you can push things and you know, beyond what point, it's just pure imagination.
[00:02:27.960 --> 00:02:43.440]   So let's first actually look a little bit back at what you saw last week with Züria.
[00:02:43.440 --> 00:02:51.280]   Also if you have any questions related to this, don't hesitate to ask. So the core concept
[00:02:51.280 --> 00:03:00.840]   last week was really this two operations which are really very closely related. I think Züria
[00:03:00.840 --> 00:03:07.280]   was even saying that there was some confusion going around and this indeed has to do with
[00:03:07.280 --> 00:03:13.720]   some of the ways that some of this is used in neural networks. But essentially there's
[00:03:13.720 --> 00:03:21.040]   two main concepts that end up mathematically with more or less the same representation
[00:03:21.040 --> 00:03:29.000]   up to a mirroring of the kernel. But conceptually they are very different. And actually the filters
[00:03:29.000 --> 00:03:35.560]   that you use or the kernels you use will typically also be different. So one of those
[00:03:35.560 --> 00:03:41.200]   is correlation. And as you know of the word correlate and so on, it's looking for things
[00:03:41.200 --> 00:03:46.440]   that have some similarities are related to each other. That's what correlations are about
[00:03:46.440 --> 00:03:54.320]   in terms of semantically what the word means. So when you read correlation you should think
[00:03:54.320 --> 00:04:01.600]   of template matching. So if we have an image processing, we do correlations, we essentially
[00:04:01.600 --> 00:04:12.680]   talk about comparing a template to the image and seeing where we get a good response. That
[00:04:12.680 --> 00:04:18.560]   also means that typically to avoid being biased to just a bright spot in the image, if you
[00:04:18.560 --> 00:04:26.320]   just do the correlation without normalizing it and having a zero mean, you will always
[00:04:26.320 --> 00:04:31.200]   have the strongest correlation with the strongest positive signal. That's not very useful. So
[00:04:31.200 --> 00:04:41.080]   typically the template with the question mark, typically you would actually have a template
[00:04:41.080 --> 00:04:48.680]   that has zero average. So that on average with a random image it has zero correlation.
[00:04:48.680 --> 00:04:56.000]   So typically that's the case with for correlation you would have kernels with zero average. And
[00:04:56.000 --> 00:05:04.800]   essentially you're looking for the pattern in your template. You're going over the image
[00:05:04.800 --> 00:05:11.440]   and everywhere you apply this correlation, you essentially in a sense you look at this
[00:05:11.440 --> 00:05:18.480]   here, this for example three by three matrix, you look at this, you vectorize that, you
[00:05:18.480 --> 00:05:24.280]   make a vector out of it, so a nine vector. And then for every patch of the image you
[00:05:24.280 --> 00:05:27.480]   will also generate a nine vector and then you will see how correlated those vectors
[00:05:27.480 --> 00:05:32.520]   are. So if there's zero average they will correlate well with something that's well
[00:05:32.520 --> 00:05:37.560]   aligned, they will anti-correlate with something that's exactly the opposite pattern. With
[00:05:37.560 --> 00:05:42.760]   linear systems, as I said with linear systems you can avoid that. If you're positively correlated
[00:05:42.760 --> 00:05:48.840]   with a signal you are automatically negatively correlated with the opposite signal. That's
[00:05:48.840 --> 00:05:57.000]   where in neural networks they insert functions, non-linear functions that can separate those
[00:05:57.000 --> 00:06:01.520]   and say okay positive correlation I care about, the negative correlation I'll ignore, I'll
[00:06:01.520 --> 00:06:08.000]   put a zero there and I'll clip it off. But anyways, this is about computing correlation.
[00:06:08.000 --> 00:06:14.760]   So it means that for the point i you have the corresponding shift in the image, you're
[00:06:14.760 --> 00:06:24.320]   shifted by the same amount i. So your template and your here, your template and the position
[00:06:24.320 --> 00:06:28.600]   in the template and the position in the image relative to the central point is the same
[00:06:28.600 --> 00:06:34.760]   offset. Because you really want to compare apple to apples. So you compare the template
[00:06:34.760 --> 00:06:43.480]   directly overlaid on the image. That's this one. The convolution is something completely
[00:06:43.480 --> 00:06:54.560]   different and you should conceptually think about it as an operation as the one there.
[00:06:54.560 --> 00:07:03.200]   So point spread function for example. So for example if you, it means imagine you have,
[00:07:03.200 --> 00:07:09.280]   you know you would normally measure everything at exactly one location but then there's some
[00:07:09.280 --> 00:07:14.720]   stuff happening, it gets a little bit spread around. Imagine you put a whole pile of things
[00:07:14.720 --> 00:07:18.400]   exactly in one location but then you shake things a little bit and they kind of fall
[00:07:18.400 --> 00:07:25.120]   a bit around it and they spread themselves out. That's what this model will model in
[00:07:25.120 --> 00:07:29.760]   the sense. It will say okay for something that should have landed here how do I distribute
[00:07:29.760 --> 00:07:35.680]   it? For example this is a great model for a blurred image. Okay the perfect image with
[00:07:35.680 --> 00:07:41.000]   a perfect lens, perfectly sharp, you know focused would put everything on exactly this
[00:07:41.000 --> 00:07:47.680]   pixel would translate things coming from that specific direction in space to the lens, to
[00:07:47.680 --> 00:07:55.640]   the pinhole and would have that all land exactly on a single pixel. But because the lens is
[00:07:55.640 --> 00:08:03.200]   not perfectly focused or any other issues it will actually be a little bit spread around.
[00:08:03.200 --> 00:08:13.320]   That modeling that is we take that value here and we will distribute it to all the neighbors.
[00:08:13.320 --> 00:08:23.920]   So if we want to model that in the same way not as something where we take a value and
[00:08:23.920 --> 00:08:34.080]   distribute it but as an operation similar to this one as an operation not on the output
[00:08:34.080 --> 00:08:44.680]   and then distribute it on the output somehow but more on the input and so take essentially
[00:08:44.680 --> 00:08:52.120]   here for a particular which image values will land in a particular pixel. So at this pixel
[00:08:52.120 --> 00:08:58.520]   here the output function what pixels will end up influencing that so that means that
[00:08:58.520 --> 00:09:05.240]   well whatever was the signal here you know part of it if I shake this pixel part of it
[00:09:05.240 --> 00:09:12.640]   will land there. And it's actually what should go to the right of it but this is the pixel
[00:09:12.640 --> 00:09:18.520]   on the left. But it's what from this pixel would land on the right is what this pixel
[00:09:18.520 --> 00:09:27.280]   will end up impacting on the output side here. For example this pixel if I shake this pixel
[00:09:27.280 --> 00:09:32.600]   around I spread it a bit around then some of it will land there. What will land there
[00:09:32.600 --> 00:09:38.240]   is what from this pixel is on the upper side of the kernel. But of course if I compute
[00:09:38.240 --> 00:09:45.880]   for this pixel then I kind of end up with from this pixel it's this is the value one
[00:09:45.880 --> 00:09:52.120]   down but I have to use in the kernel the value one up okay for seeing what impacts there.
[00:09:52.120 --> 00:09:56.640]   So that's why I end up here with exactly the same formulation except I have minus signs
[00:09:56.640 --> 00:10:01.960]   here okay the minus sign can be here or here you can switch it around but essentially you
[00:10:01.960 --> 00:10:09.920]   have a mirroring of the kernel here. But for the rest my triangle is the same and of course
[00:10:09.920 --> 00:10:14.760]   probably many of you have noticed wait a moment you're flipping the kernel so you know like
[00:10:14.760 --> 00:10:19.560]   here I flip an image but I could do a plus here and a minus here it's the same thing
[00:10:19.560 --> 00:10:26.760]   it's just changing it actually doesn't change anything. And so putting minus signs here
[00:10:26.760 --> 00:10:30.760]   is the same as actually doing a mirroring I mean a point mirroring so mirroring like
[00:10:30.760 --> 00:10:37.880]   this and like this a point mirroring of the kernel right by just putting minus signs here
[00:10:37.880 --> 00:10:44.200]   it's the same as replacing this keeping plus signs but replacing it with a mirror kernel
[00:10:44.200 --> 00:10:49.360]   and of course if my kernel is symmetric you know I haven't done anything because mirroring
[00:10:49.360 --> 00:10:54.720]   a symmetric kernel like that one well that's you know doing nothing. So that's where a lot
[00:10:54.720 --> 00:10:59.680]   of the confusion comes from and where people use often convolution and correlation mix
[00:10:59.680 --> 00:11:07.080]   up you know like inconsistently or don't pay attention which one they use. But I want for
[00:11:07.080 --> 00:11:11.360]   this lecture I want you really to be able to make the difference between the two the
[00:11:11.360 --> 00:11:16.880]   conceptual difference between the two. Even if in practice it doesn't matter much but
[00:11:16.880 --> 00:11:20.480]   it's important to know at the origin where the two come from because they really are
[00:11:20.480 --> 00:11:25.760]   representing completely different their models for the really different things even if in
[00:11:25.760 --> 00:11:33.080]   the end you can mathematically end up with the same thing. Actually here again the visualization
[00:11:33.080 --> 00:11:39.000]   so for that one you know you move from that pixel you do the point spread function and
[00:11:39.000 --> 00:11:47.040]   you move things over here or so. So like it's for the upper left corner you end up evaluating
[00:11:47.040 --> 00:11:55.960]   the thing with the down right and so on right. So here's the visualization of that. Okay so
[00:11:55.960 --> 00:12:08.480]   is that was that clear to everyone or do you have any questions related to this? Okay then
[00:12:08.480 --> 00:12:14.960]   what you also saw last week was edges and different types of features so how to extract
[00:12:14.960 --> 00:12:22.400]   edges starting with looking at high gradient regions so high gradient regions are the gradient
[00:12:22.400 --> 00:12:28.840]   is just how quickly you know neighboring pixels change are different from each other. So those
[00:12:28.840 --> 00:12:33.920]   are of course where there are edges so that's the place to start and then you saw the type
[00:12:33.920 --> 00:12:39.840]   of operations you can do to actually retrieve change of these edge pixels and so therefore
[00:12:39.840 --> 00:12:46.680]   retrieve connected edges in the image. You saw the Kani edge detector which you know
[00:12:46.680 --> 00:12:52.800]   essentially is looking at these high gradient regions or high gradient you know you can
[00:12:52.800 --> 00:12:57.480]   label the high gradient regions but then you look for local maxima in that and of course
[00:12:57.480 --> 00:13:03.280]   you really care about local maxima across the direction of across the edge along the
[00:13:03.280 --> 00:13:08.960]   edge you actually want to connect pixels in connected edges right so you have an edge is
[00:13:08.960 --> 00:13:14.360]   the separation between two regions across this direction we want to find the strongest
[00:13:14.360 --> 00:13:18.840]   area for the edge because you could have a slope and so you're caring about the steepest
[00:13:18.840 --> 00:13:22.720]   part of the slope that's where you want to localize the edge but in the other direction
[00:13:22.720 --> 00:13:26.440]   along the edge of course you just want to chain all those pixels together right so that's
[00:13:26.440 --> 00:13:30.040]   where you did the difference between those two directions you have to treat them separately
[00:13:30.040 --> 00:13:37.920]   connect along the edge and essentially find the maximum across the edge. You saw a few
[00:13:37.920 --> 00:13:44.080]   other concepts therefore edges you also saw the concept of as we said with region growing
[00:13:44.080 --> 00:13:49.360]   so the edges also can be seen as a specific type of you know linear region and so the
[00:13:49.360 --> 00:13:55.480]   concept of saying okay if I start from a strong edge I can do region growing even to slightly
[00:13:55.480 --> 00:14:00.520]   less clear edges because if I found a nice edge to start with I might want to extend
[00:14:00.520 --> 00:14:05.640]   it also in regions where it becomes a little bit less obvious so we had these two levels
[00:14:05.640 --> 00:14:16.360]   of thresholds for doing that. Then there was the Huff transform the Huff transform is an
[00:14:16.360 --> 00:14:24.840]   interesting transform that transforms from a pixel space to a parmeter space for example
[00:14:24.840 --> 00:14:31.640]   for lines it will transform from the XY pixel coordinates it will essentially determine
[00:14:31.640 --> 00:14:42.760]   for every pixel coordinate it will determine for what parmeter does it vote for so in other
[00:14:42.760 --> 00:14:50.200]   words what are the lines that go through this point and then that line that point sorry would
[00:14:50.200 --> 00:14:55.640]   vote for all these possible lines and if you in the parmeter space accumulate the votes of all
[00:14:55.640 --> 00:15:01.080]   the points you can quickly end up with a peak where there is a real line supported by many points
[00:15:01.080 --> 00:15:04.600]   because if there is essentially a hundred points on the lines then you will get a hundred votes
[00:15:04.600 --> 00:15:10.760]   for that particular parmeter while other spurious you know where there's just one line goes through
[00:15:10.760 --> 00:15:15.160]   but it's kind of just random those would not get a lot of support for no data right.
[00:15:15.880 --> 00:15:20.520]   You could actually think of extending that and making it more direct if you say well I actually
[00:15:20.520 --> 00:15:25.880]   have an idea of my gradient at my point as an example you could say I'm not going to vote for
[00:15:25.880 --> 00:15:30.680]   all possible lines going through this point I could vote for example for just the lines that are
[00:15:30.680 --> 00:15:36.200]   approximately aligned with the gradient and that way you could you know for example refine
[00:15:36.200 --> 00:15:41.480]   your vote already and only vote for the plausible lines that somewhere are aligned with the you
[00:15:41.480 --> 00:15:44.760]   know with the gradient that you observe at a particular location as one possible extension
[00:15:44.760 --> 00:15:49.480]   many many possible extensions there you saw that this is this can be done for lines this can be
[00:15:49.480 --> 00:15:56.120]   done for circles this can be done for any parametric representation as long due to curse of dimensionality
[00:15:56.120 --> 00:16:07.640]   as long as it's not too high dimensional. Then corners these are essentially areas like in this
[00:16:07.640 --> 00:16:13.880]   example here where you're not just looking for a single gradient you're looking for kind of
[00:16:13.880 --> 00:16:18.840]   unique points and so you're not looking for a single gradient like in an edge but you're now
[00:16:18.840 --> 00:16:24.360]   looking for you know gradients one way and gradients the other way coming together in one location.
[00:16:24.360 --> 00:16:31.720]   So we're looking where of course at every single point at a single point you only have one gradient
[00:16:31.720 --> 00:16:36.760]   but we're looking for a small neighborhood where we have multiple gradients showing up
[00:16:36.760 --> 00:16:42.040]   we've got a strong edge in one direction then if I look at the neighborhood I will actually
[00:16:42.040 --> 00:16:46.520]   have all the gradients locally you know less and more strong and so on but they'll all be aligned
[00:16:46.520 --> 00:16:52.360]   and if I accumulate them in this matrix I have essentially this is an inner product
[00:16:52.360 --> 00:17:00.360]   you can also see this as a sum of inner products of the gradients and of the sorry of the other
[00:17:00.360 --> 00:17:05.640]   product of the gradients and that would if all of those vectors are aligned it would by definition
[00:17:05.640 --> 00:17:11.880]   give you a rank one matrix if all of them are the same vectors because it's a rank one
[00:17:11.880 --> 00:17:16.920]   matrix times the rank one matrix so it's a vector times a vector that would be a rank one matrix.
[00:17:16.920 --> 00:17:21.800]   So it's only if there's actually different gradients coming together in a small patch
[00:17:21.800 --> 00:17:28.360]   in a small window which can really be just a few pixels but if multiple gradients are coming
[00:17:28.360 --> 00:17:34.840]   together like in a corner then you will actually have a full rank matrix here okay and that's
[00:17:34.840 --> 00:17:40.760]   essentially what you know you've seen in different ways a full rank matrix would indicate that you
[00:17:40.760 --> 00:17:46.360]   have a corner or you know like cornerness and then the corners themselves identified again
[00:17:46.360 --> 00:17:52.920]   as with the edges will find the local maxima of this so we have many points in the neighborhood that
[00:17:52.920 --> 00:18:00.120]   with a particular integration window we'll have you know we'll see two gradients two different
[00:18:00.120 --> 00:18:04.600]   orientations of gradients but then we're looking for the strongest one the best localized one
[00:18:04.600 --> 00:18:08.680]   yeah we have to do that if we want really corners well localized corners we have to do that with a
[00:18:08.680 --> 00:18:15.560]   small a small aperture and ideally one not a flat filter but a filter that has a peak because
[00:18:15.560 --> 00:18:20.440]   otherwise if it's flat it will you can freely shift it around within the flat region and not
[00:18:20.440 --> 00:18:26.280]   see a difference so you need a peaked thing like a Gaussian to evaluate to integrate this this on
[00:18:26.280 --> 00:18:32.440]   here so the window here should actually be a Gaussian to really have a corner anyway so and
[00:18:32.440 --> 00:18:41.160]   then I think the the last one on the last kind of slide is the sift transform now there are many
[00:18:41.160 --> 00:18:48.520]   machine learned feature detectors like superpoint is one example and then many others
[00:18:48.520 --> 00:18:54.280]   that have been trained on many examples of corners and images and all kinds of things
[00:18:55.800 --> 00:19:02.680]   but before that for for you know 15 years or so this was the dominant and it's still today
[00:19:02.680 --> 00:19:07.480]   actually in many systems it's still the dominant feature that's used and in many practical settings
[00:19:07.480 --> 00:19:15.240]   now they are better features that have been learned but it's still competitive and it's still
[00:19:15.240 --> 00:19:20.760]   used in many many existing systems and in on real world scenes and if you don't have priors and it's
[00:19:20.760 --> 00:19:25.240]   you just want to generally apply it it's still one of the most used features there and works really
[00:19:25.240 --> 00:19:32.280]   well the important thing is also if you then want to later design your own learned features and this
[00:19:32.280 --> 00:19:36.840]   and that the important thing is to understand a little bit the main concepts that are leveraged here
[00:19:36.840 --> 00:19:42.680]   at the at the principle level and then because those are probably good inspirations for your
[00:19:42.680 --> 00:19:47.320]   network structures for you know all of the things that you might want to put in place for the network
[00:19:47.320 --> 00:19:55.000]   to be able to efficiently compute things okay so the first thing is we're not only doing a local
[00:19:55.000 --> 00:20:02.760]   maxima in the x y positions here we're actually also doing it in a scale pyramid okay so we want
[00:20:02.760 --> 00:20:12.760]   to find a blob again how do we find a blob well we actually do convolution of actually sorry correlation
[00:20:12.760 --> 00:20:22.920]   we do correlation of of a blob with the image and we see where we get the strongest response now
[00:20:22.920 --> 00:20:29.320]   as i told you a blob is actually not a good idea because a blob is fully positive so we actually
[00:20:29.320 --> 00:20:34.120]   need to do something different what we will do we will actually do a difference of of blobs okay
[00:20:34.120 --> 00:20:41.480]   it's written there difference do g difference of gaussians so you can implement that efficiently
[00:20:41.480 --> 00:20:46.920]   because you can implement the scale pyramid and what we'll do is we'll do a difference of a large
[00:20:46.920 --> 00:20:52.840]   blob with a peak a smaller blob so the smaller blob is the peak the blob will actually look for
[00:20:52.840 --> 00:20:59.320]   a bigger blob but we'll average it out to have a zero response by subtracting from it a wider blob
[00:20:59.320 --> 00:21:04.760]   okay so we'll have something that goes like this and and then goes to zero so we want to find an
[00:21:04.760 --> 00:21:11.160]   extent of course but so we'll have essentially a big blob like a peak blob within a flatter blob
[00:21:11.160 --> 00:21:16.520]   subtracted from it they have the same integral so they average out to exactly zero but that way
[00:21:16.520 --> 00:21:22.120]   we have essentially a response like this so positive negative and then to zero okay but in the end
[00:21:22.120 --> 00:21:27.560]   that looks like a blob i don't have a picture here but i'm sure there was a picture in the in the
[00:21:27.560 --> 00:21:32.520]   slides so it's essentially something that looks like a blob within a little bit of negative around
[00:21:32.520 --> 00:21:38.120]   it and then it averages out to zero it will find you a blob of that particular size okay of the
[00:21:38.120 --> 00:21:44.680]   inner of this inner blob it will it will correlate very well with these type of blobs okay and now
[00:21:44.680 --> 00:21:50.680]   what do you do you can correlate this over the whole image you get you will get the strongest
[00:21:50.680 --> 00:21:56.440]   response where there is kind of a nice blob of that shape that's kind of circular blob kind of
[00:21:56.440 --> 00:22:03.000]   feature like a bright patch or a dark patch remember it will correlate equally with a positive or
[00:22:03.000 --> 00:22:08.440]   negative patch you would have an equally strong response just the sign will change okay so you
[00:22:08.440 --> 00:22:17.000]   don't have to look separately for bright blobs and dark blobs the and it's really the contrast to
[00:22:17.000 --> 00:22:21.880]   the neighborhood to the direct neighborhood that matters right because we subtract this blob from
[00:22:21.880 --> 00:22:31.400]   the twice as large blob then we not only do that in location but we actually also do the correlation
[00:22:31.400 --> 00:22:39.080]   across scales so what are we doing and and we and what we do is we then take the maxima in the spatial
[00:22:39.080 --> 00:22:45.960]   position x y but also in the scale okay so we'll essentially what it means is we'll try to correlate
[00:22:45.960 --> 00:22:51.000]   with a blob this size but also with a blob this size and a blob this size and a blob this size
[00:22:51.000 --> 00:22:56.440]   and we'll see which one actually gives us the strongest response okay well when do we get the
[00:22:56.440 --> 00:23:01.640]   strongest response when our blob that we fit is kind of the same size as the visual blob we see in
[00:23:01.640 --> 00:23:05.960]   the image okay then we'll have a really strong response if it's a bit too big or a bit too small
[00:23:05.960 --> 00:23:10.840]   we'll get a less good response because some of the some of the bright region will kind of you
[00:23:10.840 --> 00:23:16.600]   know be cancelled out will come negatively because our filter has a negative effect on the periphery
[00:23:16.600 --> 00:23:23.000]   and so essentially you'll get things like this here for example right or like this here so you get
[00:23:23.000 --> 00:23:27.960]   the inner blob is here so this is darker and then outside is brighter no this is not exactly a blob
[00:23:27.960 --> 00:23:34.040]   here it's actually some triangle feature but of course this is let's say darker this is brighter
[00:23:34.040 --> 00:23:38.600]   this is also brighter and here it's also brighter so together this actually responds quite well
[00:23:39.160 --> 00:23:43.320]   and a bigger blob or a smaller blob will have a less good response and a shifted blob would also
[00:23:43.320 --> 00:23:48.920]   have a less good response okay so this will then be a local maxima at this scale okay and then we
[00:23:48.920 --> 00:23:56.200]   pick this as a shift feature as a response to that filter notice this is the non-maxima suppression
[00:23:56.200 --> 00:24:01.880]   so if this one is the maxima of all its its large and all its neighbors and all the neighbors above
[00:24:01.880 --> 00:24:08.600]   and below then that's a point that we extract okay and then once we extract that then we look for
[00:24:08.600 --> 00:24:15.560]   gradients why gradients because not directly look at the patch but actually at gradients because
[00:24:15.560 --> 00:24:20.280]   gradients are much more invariant if the image if we look at the same image but with slightly
[00:24:20.280 --> 00:24:24.520]   different exposure it will be a little bit brighter or a little bit darker so we don't care about the
[00:24:24.520 --> 00:24:32.840]   exact intensity that we see we care about the patterns of so what this thing is going to look at is
[00:24:32.840 --> 00:24:38.360]   what are the patterns of edge arrangements that we have and we just care about having edges we don't
[00:24:38.360 --> 00:24:45.480]   care how bright or how strong the edges are we look at the edge statistics like first we find
[00:24:45.480 --> 00:24:51.800]   somehow a dominant orientation so for example here if I look around here the strongest edge is here
[00:24:51.800 --> 00:24:56.600]   so that leads us to this orientation here it actually has two maxima it will end up using both
[00:24:56.600 --> 00:25:04.600]   or they slightly displaced actually etc but so you find a dominant orientation the strongest edge
[00:25:05.160 --> 00:25:10.280]   orientation and then you reorient the patch the reference patch based on that orientation
[00:25:10.280 --> 00:25:16.760]   and then once you did that so it's located based on the location of the blob and it's oriented
[00:25:16.760 --> 00:25:20.920]   based on the dominant orientation of the edges where are the what is the orientation of the
[00:25:20.920 --> 00:25:29.160]   strongest set of edges and then once you did that then you divide this patch here that is scaled
[00:25:29.160 --> 00:25:36.680]   based on the feature size on the the response size you are in it and then you divide it in these 16
[00:25:36.680 --> 00:25:46.040]   in the 16 regions and in every region you make a statistic of you know how many edges are in each
[00:25:46.040 --> 00:25:50.040]   of those orientations so how many edges are you into this way and how many are this way and how
[00:25:50.040 --> 00:25:58.280]   many are this way etc and you just make that statistics and essentially that 16 times 8 orientation
[00:25:59.240 --> 00:26:07.080]   and that gives you 128 values and that's now a vector 128 long vector that describes
[00:26:07.080 --> 00:26:14.120]   what happens in this patch here okay visually like a visual pattern but in a way that if you change
[00:26:14.120 --> 00:26:18.360]   a bit the brightness if you do things like that it would not be affected you would roughly keep the
[00:26:18.360 --> 00:26:23.720]   same statistics okay so under slightly variations of brightness and so on you would still end up
[00:26:23.720 --> 00:26:29.480]   with the same vector here or roughly the same vector and so in the end to find points in one
[00:26:29.480 --> 00:26:34.280]   image and then in another image to put them in correspondence you would actually correlate
[00:26:34.280 --> 00:26:39.800]   those vectors and see how well they correspond to each other the actual way it typically works is
[00:26:39.800 --> 00:26:45.560]   if you get a you pick the strongest response and that's then your probable match between two images
[00:26:45.560 --> 00:26:51.240]   and then you actually look at okay but wait a moment maybe there's confusion here
[00:26:51.240 --> 00:26:57.000]   let me check how strong the second strongest matches if the second strongest matches you know
[00:26:57.000 --> 00:27:03.640]   within point eight or so or point seven of the correlation of the best match then you say okay
[00:27:03.640 --> 00:27:11.720]   this is too close to call I'm not declaring this a match but if it is actually if it has you know
[00:27:11.720 --> 00:27:15.880]   enough of a gap to the second best match then you declare the match okay so that's actually the whole
[00:27:15.880 --> 00:27:22.360]   algorithm pretty much in a nutshell for matching between images and that can be the starting point
[00:27:22.360 --> 00:27:29.480]   you know from a collection of images to have all of them kind of aligned to each other geometrically
[00:27:29.480 --> 00:27:41.160]   okay okay so that's that was kind of a quick overview of what you saw last week any questions yes
[00:27:41.160 --> 00:27:56.840]   yeah yeah exactly this circle is that circle yes okay and so it means that on on on this one you
[00:27:56.840 --> 00:28:01.320]   have a small rectangle a small grid on which we evaluate the gradients and on this one you'll have
[00:28:01.320 --> 00:28:06.840]   a big grid it also means that if you have two different images and one is taken from a lot
[00:28:06.840 --> 00:28:13.560]   closer than the other you would hopefully have this still find a consistent peak and so what in
[00:28:13.560 --> 00:28:18.840]   one image looks this small in the other image could look this big but because you renormalize it
[00:28:18.840 --> 00:28:25.000]   it should actually you know you should actually find the same actual region in the world ideally that
[00:28:25.000 --> 00:28:29.320]   corresponds to the same things and therefore has the same patterns and therefore you can get it in
[00:28:29.320 --> 00:28:37.720]   in correspondence okay any other questions for the previous week yes
[00:28:37.720 --> 00:28:53.960]   that's right so well if you flip it like this yes you would not normally be able to match it
[00:28:56.120 --> 00:29:00.760]   if if you just you know if you just take a picture upside down then it would actually work
[00:29:00.760 --> 00:29:05.240]   because that's just a rotation and this is invariant to rotations because instead of finding the
[00:29:05.240 --> 00:29:09.560]   edge the strongest edge in this orientation if you turn it upside down this would not point in the
[00:29:09.560 --> 00:29:14.440]   other direction and after you know and you would actually extract consistently things so if you
[00:29:14.440 --> 00:29:20.280]   just rotate it's fine if you flip it's not fine like you wouldn't know about it now we actually
[00:29:20.280 --> 00:29:29.560]   did a paper in 2012 where we explicitly looked for symmetries we actually did that for you know I
[00:29:29.560 --> 00:29:42.280]   could probably I could probably find it let me quickly show you something if I can okay
[00:29:44.360 --> 00:29:51.800]   okay so we let me actually go to another example here that you'll recognize so here you go so
[00:29:51.800 --> 00:30:00.840]   so many of you know this building right so this is ccab and you can see that this is the
[00:30:00.840 --> 00:30:06.760]   left side of the building this is the right side of the building and we exactly played on this
[00:30:06.760 --> 00:30:12.440]   so we wanted to find symmetries so normally would not automatically match those in this case we
[00:30:12.440 --> 00:30:21.160]   explicitly looked for symmetric regions because we wanted to determine the architectural symmetries
[00:30:21.160 --> 00:30:27.160]   and use those to connect or reconstruction and say that this was an exact mirroring of this
[00:30:27.160 --> 00:30:32.280]   and therefore you know geometrically they should also be mirror images of each other and so on and
[00:30:32.280 --> 00:30:38.920]   so on and to discover that what we did was we took those sift vectors these 128 vectors and
[00:30:38.920 --> 00:30:47.960]   essentially it boils down to simply you know reordering those 128 numbers because if you
[00:30:47.960 --> 00:30:57.560]   you know if you look at my cursor here and actually let me just show a few more images first
[00:30:59.320 --> 00:31:02.840]   so this was about
[00:31:02.840 --> 00:31:14.360]   okay lots of stuff so here so we detected symmetries here symmetry planes and so on and so on
[00:31:14.360 --> 00:31:18.520]   and therefore could do a better reconstruction here because we impose that symmetry in the
[00:31:18.520 --> 00:31:26.680]   reconstruction okay so a few more examples here etc skip that let me get back to the slides now
[00:31:26.840 --> 00:31:39.640]   okay mirroring this and then changing the order of this these are just statistics of which edges
[00:31:39.640 --> 00:31:45.080]   on which orientation it turns out you can just take those 128 vectors and just shuffle them around
[00:31:45.080 --> 00:31:49.560]   in the right way and you get the mirrored one so you can easily correlate with the mirrored one
[00:31:49.560 --> 00:31:55.000]   without having to do much you don't have to recompute it you just shuffle the numbers around
[00:31:55.000 --> 00:32:00.200]   and correlate with the shuffled numbers and you can find the symmetric regions which has
[00:32:00.200 --> 00:32:04.520]   which is interesting in itself to be able to find symmetries also within a facade
[00:32:04.520 --> 00:32:08.600]   facades are very symmetric also something we looked at in the past and so on so they are
[00:32:08.600 --> 00:32:15.000]   interesting things you can do in that space for for symmetries okay any other questions related to last week
[00:32:15.000 --> 00:32:22.440]   okay so let's start with this week then almost at the end of the first hour so
[00:32:24.200 --> 00:32:35.000]   so we'll talk about aliasing in particular so aliasing is really about the fact that you can't
[00:32:35.000 --> 00:32:42.520]   you can't shrink an image by just you know sub sampling if you do that you will have strange
[00:32:42.520 --> 00:32:47.800]   type of effects appearing so we'll see in the next few slides or actually in some of those pictures
[00:32:49.720 --> 00:32:54.520]   what it does is you will have small phenomena like very detailed things to suddenly show up at a
[00:32:54.520 --> 00:33:03.000]   completely different scale as like big patterns now and if you look at video it means fast phenomena
[00:33:03.000 --> 00:33:07.960]   can now show up as like slow phenomena for example so it's essentially things that one
[00:33:07.960 --> 00:33:13.800]   frequency that show up at another frequency this is kind of the wagon wheels in movies that seem to
[00:33:14.680 --> 00:33:21.880]   roll backwards for example or checkerboards that look funny in ray tracing or striped shirts that
[00:33:21.880 --> 00:33:26.120]   look funny on television and so on so let me show you an example I used to have an example
[00:33:26.120 --> 00:33:31.160]   I had to change the example wait where do we go here
[00:33:31.160 --> 00:33:40.280]   okay I used to have an example where in windows I could just open an image with high frequency
[00:33:41.160 --> 00:33:45.720]   and then I could
[00:33:45.720 --> 00:33:59.240]   okay and I could actually rescale it and you would see the aliasing happen on the fly
[00:33:59.240 --> 00:34:04.920]   in the meanwhile Microsoft has fixed that you know their viewer their basic viewer even the
[00:34:04.920 --> 00:34:09.880]   basic viewer that's built in actually properly handles aliasing now so I cannot show you that
[00:34:09.880 --> 00:34:18.040]   live anymore but here's an example of aliasing you know of a webpage that kind of illustrates that
[00:34:18.040 --> 00:34:22.920]   so this building if you don't pay attention and you sub sample the image by factor of two for example
[00:34:22.920 --> 00:34:29.400]   it might look like this obviously this pattern is probably not on the real building right this is
[00:34:29.400 --> 00:34:36.600]   this is essentially how the image properly treated would look like and so what's the issue well the
[00:34:36.600 --> 00:34:40.760]   issue is essentially that you have this very fine pattern here and as you as you would just
[00:34:40.760 --> 00:34:48.280]   rescale the image by sub sampling you might end up hitting you know multiple times the brick
[00:34:48.280 --> 00:34:54.920]   and miss the cement or vice versa and so those are the things you see here right you see suddenly
[00:34:54.920 --> 00:34:58.680]   you're hitting a lot of cement and then in this area you're hitting a lot of bricks and so you
[00:34:58.680 --> 00:35:03.400]   get these kind of strange effects showing up okay and they show up at a completely different scale
[00:35:03.400 --> 00:35:09.320]   than where they were in the original image okay so you get high frequency things that once you
[00:35:09.320 --> 00:35:15.160]   somehow mess up and don't sample densely enough those high frequency patterns show up now as very
[00:35:15.160 --> 00:35:20.760]   slow frequency patterns right and so you get these kind of effects okay so in other words it's going
[00:35:20.760 --> 00:35:28.840]   to be important to you know to figure out how to handle this and actually at two stages one is
[00:35:29.640 --> 00:35:34.760]   before we digitize before we sample for the first time so going from the continuous image
[00:35:34.760 --> 00:35:42.200]   continuous real-world scene and then discretize it at that point we have to be very careful
[00:35:42.200 --> 00:35:48.280]   to avoid these things because if you then you're if you do it wrong then you're then it's done it's
[00:35:48.280 --> 00:35:54.760]   finished you can't repair that anymore or the second thing is if we are re-sampling we go from
[00:35:54.760 --> 00:35:58.920]   for example we have a high resolution image and now we re-sample it to a low resolution image
[00:35:58.920 --> 00:36:04.360]   for example to show it on the screen there also we always have to be careful and always do the
[00:36:04.360 --> 00:36:14.040]   appropriate steps to make sure that we don't get into aliasing issues um let me also show this video
[00:36:14.040 --> 00:36:23.400]   here so this is then temporal aliasing yeah sorry for the noise there
[00:36:23.400 --> 00:36:33.640]   okay so what do you see here so you see two things two motions so you see the kind of the
[00:36:33.640 --> 00:36:39.240]   wheel motion which this reflector is kind of showing you and then if you focus on the central
[00:36:39.240 --> 00:36:46.440]   area here for example for now it looks like it's going in the reverse direction right so obviously
[00:36:46.440 --> 00:36:52.840]   you know this is a rigid wheel it all turns in the same direction but as the wheel is decelerating
[00:36:52.840 --> 00:36:57.000]   you kind of see things kind of switch from turning one way to turning the other way to
[00:36:57.000 --> 00:37:03.000]   again turning the same way so clearly something something funny is going on here right
[00:37:03.000 --> 00:37:14.120]   okay
[00:37:14.120 --> 00:37:29.640]   yeah i got this kind of wondering what's going on there
[00:37:31.320 --> 00:37:34.440]   okay
[00:37:34.440 --> 00:37:41.400]   okay so essentially what do we have we have something like this going on here
[00:37:41.400 --> 00:37:47.240]   initially we're sampling and and you know we've seen is a little bit the first lecture right so
[00:37:47.240 --> 00:37:54.280]   initially we're sampling up you know more than fast enough to actually capture consistently the
[00:37:54.280 --> 00:37:59.800]   pattern that's actually there right the visual pattern in the image on the right we kind of just
[00:37:59.800 --> 00:38:03.720]   at the edge we're just still sample enough so we'll have a black pixel a white pixel a black
[00:38:03.720 --> 00:38:08.600]   pixel a white pixel we'll actually still have the main pattern that's visible there in the image
[00:38:08.600 --> 00:38:17.400]   we'll actually still capture it with our sampling but then once we get here or here we start seeing
[00:38:17.400 --> 00:38:23.400]   issues right so here we completely miss this white pixel so we have a big black region it looks like a
[00:38:23.400 --> 00:38:28.920]   big big black region then we again have a white pixel so but so we we essentially have the pattern
[00:38:28.920 --> 00:38:33.640]   show up at a different frequency than the one that's actually in the image and of course here
[00:38:33.640 --> 00:38:38.680]   in the end we wouldn't here we would just end up consistently you know sampling in the black region
[00:38:38.680 --> 00:38:46.040]   and so we'd have a fully black image instead of a you know ideally actually a question what should
[00:38:46.040 --> 00:38:53.000]   we have ideally as an image you know if we sample at this resolution what would we like to have
[00:38:53.000 --> 00:39:07.000]   so if if you represent that pattern right I mean this pattern here at this resolution what would be
[00:39:07.000 --> 00:39:09.560]   what would be the correct thing to do
[00:39:10.360 --> 00:39:19.720]   okay so what do you mean like
[00:39:19.720 --> 00:39:29.720]   but so you would have one black one white or well but then you would actually have a checkerboard but
[00:39:29.720 --> 00:39:35.080]   at you know half the resolution right so you have a checkerboard at a different frequency
[00:39:36.120 --> 00:39:40.280]   it would be twice as you know like it would look like it's twice as big a checkerboard than the one
[00:39:40.280 --> 00:39:45.560]   you actually started from right so that's probably not the best option
[00:39:45.560 --> 00:39:54.440]   well that's what we have but that's what clearly we know is wrong right
[00:39:54.440 --> 00:40:03.560]   exactly so the correct thing would be actually saying well if we can't represent the individual
[00:40:03.560 --> 00:40:09.000]   patterns anymore the correct thing to do is actually to you know you would look at this if you look
[00:40:09.000 --> 00:40:15.000]   so if you look at the you know black and white very fast alternating black and white from very far
[00:40:15.000 --> 00:40:19.640]   away what are you seeing you're not seeing either black or white image you're actually seeing something
[00:40:19.640 --> 00:40:25.720]   grayish right that's what you will see that would actually be the correct representation at that
[00:40:25.720 --> 00:40:29.320]   point it's the best you can do you know it's disappointing you cannot actually show the
[00:40:29.320 --> 00:40:34.760]   checkerboard anymore but hey it's beyond what you can still sample so at that point you don't want
[00:40:34.760 --> 00:40:40.440]   to hallucinate patterns that are not there you just want to say okay on average this is essentially
[00:40:40.440 --> 00:40:45.480]   grayscale okay so that's that's actually what we will want to have and will be the correct thing
[00:40:45.480 --> 00:40:51.160]   you can still do essentially those high frequencies you know on average is a gray image and the high
[00:40:51.160 --> 00:40:55.880]   frequencies are just beyond what we can represent so if we can't represent them anymore we just want
[00:40:55.880 --> 00:41:01.720]   to abstain as opposed to hallucinate something different okay and invent patterns that are not
[00:41:01.720 --> 00:41:10.280]   there at all that that's what we want to avoid okay so here's an example you see here a pattern
[00:41:10.280 --> 00:41:19.320]   smoothly varying pattern that essentially goes from a you know slow in the upper left corner
[00:41:19.880 --> 00:41:26.200]   to fast varying in the lower right corner and then we are slowly sub sampling it we're strictly
[00:41:26.200 --> 00:41:33.080]   sub sampling here okay so we just these are you know I don't know maybe 256 by 256 or so and then
[00:41:33.080 --> 00:41:39.960]   that's 128 and so on so we just sub sample every time it all goes well for a while you know qualitatively
[00:41:39.960 --> 00:41:44.360]   this picture and this picture and this picture and this picture they're all the same they just have
[00:41:44.360 --> 00:41:49.480]   less pixels but they still qualitatively show the same patterns but then something funny happens
[00:41:49.480 --> 00:41:55.880]   right when we get here suddenly we are we it's a different picture right it looks different
[00:41:55.880 --> 00:42:02.280]   it has different patterns that's the zooming of that picture right so we essentially have
[00:42:02.280 --> 00:42:06.760]   hallucinated or we have now new patterns that show up okay and so that's the thing we want to avoid
[00:42:06.760 --> 00:42:09.720]   like with the checkerboard we don't want to invent a checkerboard at a different resolution
[00:42:09.720 --> 00:42:15.240]   we actually want to say well you know we can't represent it anymore so you know we just have a
[00:42:15.880 --> 00:42:19.480]   gray thing because that's the only thing that still is kind of that's the average
[00:42:19.480 --> 00:42:27.240]   that's something we can still represent correctly okay
[00:42:27.240 --> 00:42:35.320]   so in a sense the open questions are still you know what causes the tendency of differentiation
[00:42:35.320 --> 00:42:40.920]   to emphasize noise there's something we didn't explicitly discuss but when you actually in
[00:42:40.920 --> 00:42:46.840]   filtering and so on if you if you look for sharpening filters and so on what they tend to do is
[00:42:46.840 --> 00:42:51.800]   actually to enhance the noise to create more noise make the image look more noisy
[00:42:51.800 --> 00:43:00.200]   we're still also discussing what's exactly the difference between discrete and continuous images
[00:43:00.200 --> 00:43:07.640]   as we go from one to the other what happens the key question on the previous slides is okay what is
[00:43:07.640 --> 00:43:16.360]   aliasing and how do we avoid it and in general we'll actually address that with the Fourier transform
[00:43:16.360 --> 00:43:24.520]   which is a language you know which is changing the representation of the image from a localized
[00:43:24.520 --> 00:43:28.600]   pixel per pixel description of this point looks like this and the next point looks like that and so
[00:43:28.600 --> 00:43:36.360]   on to more something in terms of broad patterns describing the image in terms of broad patterns
[00:43:36.920 --> 00:43:40.840]   that are global over the whole image so we're saying well on average the image is
[00:43:40.840 --> 00:43:46.520]   you know gray for example but then we have the left is brighter than the right
[00:43:46.520 --> 00:43:52.040]   and then we have this kind of wave pattern you know at this particular frequency going through
[00:43:52.040 --> 00:43:59.080]   and then also the top is darker than the bottom for example etc etc so we and so we can gradually
[00:43:59.080 --> 00:44:04.360]   describe in all these kind of global patterns over the image and as we'll see also very important
[00:44:04.360 --> 00:44:11.480]   of how exactly those patterns are shifted with respect to each other so that's a Fourier transform
[00:44:11.480 --> 00:44:17.320]   after the break we'll formally introduce when the Fourier transform is okay okay so
[00:44:17.320 --> 00:44:27.480]   so this is Fourier transform so what we do is in any sense think of
[00:44:29.400 --> 00:44:33.720]   so we will define the Fourier we define the Fourier transform actually both in a
[00:44:33.720 --> 00:44:42.120]   in for continuous image as well as for discrete image it's the easiest to think about a discrete
[00:44:42.120 --> 00:44:52.600]   case in that case think of taking the image as you know it's a matrix of numbers and so in that
[00:44:52.600 --> 00:44:58.280]   case you can always take that matrix of numbers and just all you know as they actually you know
[00:44:58.280 --> 00:45:04.440]   organized in memory anyways as a long list of numbers so you can also just decide that this
[00:45:04.440 --> 00:45:10.920]   matrix of numbers is actually a vector of numbers very long vector of numbers right but it's still a
[00:45:10.920 --> 00:45:19.240]   vector and as with any vector space as you've seen in linear algebra you can actually do a basis
[00:45:19.240 --> 00:45:26.200]   transform okay so in many ways it's nothing else than a basis transform now it's not a three by three
[00:45:26.200 --> 00:45:30.760]   you know it's not a three vector that you apply a three by three matrix to that kind of corresponds
[00:45:30.760 --> 00:45:35.960]   to a rotation for example rotation is nice because it preserves the length of vectors we'll actually
[00:45:35.960 --> 00:45:41.880]   see we'll see much more of that next week that these also in a sense preserve lengths of vectors
[00:45:41.880 --> 00:45:47.480]   so these are kind of good transforms in a sense and again we'll define that next time
[00:45:47.480 --> 00:45:52.200]   there's actually you here because those are unitary transforms so kind of like rotations
[00:45:55.320 --> 00:46:03.640]   but okay so essentially very long vector we apply transform so this is a basis transform so that's
[00:46:03.640 --> 00:46:10.120]   essentially a square matrix now if this was a thousand by thousand image that's a million
[00:46:10.120 --> 00:46:16.360]   dimensional vector, million dimensional vector space or basis transform is now a million by a
[00:46:16.360 --> 00:46:25.320]   million numbers that's this matrix and then you know so that's 12 zeros at one with 12 zeros
[00:46:25.320 --> 00:46:31.640]   lots of numbers here but essentially here again we then obtain something of the same length
[00:46:31.640 --> 00:46:37.560]   both a vector of with as many components and actually the vector also has the same length
[00:46:37.560 --> 00:46:45.320]   because this is unity unitary transform anyway so so in that case you can kind of think of you have
[00:46:45.320 --> 00:46:50.120]   numbers you multiply with the matrix you get other numbers and you know they're somehow equivalent
[00:46:50.120 --> 00:46:57.960]   but they're in a different space okay different bases so so that's what we'll do we'll actually
[00:46:57.960 --> 00:47:04.200]   look at doing the same in the continuous domain okay so we can also there do a basis transform
[00:47:04.200 --> 00:47:12.920]   okay it's a little bit maybe more abstract but it's the same thing anything here the matrix is
[00:47:12.920 --> 00:47:17.880]   essentially you can write it as a sum of you know components that you multiply with other you know
[00:47:17.880 --> 00:47:22.600]   components of the basis vectors that get multiplied with components of the vector and this and that
[00:47:22.600 --> 00:47:30.120]   okay if you go to the continuous space you go from you go from components
[00:47:30.120 --> 00:47:37.240]   from sum of components you go to integrals it's the natural you know continuous representations
[00:47:37.240 --> 00:47:43.320]   of what would here end up being sums in in this operation okay so in the end what what you end
[00:47:43.320 --> 00:47:52.200]   up doing is you have your function here function j of x y so you have a function that's defined in
[00:47:52.200 --> 00:48:00.680]   x y coordinates that's your image and now what we'll do is we'll multiply that with something that
[00:48:00.680 --> 00:48:08.200]   depends on x y but also depends on you know two other numbers u and v in this case okay
[00:48:08.200 --> 00:48:15.320]   and then what we'll do is we'll integrate this over the whole image that's the same as you know
[00:48:15.320 --> 00:48:20.600]   summing over all the vectors of our input space so we'll integrate over the whole and this is
[00:48:20.600 --> 00:48:25.240]   two-dimensional so it's even a bit more complicated we'll integrate over both x and y direction over
[00:48:25.240 --> 00:48:33.800]   the whole image here so this is integral over the whole image we integrate that and so we multiply
[00:48:33.800 --> 00:48:40.120]   with these basis vectors so this happens to be our basis vectors or sorry the vectors are here
[00:48:40.120 --> 00:48:43.800]   here we talk about functions so these are basis functions these are functions that
[00:48:43.800 --> 00:48:51.480]   depend in every point of our x y space okay and they have we want to transform we need this the
[00:48:51.480 --> 00:48:57.720]   equivalent we need this to be square so if we started from two dimensions of a full two-dimensional
[00:48:57.720 --> 00:49:03.000]   space in a sense we need to end up also with a two-dimensional space otherwise obviously there
[00:49:03.000 --> 00:49:07.880]   is a mismatch we cannot map a two-dimensional space to one-dimensional space and accept that we can
[00:49:07.880 --> 00:49:12.680]   you know that we preserve the information so we need to map it to a full two-dimensional space
[00:49:12.680 --> 00:49:22.920]   the two-dimensional space is the uv space here okay and so so essentially we will at every point
[00:49:22.920 --> 00:49:29.480]   here we'll multiply it with the basis function essentially and we'll we'll multiply it with
[00:49:29.480 --> 00:49:34.440]   how that basis function the value of that basis function at that particular x y location
[00:49:34.440 --> 00:49:39.240]   and we do that for all of the x y locations over the whole two-dimensional space
[00:49:40.200 --> 00:49:47.000]   and do a big integral and end up with one number right and because we integrate over the whole
[00:49:47.000 --> 00:49:55.320]   x y space essentially after this integral x and y have disappeared right because we integrated
[00:49:55.320 --> 00:49:59.800]   over all the x y so we summed over all the x y's essentially or integrated in the continuous case
[00:49:59.800 --> 00:50:04.920]   so we're not dependent on x y anymore right if you integrate over x y then you're not dependent
[00:50:04.920 --> 00:50:11.160]   anymore so the only variables left will be u and v right because those are still here right so they're
[00:50:11.160 --> 00:50:16.680]   not integrated away because you know we don't integrate over them we integrate over x y so u
[00:50:16.680 --> 00:50:23.880]   and v will remain as will not be the variables okay so we get a function in u and v okay so we
[00:50:23.880 --> 00:50:29.080]   start from a function in x y we do this integral here of all the basis functions for the free
[00:50:29.080 --> 00:50:34.840]   transfer and we end up with a different function that's not dependent on u and v that's our transform
[00:50:34.840 --> 00:50:39.880]   so we went from a function in the x y space defined over the x y space we end up with a
[00:50:39.880 --> 00:50:46.840]   function that is defined over a different space the u v space okay now what are those functions
[00:50:46.840 --> 00:50:52.760]   here they look a bit funny uh notice this is essentially square root of minus one so this is
[00:50:52.760 --> 00:50:58.040]   actually a complex basis there's reasons that it's complex in particular at the intuitive level
[00:50:58.920 --> 00:51:03.880]   you can see it as this function you should know is actually equivalent to this function so it's a
[00:51:03.880 --> 00:51:12.120]   cosine at a particular frequency defined by u and v here the frequency is can be different in the x
[00:51:12.120 --> 00:51:18.840]   and y domain so there's a different multiplier to how quickly x changes to how quickly y changes
[00:51:18.840 --> 00:51:25.160]   right the u and the v the ratio of u and v will tell you how how quickly one change corresponds to
[00:51:25.160 --> 00:51:30.360]   in one dimension corresponds to a change in the other direction on the one hand you have a cosine
[00:51:30.360 --> 00:51:40.040]   here as a real component of this function and you have a sign that corresponds to the imaginary
[00:51:40.040 --> 00:51:46.680]   component of this function okay why does why is this somehow useful or why do we have a complex
[00:51:46.680 --> 00:51:53.640]   function here well it's because with a single complex number in a sense we so we can capture
[00:51:53.640 --> 00:52:00.600]   both the cosine and the sine now as you probably as you should all remember the cosine and the sine
[00:52:00.600 --> 00:52:09.400]   function actually the same functions except they're shifted by 90 degrees okay and interestingly enough
[00:52:09.400 --> 00:52:15.640]   you can take so you can change a cosine to a sine by shifting it but more importantly
[00:52:15.640 --> 00:52:22.440]   if you have a cosine and a sine you can actually create any shifted and you know scaled up and down
[00:52:22.440 --> 00:52:29.320]   sine or cosine by just doing a linear combination of a sine and a cosine so with linear combinations
[00:52:29.320 --> 00:52:35.480]   of sines and cosines at the you know the canonical locations of the canonical cosine and the canonical
[00:52:35.480 --> 00:52:41.960]   sine you can actually generate any shifted version also it's actually just a sum of a cosine and a
[00:52:41.960 --> 00:52:53.000]   sine so by being it by using this basis function and being able to you know multiply this with an
[00:52:53.000 --> 00:53:02.200]   imaginary number with a sorry with a complex number combinations of sine of a real part and an
[00:53:02.200 --> 00:53:09.880]   imaginary part you can choose and of course by doing that you can bring so let's say well let's
[00:53:09.880 --> 00:53:17.000]   look at the real part if I do want you know the real part is one but then I add to it here I do times
[00:53:17.000 --> 00:53:26.440]   the imaginary part would be something with i then I get i times i that gives me minus one
[00:53:26.440 --> 00:53:32.040]   and so I get another real component to come in in it right so by multiplying with a complex number
[00:53:32.040 --> 00:53:37.880]   I can actually bring some of the sine into the real domain also okay and so I can end up with
[00:53:37.880 --> 00:53:43.160]   my real part of my function be a combination a linear combination of a cosine and a sine
[00:53:43.160 --> 00:53:47.880]   with whatever combination I want by choosing different real and imaginary numbers that I
[00:53:47.880 --> 00:53:52.600]   multiply this one okay so it turns out this function is actually a very generic function
[00:53:52.600 --> 00:54:00.520]   that allows me to represent any possible sine or cosine shifted you know with an arbitrary phase
[00:54:00.520 --> 00:54:06.280]   and scaled up with an arbitrary amplitude by just choosing different linear combinations
[00:54:07.240 --> 00:54:12.360]   with complex numbers right of this thing that's what a real part and there's also always going
[00:54:12.360 --> 00:54:23.400]   with it an imaginary part okay um okay so I think those are the the main points that I wanted to
[00:54:23.400 --> 00:54:29.240]   bring here so that's essentially here the formal definition of the Fourier transform okay now we'll
[00:54:29.240 --> 00:54:36.840]   look a little bit at you know a few more points of why this is important and then also what those
[00:54:36.840 --> 00:54:43.880]   two-dimensional Fourier bases actually look like okay first in one dimension and oh yes
[00:54:43.880 --> 00:55:13.860]   [ Inaudible ]
[00:55:13.860 --> 00:55:25.840]   Yeah, I mean, so essentially, yes, so the X and Y and U and V can also be complex numbers, yes.
[00:55:25.840 --> 00:55:31.840]   [ Inaudible ]
[00:55:31.840 --> 00:55:34.840]   Yes, yes, that's correct. Yes.
[00:55:34.840 --> 00:55:50.820]   Yes. Yes, so we start from real numbers here, but we end up with essentially a complex function, yes.
[00:55:50.820 --> 00:56:03.820]   And so what I was trying to explain is that the reason we do complex numbers is that that gives us this way to have the cosine and the sine and the combination of those kind of mixed up.
[00:56:03.820 --> 00:56:13.820]   Okay, so in a sense, or actually, you know, I don't know if there's any electrical engineers here, probably not.
[00:56:13.820 --> 00:56:29.820]   The reason that electrical engineers, you know, and actually studied electrical engineering long ago, the reason electrical engineers, you know, look at this very much in detail is because, you know, they look at systems,
[00:56:29.820 --> 00:56:34.820]   they try to model systems with inputs and outputs and stuff like that.
[00:56:34.820 --> 00:56:51.820]   And then within systems, all the possible systems, in general, you always, you know, the simplest way and actually, as long as you, you know, as long as you operate in a certain regime, you hope that your system behaves linearly.
[00:56:51.820 --> 00:57:04.820]   So that means that if you, you know, you do something at the input, whatever you do, you get kind of, like, if you move a little bit up, then you also expect your output to be a little bit bigger.
[00:57:04.820 --> 00:57:08.820]   And if you move a little bit down, it's a little bit down, and that's all, that should all be proportional.
[00:57:08.820 --> 00:57:10.820]   Proportional means linear, right?
[00:57:10.820 --> 00:57:17.820]   If I do a small change, I will maybe, you know, here for a parameter, maybe it will go up a little bit. The output will also go up a little bit.
[00:57:17.820 --> 00:57:25.820]   If I do a twice that small change, I would expect the output to also be changed by twice the amount I saw before with this, you know, like with a small change.
[00:57:25.820 --> 00:57:30.820]   So you expect those things to be proportional, at least within some regime.
[00:57:30.820 --> 00:57:36.820]   You know, if you do a huge change, then, you know, things could saturate, complicated things could start coming in and so on.
[00:57:36.820 --> 00:57:38.820]   There would be some non-linear effects.
[00:57:38.820 --> 00:57:49.820]   But as long as you operate in a reasonable regime, you expect that whatever you do at the input, you kind of see reflected at the output, like proportionally at the output.
[00:57:49.820 --> 00:58:03.820]   Now, if you have a dynamic system, where instead of just, you know, you change a setting and you wait for it to settle, you have something where you, you know, you do, you move, you keep moving something up and down and this and that and so on.
[00:58:03.820 --> 00:58:26.820]   The interesting thing is, these Fourier bases, which are sines and cosines, essentially, they turn out to be, for a linear system, linear dynamic system, these Fourier functions turn out to be, for linear systems, they turn out to be the eigenfunctions.
[00:58:26.820 --> 00:58:37.820]   If you remember eigenvectors, what is an eigenvector? You start from a vector, you multiply it with a matrix and you get the same vector out, except that it's longer or shorter.
[00:58:37.820 --> 00:58:40.820]   So it can be scaled differently, but it's the same vector.
[00:58:40.820 --> 00:58:50.820]   Same thing here, for functions, the same concept for functions mean I put a function at the input, I get the same function at the output.
[00:58:50.820 --> 00:58:58.820]   You know, again, think of this as a discretized vector, for example, you get a vector at the input, you get the same vector at the output, except it's bigger or smaller.
[00:58:58.820 --> 00:59:05.820]   And of course, in this case, it can also be shifted, right? It can be a bit delayed. So I oscillate something at the beginning.
[00:59:05.820 --> 00:59:13.820]   It will also oscillate at the output, but it could be a bit delayed or a bit accelerated. That delayed or accelerated is a shift.
[00:59:13.820 --> 00:59:23.820]   That's why we need a way to somehow blend cosines and sines, because that's also a way to represent a shift, a phase difference.
[00:59:23.820 --> 00:59:30.820]   That's why we need districts with complex numbers to have the cosine and the sine together with one complex coefficient.
[00:59:30.820 --> 00:59:33.820]   We can actually represent a shifted sine or cosine.
[00:59:33.820 --> 00:59:47.820]   Anyway, so in electrical engineers use this all the time, because it's essentially the simple version of systems, and then as soon as it's nonlinear, it gets all crazy and impossible to model and stuff like that.
[00:59:47.820 --> 00:59:56.820]   And you get chaotic behavior and literally chaotic for meaning that you get very strange behaviors and stuff like that, and often very hard to model.
[00:59:56.820 --> 01:00:04.820]   So in general, everything is you try to operate within a linear regime, and then everything will follow these things and etc.
[01:00:04.820 --> 01:00:09.820]   So your colleagues in ETH see a lot of Fourier transforms.
[01:00:09.820 --> 01:00:21.820]   So it means essentially that if I now put in a different frequency at the input, if it's a linear system, I will have that same frequency show up at the output, which is quite interesting, right?
[01:00:21.820 --> 01:00:27.820]   If it's a linear system, whatever frequency I applied input, I get that frequency at the output. I cannot get a different frequency.
[01:00:27.820 --> 01:00:32.820]   If you have a nonlinear system, I can also get multiples of that frequency typically.
[01:00:32.820 --> 01:00:49.820]   These are called harmonics. If you play music or so, you know that it's actually a nonlinear system. You play a note, you don't get exactly the same sine coming out, but you get also multiples of that sine, which actually correspond to having different wave shapes, like four different instruments.
[01:00:49.820 --> 01:00:55.820]   Anyways, enough of that. So now let's get back to our images.
[01:00:55.820 --> 01:00:58.820]   Here's one example of a basis function.
[01:00:58.820 --> 01:01:01.820]   So we'll assume it's defined over the whole space.
[01:01:01.820 --> 01:01:06.820]   Here we just have it limited to a particular region.
[01:01:06.820 --> 01:01:11.820]   We'll actually also see what that means and what the assumptions are outside of that region.
[01:01:11.820 --> 01:01:23.820]   But for now, let's assume that this extends out to infinity, you know, to the whole plane.
[01:01:23.820 --> 01:01:29.820]   So this is an example of one of those basis elements. Of course, I just showed a real part here.
[01:01:29.820 --> 01:01:34.820]   So it's essentially a kind of a wave pattern, sine cosine in intensity, right?
[01:01:34.820 --> 01:01:43.820]   So it goes from dark to bright to dark to bright, kind of alternates as a consistent wave pattern.
[01:01:43.820 --> 01:01:46.820]   And you see it has a certain orientation.
[01:01:46.820 --> 01:01:51.820]   So if we look now in the UV space, right, this is our other space.
[01:01:51.820 --> 01:01:55.820]   So this is the function in the XY space.
[01:01:55.820 --> 01:02:00.820]   And now if we, so this is actually a basis function, one of those basis functions.
[01:02:00.820 --> 01:02:05.820]   If we look in the UV space, we actually have just one dot.
[01:02:05.820 --> 01:02:09.820]   Actually, we have two dots because they actually, those two will actually do the same thing.
[01:02:09.820 --> 01:02:14.820]   And so this is where you actually get those two and they are somehow coupled together.
[01:02:14.820 --> 01:02:16.820]   But that doesn't really matter.
[01:02:16.820 --> 01:02:20.820]   But essentially you get that kind of paired dot.
[01:02:20.820 --> 01:02:22.820]   They're symmetric around the origin.
[01:02:22.820 --> 01:02:29.820]   So the frequency and the negative frequencies is kind of essentially morning down to the same thing.
[01:02:29.820 --> 01:02:39.820]   The, it's the combination of those, you know, is sines and cosines at that frequency.
[01:02:39.820 --> 01:02:41.820]   And so we get two things here that matter.
[01:02:41.820 --> 01:02:43.820]   It's the magnitude.
[01:02:43.820 --> 01:02:50.820]   And so actually, sorry, so the length of this here corresponds to the frequency.
[01:02:50.820 --> 01:02:54.820]   And the orientation of this, for example, in this case, oriented this way,
[01:02:54.820 --> 01:02:59.820]   you can see that that's actually exactly the orientation of the biggest change, right?
[01:02:59.820 --> 01:03:04.820]   The fastest change are in this orientation, which is the same as the direction of this vector here.
[01:03:04.820 --> 01:03:05.820]   Okay?
[01:03:05.820 --> 01:03:12.820]   And the length of this corresponds to, corresponds to the frequency.
[01:03:12.820 --> 01:03:18.820]   In particular, this is a small length compared to this size here.
[01:03:18.820 --> 01:03:20.820]   So this is a small vector.
[01:03:20.820 --> 01:03:22.820]   So that means we have a small frequency.
[01:03:22.820 --> 01:03:25.820]   Now a small frequency means a slow frequency.
[01:03:25.820 --> 01:03:26.820]   That's small.
[01:03:26.820 --> 01:03:27.820]   That means slowly change.
[01:03:27.820 --> 01:03:33.820]   So it actually means in the image is like big waves because they slowly change.
[01:03:33.820 --> 01:03:35.820]   Here's another example.
[01:03:35.820 --> 01:03:37.820]   Notice it's a different orientation.
[01:03:37.820 --> 01:03:38.820]   It's this orientation.
[01:03:38.820 --> 01:03:43.820]   Okay, so you can find back that orientation of fastest change here.
[01:03:43.820 --> 01:03:45.820]   And it's a longer vector.
[01:03:45.820 --> 01:03:47.820]   So what is a longer vector correspond to?
[01:03:47.820 --> 01:03:49.820]   It corresponds to a higher frequency.
[01:03:49.820 --> 01:03:52.820]   A higher frequency means faster change.
[01:03:52.820 --> 01:03:56.820]   So that means that essentially the pattern is smaller.
[01:03:56.820 --> 01:03:59.820]   The wave here is a smaller wave because it changes faster.
[01:03:59.820 --> 01:04:00.820]   Okay?
[01:04:00.820 --> 01:04:03.820]   So notice that kind of somewhat an intuitive thing.
[01:04:03.820 --> 01:04:08.820]   So a bigger frequency means actually smaller, you know, more quick changes,
[01:04:08.820 --> 01:04:12.820]   so smaller, smaller patterns showing up in the image.
[01:04:12.820 --> 01:04:18.820]   Here at the edge, even longer, so you see then how this kind of looks like.
[01:04:18.820 --> 01:04:20.820]   Again, this is the orientation.
[01:04:20.820 --> 01:04:23.820]   So that's the orientation of fastest change.
[01:04:23.820 --> 01:04:24.820]   Okay?
[01:04:24.820 --> 01:04:27.820]   Does that all make sense to everyone?
[01:04:27.820 --> 01:04:28.820]   Yep.
[01:04:28.820 --> 01:04:34.820]   Okay, so here's then an example of how to look at this as basis functions.
[01:04:34.820 --> 01:04:35.820]   Right?
[01:04:35.820 --> 01:04:42.820]   So essentially, and here in this case for just some discrete values.
[01:04:42.820 --> 01:04:45.820]   So we have this kind of cosine sign kind of combination.
[01:04:45.820 --> 01:04:48.820]   So the four zero, zero zero.
[01:04:48.820 --> 01:04:53.820]   So if we fill in zero here for u and for v, we get essentially, well,
[01:04:53.820 --> 01:04:58.820]   no dependency on, you know, like we get essentially something that's just constant.
[01:04:58.820 --> 01:04:59.820]   Right?
[01:04:59.820 --> 01:05:02.820]   In particular, if we fill in zero here, we just get a one.
[01:05:02.820 --> 01:05:05.820]   And if we're in zeros here, so here and here,
[01:05:05.820 --> 01:05:09.820]   meaning that the whole thing in between here is essentially zero.
[01:05:09.820 --> 01:05:10.820]   Sign of zero is zero.
[01:05:10.820 --> 01:05:12.820]   So we get a one here and a zero here.
[01:05:12.820 --> 01:05:13.820]   Okay?
[01:05:13.820 --> 01:05:17.820]   So we just get a constant one and no imaginary part.
[01:05:17.820 --> 01:05:19.820]   That's essentially this here.
[01:05:19.820 --> 01:05:20.820]   It's a fully bright image.
[01:05:20.820 --> 01:05:21.820]   No change.
[01:05:21.820 --> 01:05:22.820]   Constant.
[01:05:22.820 --> 01:05:23.820]   Okay?
[01:05:23.820 --> 01:05:32.820]   Then with one for u and zero for v.
[01:05:32.820 --> 01:05:33.820]   Right?
[01:05:33.820 --> 01:05:34.820]   So what do we get?
[01:05:34.820 --> 01:05:38.820]   We get essentially something that just depends on x at a particular frequency.
[01:05:38.820 --> 01:05:42.820]   So it will, in this case, just do one period.
[01:05:42.820 --> 01:05:45.820]   That's this guy here.
[01:05:45.820 --> 01:05:46.820]   Okay?
[01:05:46.820 --> 01:05:51.820]   The, because we normalize you to pi, so over, you know,
[01:05:51.820 --> 01:05:56.820]   essentially as we do, as we have x here from zero to one,
[01:05:56.820 --> 01:06:01.820]   we essentially just kind of go through one full period from bright to dark to bright.
[01:06:01.820 --> 01:06:02.820]   Okay?
[01:06:02.820 --> 01:06:07.820]   If we now multiply, you know, we have, here we have two and zero.
[01:06:07.820 --> 01:06:08.820]   So what do we get?
[01:06:08.820 --> 01:06:11.820]   We get two x plus zero y.
[01:06:11.820 --> 01:06:12.820]   Okay?
[01:06:12.820 --> 01:06:13.820]   So we're not dependent on y.
[01:06:13.820 --> 01:06:16.820]   So essentially you can see we're not dependent on y.
[01:06:16.820 --> 01:06:17.820]   Okay?
[01:06:17.820 --> 01:06:19.820]   In the y direction, nothing changes.
[01:06:19.820 --> 01:06:22.820]   It's always constant in the y direction, in the vertical direction.
[01:06:22.820 --> 01:06:27.820]   But in the horizontal direction, it changes, but now it changes not with x, but with two x.
[01:06:27.820 --> 01:06:30.820]   So by the time we reach one, we've done two periods.
[01:06:30.820 --> 01:06:31.820]   Okay?
[01:06:31.820 --> 01:06:34.820]   Because of the normalization with two pi again.
[01:06:34.820 --> 01:06:35.820]   Right?
[01:06:35.820 --> 01:06:36.820]   Okay?
[01:06:36.820 --> 01:06:37.820]   And so on.
[01:06:37.820 --> 01:06:38.820]   Right?
[01:06:38.820 --> 01:06:45.820]   And you see that then as we start having, for example, one one, now it will change kind of diagonally.
[01:06:45.820 --> 01:06:52.820]   And so along the diagonal, by the time we get here, we get to one and then we get to kind of two here.
[01:06:52.820 --> 01:06:54.820]   So I'm not sure it's perfectly.
[01:06:54.820 --> 01:07:04.820]   Yeah, so essentially we get one period here from zero to one and we get one period here from zero to one and then it's constant along this line.
[01:07:04.820 --> 01:07:07.820]   And then by the time we're here, we actually have moved two.
[01:07:08.820 --> 01:07:09.820]   And so on.
[01:07:09.820 --> 01:07:10.820]   Right?
[01:07:10.820 --> 01:07:17.820]   And so you see that depending on the different one and three, we get kind of three periods here and only one period there.
[01:07:17.820 --> 01:07:18.820]   Yeah.
[01:07:18.820 --> 01:07:21.820]   Could you explain again why there are two pi factors in there?
[01:07:21.820 --> 01:07:26.820]   Is that thinking that for the weight functions that we multiply there, it's like two pi, it should be both the same thing.
[01:07:26.820 --> 01:07:29.820]   Wait, wait.
[01:07:29.820 --> 01:07:35.820]   So two pi is there because if you not put one in there, you end up having exactly one period.
[01:07:35.820 --> 01:07:36.820]   Right?
[01:07:36.820 --> 01:07:38.820]   You're not exactly one weight.
[01:07:38.820 --> 01:07:40.820]   Well, you will.
[01:07:40.820 --> 01:07:45.820]   So you do from zero to one times two pi, you go from zero to two pi.
[01:07:45.820 --> 01:07:46.820]   Right?
[01:07:46.820 --> 01:07:47.820]   So.
[01:07:47.820 --> 01:07:48.820]   Okay.
[01:07:48.820 --> 01:07:51.820]   Any, any other questions?
[01:07:51.820 --> 01:07:53.820]   Oh, okay.
[01:07:53.820 --> 01:08:02.820]   Okay, so it turns out that the, the actual complex numbers are a little bit complicated to work with.
[01:08:02.820 --> 01:08:14.820]   So in practice, it makes a lot more sense to, to not look at, you know, the real and the imaginary part, but to look at the phase and magnitude.
[01:08:14.820 --> 01:08:15.820]   Okay.
[01:08:15.820 --> 01:08:30.820]   So meaning that, you know, we, we essentially look at when we get a number, when we get a complex number, what we look at is, okay, for that complex number.
[01:08:31.820 --> 01:08:37.820]   Is it like how, how long is the vector of the complex number?
[01:08:37.820 --> 01:08:46.820]   So what's the, the, the length of that vector, the, you know, the A plus I times B, how long is that vector?
[01:08:46.820 --> 01:08:51.820]   And, and then what is the orientation of that vector?
[01:08:51.820 --> 01:08:55.820]   Don't confuse it with the UV space, right?
[01:08:55.820 --> 01:09:08.820]   I'm now talking about in, in, I'm talking about essentially this, the, the, what the Fourier transform, the value that comes out of the Fourier transform.
[01:09:08.820 --> 01:09:23.820]   So for every frequency, for every UV value, the number we get, which is now a complex number, right, we started from a real image, but the transform of it is now a complex number for each value.
[01:09:23.820 --> 01:09:33.820]   For each of those values, we get essentially a real and imaginary component, and those are hard to kind of get, you know, much understanding from.
[01:09:33.820 --> 01:09:39.820]   What we care about is how strong is the signal there, how, how strong is the response for that UV value, for that frequency?
[01:09:39.820 --> 01:09:52.820]   So that pattern, how strongly is that, so essentially what we're talking about, remember, it's when you do apply a filter or this or that, what you're talking about is how strongly is that pattern present?
[01:09:52.820 --> 01:10:01.820]   When I say that pattern, I'm talking about these patterns. How strongly is there a constant pattern? How strongly is there this, this frequency at that orientation? How strongly is that present?
[01:10:01.820 --> 01:10:06.820]   The question, how strongly is, what is the magnitude of the response?
[01:10:06.820 --> 01:10:13.820]   I don't care if it's in the imaginary or the real part, that's less important, but I want to know how strong is the response, how strong is that signal?
[01:10:13.820 --> 01:10:21.820]   Because really the real versus imaginary part, we're really just talking of how, you know, that pattern, how is it shifted?
[01:10:21.820 --> 01:10:30.820]   Right? The real versus imaginary. Is it this pattern or is it this pattern shifted here or here? That's what we're talking about.
[01:10:30.820 --> 01:10:34.820]   That's the ratio of real versus imaginary, that's what we're talking about.
[01:10:34.820 --> 01:10:41.820]   So essentially, first is how strong is the pattern, is one, and then what is the phase? The phase means how is it shifted?
[01:10:41.820 --> 01:10:51.820]   Okay, it's the ratio between real and imaginary. So instead of having the real and imaginary part and care about those two numbers, we care about, well, how long is that vector on the one hand,
[01:10:51.820 --> 01:11:01.820]   and then what is the orientation of that vector, which actually means where is the dark part, where is the white part, how is it shifted?
[01:11:01.820 --> 01:11:07.820]   Okay, how much cosine versus sine is it? Okay? That's what this is about.
[01:11:07.820 --> 01:11:23.820]   What's interesting is that for real images, real world images, the rough pattern of the magnitude transform, right?
[01:11:23.820 --> 01:11:30.820]   So essentially, we take a single image and we now do the Fourier transform and we get a real and imaginary part.
[01:11:30.820 --> 01:11:34.820]   So we're not going to look at the real and imaginary image separately. We cannot look at both together.
[01:11:34.820 --> 01:11:39.820]   I mean, we could maybe put them in some colors or something complicated.
[01:11:39.820 --> 01:11:47.820]   But what we'll do is we'll generate two images. One is how strong is the response for a particular frequency for a particular pattern, and the second one is how is it shifted?
[01:11:47.820 --> 01:12:00.820]   Okay? What's interesting is if we look at that magnitude, like how strong is this frequency present, it turns out that most natural images have roughly the same magnitude transform.
[01:12:00.820 --> 01:12:09.820]   Okay? That magnitude part, so essentially, it looks almost the same for all images. What looks very different is the phase transform.
[01:12:09.820 --> 01:12:16.820]   That's like what are the particular shifts of these different frequencies we respect to each other.
[01:12:16.820 --> 01:12:23.820]   So we'll do an interesting experiment. We'll take here one natural image, an image of the natural world.
[01:12:23.820 --> 01:12:31.820]   This is the magnitude transform for the sheet picture. What do you see here? It's like it's a lot darker. It's a lot brighter here.
[01:12:31.820 --> 01:12:37.820]   So there's more low frequencies. So this is 0, 0 in the middle here.
[01:12:37.820 --> 01:12:47.820]   And then, you know, you have, it gets essentially as you go away from the center to higher frequencies, you get less and less response.
[01:12:47.820 --> 01:12:51.820]   This is the phase transform. Okay?
[01:12:51.820 --> 01:13:01.820]   And then we have another animal, which of course also, as you can notice, some very strong, you know, patterns present here.
[01:13:01.820 --> 01:13:06.820]   This is the magnitude transform, and this is the phase transform.
[01:13:06.820 --> 01:13:19.820]   So now what we'll do is we'll actually mix up, we'll switch out the, so we'll take the magnitude of one animal picture and the phase of the other and recombine them.
[01:13:19.820 --> 01:13:32.820]   Okay? And so here, if we take the zebra phase and the cheetah magnitude image and recombine them, so we take the strength of each pattern's presence
[01:13:32.820 --> 01:13:47.820]   from the cheetah, but we take the phase shift from the zebra, then what do we see here? We see the zebra, right?
[01:13:47.820 --> 01:13:56.820]   So what ends up being important, it seems for, you know, like the reconstruction in a sense, you don't see a cheetah here, right? It's just a zebra.
[01:13:56.820 --> 01:14:06.820]   So it turns out that it's really this phase, the phase turns out to really be the thing that determines and the magnitude is actually less important,
[01:14:06.820 --> 01:14:10.820]   and also it's kind of very similar for most natural images.
[01:14:10.820 --> 01:14:20.820]   So this is kind of interesting. The reverse experiment here, so here we have the, which one is it, the cheetah phase and the zebra magnitude,
[01:14:20.820 --> 01:14:27.820]   and we see that we see the cheetah, so the phase is the kind of the specific factor, the discriminatory.
[01:14:27.820 --> 01:14:34.820]   Now why is that? Well, it's because we've done this decomposition in waves of different frequencies, okay?
[01:14:34.820 --> 01:14:39.820]   So waves of this frequency and waves of, you know, higher frequency and so on.
[01:14:39.820 --> 01:14:52.820]   What we'll see also in this lecture, the coming lecture, what we'll see is that the way to get a hard edge, like, you know, a black and white edge like here,
[01:14:52.820 --> 01:14:59.820]   the only way to construct that from signs and cosines, signs and cosines are always soft, you know, transitions.
[01:14:59.820 --> 01:15:07.820]   The only way to get a really hard edge from a combination of signs and cosines is actually to put a lot, to exactly shift them at the right place,
[01:15:07.820 --> 01:15:12.820]   to have the maximum slope of all of your signs and cosines to align exactly in the right location.
[01:15:12.820 --> 01:15:22.820]   So that's essentially what you see here. To get hard edges, you actually need a very precise phase alignment of it to get a hard edge at a particular location.
[01:15:22.820 --> 01:15:27.820]   You get exactly all of your signs and cosines to align exactly correctly.
[01:15:27.820 --> 01:15:37.820]   Aligning is essentially a phase property, which cosine combination do I have so that I shifted exactly at the right place.
[01:15:37.820 --> 01:15:41.820]   So that's what you see here, okay?
[01:15:41.820 --> 01:15:43.820]   Here's another experiment.
[01:15:43.820 --> 01:15:50.820]   So here, another, you know, picture taken in the wild.
[01:15:50.820 --> 01:15:59.820]   You see, of course, a very strong, you know, pattern here, a particular set of frequencies that should be strongly present in the image.
[01:15:59.820 --> 01:16:11.820]   If you actually look at the Fourier transform, in this case, so this is the magnitude transform, you see that you actually have, you know, strong response, of course, at 0, 0.
[01:16:11.820 --> 01:16:22.820]   But then, also, beyond that, you actually see that there's a number of frequencies that show up strongly, and then, you know, multiples of that also.
[01:16:22.820 --> 01:16:24.820]   So these are higher frequencies.
[01:16:24.820 --> 01:16:36.820]   So those are, you know, so this frequency here, this essentially corresponds to this distance here, so to that wave.
[01:16:36.820 --> 01:16:41.820]   And then, of course, you also have multiples of that to be able to represent the more intricate patterns.
[01:16:41.820 --> 01:16:42.820]   Okay?
[01:16:42.820 --> 01:16:46.820]   So interesting if we mask that out, this is kind of what you would get.
[01:16:46.820 --> 01:16:52.820]   So if you mask that out, and then you do a reconstruction, so you do essentially the inverse transform.
[01:16:52.820 --> 01:16:55.820]   So you see we can actually do inverse transforms.
[01:16:55.820 --> 01:16:59.820]   If you do the inverse transform of that, then you get this image here.
[01:16:59.820 --> 01:17:05.820]   So you see, it's kind of, you still recognize the thing, but somehow that really strong, that's the strong edges.
[01:17:05.820 --> 01:17:07.820]   Those have been attenuated now.
[01:17:07.820 --> 01:17:09.820]   That's kind of what you see here.
[01:17:09.820 --> 01:17:14.820]   And notice also, you get some ghost effects here.
[01:17:14.820 --> 01:17:18.820]   That's because by taking those things out, remember, these are all global patterns.
[01:17:18.820 --> 01:17:25.820]   And so if you change them in one location, they will actually somehow tend to show up, then, you know, like have ghost effects in other locations.
[01:17:25.820 --> 01:17:26.820]   Okay?
[01:17:26.820 --> 01:17:36.820]   Now, we look at a few important Fourier transform pairs, or properties of the Fourier transform.
[01:17:36.820 --> 01:17:37.820]   Okay?
[01:17:37.820 --> 01:17:39.820]   So first, it's a linear transform.
[01:17:39.820 --> 01:17:46.820]   That means that if you have the same function, but like you multiply it by a factor of two, it's twice as big.
[01:17:46.820 --> 01:17:50.820]   At the output, you will have an output that's twice as big also.
[01:17:50.820 --> 01:17:58.820]   So if your function is twice as bright, then your Fourier transform will also be twice, the magnitude will be twice as big.
[01:17:58.820 --> 01:18:02.820]   The phase will not change, and the magnitude will be multiplied by a factor of two.
[01:18:02.820 --> 01:18:03.820]   Okay?
[01:18:03.820 --> 01:18:04.820]   Et cetera.
[01:18:04.820 --> 01:18:09.820]   If you do a linear combination of two images, you will get a linear combination of the two Fourier transforms.
[01:18:09.820 --> 01:18:10.820]   Okay?
[01:18:10.820 --> 01:18:13.820]   With the same combination.
[01:18:13.820 --> 01:18:16.820]   Very important, there is an inverse Fourier transform.
[01:18:16.820 --> 01:18:28.820]   This kind of rather trivial in the case to prove in the case of the discrete Fourier transform, obviously, because this is a basis transform.
[01:18:28.820 --> 01:18:30.820]   A basis transform is always invertible.
[01:18:30.820 --> 01:18:41.820]   So obviously, that inverse matrix should exist, or that matrix, that U matrix is invertible, has to be invertible, otherwise it wouldn't be called a basis transform.
[01:18:41.820 --> 01:18:46.820]   This can actually also be, there is an inverse Fourier transform also in the continuous domain.
[01:18:46.820 --> 01:18:48.820]   So if you work on the continuous representation.
[01:18:48.820 --> 01:18:54.820]   So remember here, we both look, we define it both on the continuous space as well as on the discrete space.
[01:18:54.820 --> 01:18:59.820]   That's the discrete representation, but the integrals was our representation.
[01:18:59.820 --> 01:19:10.820]   So we can do, what we've seen today is we can do the Fourier transform both on the continuous, in the continuous domain representation, as well as in the discrete domain representation.
[01:19:10.820 --> 01:19:17.820]   What's important is that if we scale down the function, we shrink things.
[01:19:17.820 --> 01:19:18.820]   Right?
[01:19:18.820 --> 01:19:22.820]   So we have now the scale of the patterns, we make them smaller.
[01:19:22.820 --> 01:19:24.820]   That actually means high frequencies.
[01:19:24.820 --> 01:19:31.820]   So if we scale down in the spatial domain, then we'll scale up in the frequency domain.
[01:19:31.820 --> 01:19:37.820]   So if we go to smaller details, so we take an image and we shrink it by a factor of two.
[01:19:37.820 --> 01:19:43.820]   Our Fourier transform will actually increase by a factor of two, because the frequencies now get bigger.
[01:19:43.820 --> 01:19:45.820]   So this is kind of not so intuitive.
[01:19:45.820 --> 01:19:48.820]   So it actually means high frequencies, small details.
[01:19:48.820 --> 01:19:53.820]   So shrinking spatial domains mean increasing our images in the frequency domain.
[01:19:53.820 --> 01:19:58.820]   So that's important to really know that it's that inverse behavior.
[01:19:58.820 --> 01:20:02.820]   Really, really interesting.
[01:20:02.820 --> 01:20:07.820]   You know, this is again where the Gaussian is just this amazing function.
[01:20:07.820 --> 01:20:12.820]   The Fourier transform of Gaussian is a Gaussian.
[01:20:12.820 --> 01:20:15.820]   And of course, this is still valid here.
[01:20:15.820 --> 01:20:25.820]   So it means that if we make a bigger blob in the spatial domain, we get a smaller blob in the frequency domain and vice versa.
[01:20:25.820 --> 01:20:31.820]   If you go to a very peak thing in the spatial domain, then we get a very spread out thing in the frequency domain.
[01:20:31.820 --> 01:20:34.820]   So we still have that, but it's a Gaussian and gives a Gaussian.
[01:20:34.820 --> 01:20:40.820]   The size of the Gaussian, as we increase in one domain, it will shrink in the other domain for its inverse.
[01:20:40.820 --> 01:20:44.820]   Or it's Fourier transform or inverse Fourier transform.
[01:20:44.820 --> 01:20:50.820]   Then there's also this interesting function, which is a box.
[01:20:50.820 --> 01:20:52.820]   What's special about a box?
[01:20:52.820 --> 01:20:56.820]   Well, a box is essentially a function that's perfectly spatially limited.
[01:20:56.820 --> 01:20:59.820]   It's constant, and then it's zero.
[01:20:59.820 --> 01:21:03.820]   So it's a perfectly spatially limited function.
[01:21:03.820 --> 01:21:09.820]   The inverse, the Fourier transform of that is this function here.
[01:21:09.820 --> 01:21:13.820]   It's a sinus function divided by x.
[01:21:13.820 --> 01:21:21.820]   So it's called a sinc function, sinc x or whatever, but it's essentially, it's a sinus divided by x.
[01:21:21.820 --> 01:21:25.820]   So it's, you know, the envelope of this function is the one over x.
[01:21:25.820 --> 01:21:32.820]   So it's a sinc function that gets gradually smaller and smaller, scaled by one over x, essentially.
[01:21:32.820 --> 01:21:42.820]   And then essentially in two dimensions it looks like this.
[01:21:42.820 --> 01:21:45.820]   So we have that effect in one dimension and in the other dimension.
[01:21:45.820 --> 01:21:53.820]   What's critical about this is if you compare this function that goes to zero
[01:21:53.820 --> 01:21:59.820]   and this function that also goes to zero, if you remember from analysis and so on,
[01:21:59.820 --> 01:22:08.820]   this one goes to zero with one over x, this one goes to zero with a double exponential.
[01:22:08.820 --> 01:22:10.820]   That's very different.
[01:22:10.820 --> 01:22:13.820]   This goes to zero very fast.
[01:22:13.820 --> 01:22:18.820]   A Gaussian has a finite extent and very quickly goes very fast towards zero.
[01:22:18.820 --> 01:22:22.820]   So if you know like this is kind of the probabilities, for example,
[01:22:22.820 --> 01:22:28.820]   and the probabilities that you are at three sigma or five sigma or ten sigma is almost perfectly zero.
[01:22:28.820 --> 01:22:35.820]   Something that goes with one over x, well, if you're ten out, you're still at one tenth, right?
[01:22:35.820 --> 01:22:37.820]   The function value, right?
[01:22:37.820 --> 01:22:43.820]   So as a probability function, you're quite likely to have things at one over, at factor ten out.
[01:22:43.820 --> 01:22:47.820]   Like I said, the equivalent sigma, you have your box and then you do the Fourier transform.
[01:22:47.820 --> 01:22:51.820]   At ten you're still likely at hundred, it's still one over hundred.
[01:22:51.820 --> 01:22:54.820]   That's still a reasonable number, that's one percent or so.
[01:22:54.820 --> 01:22:59.820]   So it's actually, it only degrades very slowly.
[01:22:59.820 --> 01:23:01.820]   So this will create issues.
[01:23:01.820 --> 01:23:03.820]   The fact that this has this infinite extent.
[01:23:03.820 --> 01:23:08.820]   So being perfectly bound in the spatial domain means you actually have quite an extent,
[01:23:08.820 --> 01:23:14.820]   your function is quite large in the frequency domain.
[01:23:14.820 --> 01:23:16.820]   Here's a few more.
[01:23:16.820 --> 01:23:19.820]   Okay, that one we've already seen in a sense.
[01:23:19.820 --> 01:23:21.820]   These are basis function.
[01:23:21.820 --> 01:23:28.820]   So a sine transforms to just these two peaks, these kind of coupled peaks, plus and minus the value,
[01:23:28.820 --> 01:23:32.820]   but essentially it's just, it's exactly those values and then a shifted sine and so on
[01:23:32.820 --> 01:23:37.820]   is different linear combinations of these, also these complex numbers.
[01:23:37.820 --> 01:23:48.820]   The vice versa, if you haven't, this is something we'll look more in detail at next, on Thursday.
[01:23:48.820 --> 01:23:55.820]   But if you have a perfect, exactly localized point in one, and we'll have to discuss that more in detail,
[01:23:55.820 --> 01:24:00.820]   what that means, because we're talking integrals and a single point would lead to zero integral
[01:24:00.820 --> 01:24:04.820]   and we don't want a zero integral, we want a finite integral and so on.
[01:24:04.820 --> 01:24:09.820]   We'll discuss all of that in detail on Thursday, but this is a special function called delta function,
[01:24:09.820 --> 01:24:11.820]   which is perfectly localized.
[01:24:11.820 --> 01:24:14.820]   It's zero everywhere except at one location.
[01:24:14.820 --> 01:24:19.820]   Somehow we'll see it integrates still to one, which is a bit magic, so we'll see how we get there.
[01:24:19.820 --> 01:24:25.820]   But essentially that perfectly localized spatial function in the frequency domain
[01:24:25.820 --> 01:24:32.820]   essentially needs all the frequencies to come together to create that perfectly localized thing
[01:24:32.820 --> 01:24:42.820]   in the spatial domain is perfectly spread out, fully spread out over the whole frequency domain.
[01:24:42.820 --> 01:24:46.820]   Also very interesting is this function, which, so if we assume we have this function,
[01:24:46.820 --> 01:24:55.820]   you can kind of make copies of this function, like just a regular copy of this function.
[01:24:55.820 --> 01:25:01.820]   Then, if they're spaced by a distance of t, then the Fourier transform of that will be the same function
[01:25:01.820 --> 01:25:05.820]   but spaced with one over t.
[01:25:05.820 --> 01:25:12.820]   We'll, as again, as we'll define the delta function Thursday, and then of course that thing is kind of defined
[01:25:12.820 --> 01:25:16.820]   as a copies of delta function, so we'll get to that.
[01:25:16.820 --> 01:25:19.820]   The reason I already showed this here is because it's really, really important in this course,
[01:25:19.820 --> 01:25:22.820]   because this will be our model for sampling.
[01:25:22.820 --> 01:25:29.820]   These are essentially the exactly measuring in one location or in all these regular grid locations.
[01:25:29.820 --> 01:25:32.820]   That's the Fourier transform of it.
[01:25:32.820 --> 01:25:36.820]   So this is how we'll model sampling, that will be the Fourier transform of our sampling function.
[01:25:36.820 --> 01:25:42.820]   And this is what, on Thursday, will really make us understand what happens when you do aliasing,
[01:25:42.820 --> 01:25:46.820]   what happens when we do sampling and then when does it go wrong.
[01:25:46.820 --> 01:25:50.820]   This will essentially be the key to explaining that.
[01:25:50.820 --> 01:25:56.820]   Then we define this box function, perfectly localized finite extent in space.
[01:25:56.820 --> 01:26:07.820]   This has this kind of very slowly extinguishing function, alternating positive and negative in the frequency domain.
[01:26:07.820 --> 01:26:13.820]   The Gaussian is this very nice function we set that has its own, you know, the Fourier transform of Gaussian is a Gaussian.
[01:26:13.820 --> 01:26:19.820]   Again, don't forget the scale, that larger scale here makes smaller scale there, vice versa.
[01:26:19.820 --> 01:26:23.820]   And then also quite interesting, and you actually know this at just mirror of the picture,
[01:26:23.820 --> 01:26:30.820]   but actually the Fourier transform, if you want a perfect filter, this will end up being the perfect filter,
[01:26:30.820 --> 01:26:33.820]   because why is it a perfect filter?
[01:26:33.820 --> 01:26:40.820]   Well, it's essentially something that is exactly preserves a certain set of frequencies exactly, all flat,
[01:26:40.820 --> 01:26:45.820]   so it doesn't distort anything, it preserves exactly within a certain frequency range,
[01:26:45.820 --> 01:26:48.820]   and then, you know, kills off all the other frequencies.
[01:26:48.820 --> 01:26:52.820]   So in that sense, it will be something that will be the perfect filter,
[01:26:52.820 --> 01:26:57.820]   because it preserves exactly the signal we care about, and zeros out exactly everything else.
[01:26:57.820 --> 01:27:03.820]   But sadly enough, it's a filter that we see here is kind of an infinitely large filter,
[01:27:03.820 --> 01:27:08.820]   it kind of doesn't want to die out, it keeps kind of, you know, like only very slowly dying out,
[01:27:08.820 --> 01:27:13.820]   and so applying that filter to a finite image will already be a challenge by itself and so on.
[01:27:13.820 --> 01:27:17.820]   Okay.
[01:27:17.820 --> 01:27:21.820]   And so I think, I'll leave it at that, I'll just give you a preview of this thing.
[01:27:21.820 --> 01:27:31.820]   This will be the key, this is really one of the most important theorems of the course.
[01:27:31.820 --> 01:27:38.820]   The convolution theorem, we're not going to derive it, but essentially,
[01:27:38.820 --> 01:27:43.820]   the convolution theorem can be used in two ways.
[01:27:43.820 --> 01:27:50.820]   So the Fourier transform of the convolution of two functions is the product of their Fourier transforms.
[01:27:50.820 --> 01:27:51.820]   Okay.
[01:27:51.820 --> 01:27:56.820]   So if we convolve two functions, which is a model for applying a filter operation, right,
[01:27:56.820 --> 01:27:58.820]   you saw that last week.
[01:27:58.820 --> 01:28:02.820]   So doing a convolution, which, you know, can be to blur the image,
[01:28:02.820 --> 01:28:05.820]   or to do, you know, all these kind of linear filter operations,
[01:28:05.820 --> 01:28:12.820]   or also, you know, convolutions and correlations, as we've seen, we can represent all of those the same.
[01:28:12.820 --> 01:28:19.820]   So those filtering operations in the spatial domain, in the frequency domain,
[01:28:19.820 --> 01:28:22.820]   they're just the product of those functions.
[01:28:22.820 --> 01:28:28.820]   Product of the functions, if you remember, if you think of this in discrete,
[01:28:28.820 --> 01:28:34.820]   in a discrete representation, it really means, like, if you think of basis transforms in linear algebra
[01:28:34.820 --> 01:28:38.820]   and all that stuff, this is really where you can do a point-by-point,
[01:28:38.820 --> 01:28:44.820]   direct kind of effect computation, like, you know,
[01:28:44.820 --> 01:28:47.820]   like if you want to know what happens to a certain frequency,
[01:28:47.820 --> 01:28:52.820]   it's not dependent on all the frequencies, all the, so if you want to know what happens to a point in the space here,
[01:28:52.820 --> 01:28:54.820]   you have to combine many points.
[01:28:54.820 --> 01:28:56.820]   With a convolution, you have to bring many points together, isn't that?
[01:28:56.820 --> 01:29:03.820]   In the frequency domain, what happens to one frequency is completely separate from what happens to another frequency.
[01:29:03.820 --> 01:29:06.820]   Every frequency just gets multiplied through a filtering operation.
[01:29:06.820 --> 01:29:11.820]   That frequency just has its own effect, it doesn't depend on other frequencies.
[01:29:11.820 --> 01:29:20.820]   What happens to frequency, whatever, you know, UV, it's directly based on what the filter does to UV,
[01:29:20.820 --> 01:29:24.820]   it doesn't depend on the presence of other frequencies in the image or something like that.
[01:29:24.820 --> 01:29:29.820]   And vice versa, there's the opposite one, so we'll look at that in detail on Thursday.
[01:29:30.820 --> 01:29:35.820]   (Applause)

