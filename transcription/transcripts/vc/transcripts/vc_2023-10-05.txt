
[00:00:00.000 --> 00:00:08.540]   Hello everyone. So today's lecture we are covering the image features. I'm
[00:00:08.540 --> 00:00:14.520]   covering again for Professor Mark Poliface. He will be back on Tuesday to do
[00:00:14.520 --> 00:00:22.160]   the next lectures with you. So to give you kind of a bit of an impression what
[00:00:22.160 --> 00:00:32.840]   image features are. Let's see if I can show you the videos. So to give you a
[00:00:32.840 --> 00:00:48.560]   brief introduction on what we are going to learn. So as you can see here
[00:00:48.560 --> 00:00:53.560]   they took images from the internet and they're basically capable of taking
[00:00:53.560 --> 00:00:57.280]   this random images taken from tourists and actually capable of doing 3d
[00:00:57.280 --> 00:01:02.840]   reconstruction. So you can see here all these colored points are actually
[00:01:02.840 --> 00:01:08.960]   features. So this is a bit to show you that everything you're going to learn in
[00:01:08.960 --> 00:01:12.640]   this lecture actually has a real application and you can actually do
[00:01:12.640 --> 00:01:16.720]   really cool stuff. Like you can see here where they are reconstructing the
[00:01:16.720 --> 00:01:21.920]   travesty fountain and this is all basically with the pictures you post
[00:01:21.920 --> 00:01:26.880]   every day on Instagram. So they don't have information about pose, they don't
[00:01:26.880 --> 00:01:31.280]   have anything, they even have pictures with you posing in front of the
[00:01:31.280 --> 00:01:37.880]   monument and this is all done with features and we basically can match
[00:01:37.880 --> 00:01:43.520]   elements in the images together and then okay this is not interesting anymore
[00:01:43.520 --> 00:01:51.560]   for you. But just here to show you another one this is actually one of the
[00:01:51.560 --> 00:02:01.880]   papers of Mark. They basically took again random pictures, holiday pictures and
[00:02:01.880 --> 00:02:08.960]   they actually managed to do this not in a super super cluster with tons of GPUs
[00:02:08.960 --> 00:02:14.920]   they actually did this with one GPU and you can see here like there are some
[00:02:14.920 --> 00:02:19.640]   pictures that they kind of are showing how they they're using sift which you're
[00:02:19.640 --> 00:02:26.000]   going to learn as well later. Let's try to find one of the reconstructions.
[00:02:26.000 --> 00:02:32.600]   Okay here you can see that all the pictures they are showing before they're
[00:02:32.600 --> 00:02:35.800]   actually using them so each of this kind of cameras that you're seeing
[00:02:35.800 --> 00:02:40.240]   floating was actually predicted this pose and then they were actually capable
[00:02:40.240 --> 00:02:44.840]   of reconstructing the coliseum which as you can see they just have one view point
[00:02:44.840 --> 00:02:52.320]   so you don't have a 360 of it. So this is basically just to show you that what
[00:02:52.320 --> 00:02:57.720]   you're learning here is later used in research so if any of you decide to do
[00:02:57.720 --> 00:03:03.360]   a PhD this is what you're maybe going to work on.
[00:03:03.360 --> 00:03:13.760]   Okay so now that you have seen in a really high level what features are
[00:03:13.760 --> 00:03:22.920]   let's start with the lecture. Yeah so there seems to be a kind of miscommunication
[00:03:22.920 --> 00:03:30.120]   between me and your TA. He has the slides so he should be uploading them but I
[00:03:30.120 --> 00:03:35.040]   can't really tell you more. I just know I send them to him and he told me yeah he
[00:03:35.040 --> 00:03:43.200]   will upload them. You will get them that is the only information I have. I know
[00:03:43.200 --> 00:03:47.040]   there is actually some recordings from last year as well if you want to watch
[00:03:47.040 --> 00:03:52.040]   videos of Mark presenting this as well so but I can't really tell you anything
[00:03:52.040 --> 00:03:59.440]   because I'm not normally teaching this so I have no clue. So I will try to figure
[00:03:59.440 --> 00:04:04.600]   it out. I will try to push a bit Philip to kind of give you the slides. But yeah
[00:04:04.600 --> 00:04:09.240]   okay feel free to take pictures or something I saw in the last lecture that
[00:04:09.240 --> 00:04:12.520]   people were like taking pictures of the slides because it was easier to take
[00:04:12.520 --> 00:04:23.240]   notes so you're allowed just don't take pictures of me. Okay so let's start with
[00:04:23.240 --> 00:04:29.160]   the recap of what we did on Tuesday. You had now I think the two practical
[00:04:29.160 --> 00:04:32.960]   lectures so you should know by now what the difference is between the
[00:04:32.960 --> 00:04:40.200]   correlation and convolutions which basically in my way explained correlations
[00:04:40.200 --> 00:04:45.560]   are where the kernel is static on the on the image while on the convolution
[00:04:45.560 --> 00:04:51.720]   we're having this movement which caused a bit of confusion on Tuesday which is
[00:04:51.720 --> 00:04:55.080]   where the kernel is actually moving over the neighborhood of the pixel we're
[00:04:55.080 --> 00:04:59.960]   applying the convolution and in a mathematical way we're having the two
[00:04:59.960 --> 00:05:04.760]   equations which basically gives you an information about that the correlation
[00:05:04.760 --> 00:05:12.720]   is actually the opposite of the convolution. So yeah let's start with
[00:05:12.720 --> 00:05:17.240]   what we're doing today which is as we said we are doing image features so to
[00:05:17.240 --> 00:05:22.120]   that we need first need to kind of start so basically first of all we're doing
[00:05:22.120 --> 00:05:26.160]   template matching so what is template matching and basically in template
[00:05:26.160 --> 00:05:29.920]   matching we're having an image like this one and we're having a template which
[00:05:29.920 --> 00:05:33.680]   in this case you can see up there which is the eye and what we're trying to do
[00:05:33.680 --> 00:05:38.440]   is we are trying to localize the template actually in the image so in this
[00:05:38.440 --> 00:05:46.160]   case we find one eye and basically since we have a symmetric face you can
[00:05:46.160 --> 00:05:50.080]   actually find the second one as well so we have actually a double match in this
[00:05:50.080 --> 00:05:58.640]   case. Okay so how do we do this actually in a more mathematical way so basically
[00:05:58.640 --> 00:06:02.560]   what we're doing is a search for the best match by minimizing the intensity
[00:06:02.560 --> 00:06:09.240]   between the image and between the template squared so which we then can
[00:06:09.240 --> 00:06:15.720]   write as this huge equation which we can simplify which is good because we have
[00:06:15.720 --> 00:06:20.960]   the image which is fixed and we have the template so actually we're left with
[00:06:20.960 --> 00:06:27.000]   just one part of the equation which then we can actually define as basically the
[00:06:27.000 --> 00:06:34.000]   image convoluted over the template which is what we get out here so and as said
[00:06:34.000 --> 00:06:37.800]   before basically you can see the correlation is equivalent to the
[00:06:37.800 --> 00:06:41.440]   convolution of the image but we are changing the template the coordinate
[00:06:41.440 --> 00:06:49.560]   system. Okay so what happens with template matching so to kind of know if
[00:06:49.560 --> 00:06:54.440]   we're doing the template matching correctly we have to apply some kind
[00:06:54.440 --> 00:06:57.920]   of like filtering so that's why we're applying the Cauchy-Schwarz inequality
[00:06:57.920 --> 00:07:04.440]   which is this huge term so we're using the equation we had before from template
[00:07:04.440 --> 00:07:08.920]   matching and we have this has to be smaller or equal than the inequality of
[00:07:08.920 --> 00:07:15.480]   Cauchy-Schwarz so what this basically is showing us the equality is dependent on
[00:07:15.480 --> 00:07:21.000]   this value alpha which has to count a condition which has to be bigger or
[00:07:21.000 --> 00:07:26.920]   equal to zero so to kind of showcases in a block diagram which makes it I think
[00:07:26.920 --> 00:07:31.800]   easier to kind of see we're having our image we're having our given template
[00:07:31.800 --> 00:07:37.920]   and we're getting our response so basically we are trying to search for
[00:07:37.920 --> 00:07:43.440]   the peaks what is what we are trying to find and hopefully in the end we get the
[00:07:43.440 --> 00:07:49.480]   object location so that's kind of the normal like pipeline we are going to
[00:07:49.480 --> 00:08:01.560]   follow to actually get get template matching in this case. Okay so yeah
[00:08:01.560 --> 00:08:07.560]   it's mostly to kind of like have because in the end so as you have seen in the
[00:08:07.560 --> 00:08:11.400]   example of the eye we're doing template matching but there's also a lot of things
[00:08:11.400 --> 00:08:16.200]   that kind of like as an example the eye you were clearly seeing which eye it was
[00:08:16.200 --> 00:08:20.240]   while on the network it's much more complex when you're doing this on a
[00:08:20.240 --> 00:08:25.880]   computer level to actually see the difference so basically the Cauchy-Schwarz
[00:08:25.880 --> 00:08:29.240]   inequality what it's doing it's kind of helping you to actually filter between
[00:08:29.240 --> 00:08:34.360]   the strong information and between the weaker information so that's kind of how
[00:08:34.360 --> 00:08:39.080]   you need to on a really high level envision this it's just ways of kind of
[00:08:39.080 --> 00:08:43.240]   being really sure that what we are doing is actually the correct match you will
[00:08:43.240 --> 00:08:47.920]   see further in the lecture that we will have to deal with more than this kind of
[00:08:47.920 --> 00:08:51.600]   things just because in the end we are having a lot of matches because images
[00:08:51.600 --> 00:08:57.160]   are really complex I mean the example that you had was kind of simple that's
[00:08:57.160 --> 00:09:03.920]   why why you it will make more sense the more we advance here by the way if you
[00:09:03.920 --> 00:09:08.320]   have a question make a noise because I don't really like can see all of you at
[00:09:08.320 --> 00:09:14.320]   the same time okay so let's start I mean you have done as far as I know you did
[00:09:14.320 --> 00:09:21.240]   edge detection already in the practical classes right okay so basically to
[00:09:21.240 --> 00:09:26.800]   define what edge detection is we have here what is the idea in the continuous
[00:09:26.800 --> 00:09:31.240]   space so we have we're detecting the local gradients which are mathematically
[00:09:31.240 --> 00:09:36.040]   defined like this but of course this changes on a digital image which is
[00:09:36.040 --> 00:09:40.000]   what we are mostly interested in which some of these things you have already
[00:09:40.000 --> 00:09:43.740]   seen we saw them on Tuesday but you also have seen them probably in the
[00:09:43.740 --> 00:09:48.840]   practical classes you have the differences central difference so we are
[00:09:48.840 --> 00:09:56.040]   minus one one and the zero in between we have a pre-wit filter and the sobel
[00:09:56.040 --> 00:10:07.840]   filter which both cases are highlighting the vertical edges okay so I mean half
[00:10:07.840 --> 00:10:11.920]   of these filters you saw them already on Tuesday so we're having pre-wits sobel
[00:10:11.920 --> 00:10:17.800]   and robert so what are we showcasing here we can see that we can apply the
[00:10:17.800 --> 00:10:22.040]   pre-wit filter and the sobel filter on the vertical edges but also on the
[00:10:22.040 --> 00:10:27.240]   horizontal edges while of course these two cover the horizontal and the
[00:10:27.240 --> 00:10:32.640]   verticals and robert's covers the diagonal edges so this is what you kind
[00:10:32.640 --> 00:10:37.240]   of need to keep in mind out of here that we can actually track all of the
[00:10:37.240 --> 00:10:41.840]   different edges that kind of appear in an image so to kind of visualize this I
[00:10:41.840 --> 00:10:47.240]   actually try to showcase this better since on Tuesday was not really nicely
[00:10:47.240 --> 00:10:54.040]   visible and it was just me trying to show you things so now I put circles so
[00:10:54.040 --> 00:10:58.560]   first of all let's focus on this one which is the vertical one on the pre-wit
[00:10:58.560 --> 00:11:02.600]   filter you can see here in this part of the bridge that actually here we are
[00:11:02.600 --> 00:11:06.840]   highlighting the lines of the bridge that are vertical while in this case which
[00:11:06.840 --> 00:11:12.200]   is applied the horizontal one they are noticeable but not compared to this one
[00:11:12.200 --> 00:11:18.560]   while if we take the horizontal one you can actually see here that the upper
[00:11:18.560 --> 00:11:22.480]   part of the bridge which in the horizontal one is actually highlighted and
[00:11:22.480 --> 00:11:28.680]   we can clearly see it in the vertical one we cannot see it at all so of course
[00:11:28.680 --> 00:11:33.840]   this image is kind of like really specific on lines and it has all of them
[00:11:33.840 --> 00:11:40.480]   but let's apply this to an image that is yes this image you will see it a lot in
[00:11:40.480 --> 00:11:46.440]   computer vision this is Bill and basically Bill's face is actually often
[00:11:46.440 --> 00:11:51.400]   used to showcase the filters so we are applying exactly the same filters that
[00:11:51.400 --> 00:11:55.200]   we were applying before we are having the vertical edges and the horizontal edges
[00:11:55.200 --> 00:12:01.320]   so for the vertical ones you can actually see here on the side of his face
[00:12:01.320 --> 00:12:06.400]   that the hairline as an example here or the face line is much more highlighted
[00:12:06.400 --> 00:12:13.720]   than on the horizontal one well in the horizontal one we have of course the
[00:12:13.720 --> 00:12:18.160]   upper part of his head well in this case we're actually not having any information
[00:12:18.160 --> 00:12:26.840]   about what is going on here okay so to show this what we are basically doing is
[00:12:26.840 --> 00:12:31.440]   we're taking all the horizontals and all the verticals and doing the logarithmic
[00:12:31.440 --> 00:12:35.040]   summation of basically the gradients on the horizontal and vertical and this is
[00:12:35.040 --> 00:12:40.440]   what you can see so as you can see it is really sensitive we are having tons of
[00:12:40.440 --> 00:12:44.800]   noise in this area so we are actually having prediction on edges which we
[00:12:44.800 --> 00:12:48.920]   don't we are not really interested in having the background we're interested
[00:12:48.920 --> 00:12:55.120]   actually in Bill's face so what we can do to actually show how sensible this is
[00:12:55.120 --> 00:12:58.920]   and how actually weak or strong some of these lines are we can apply a threshold
[00:12:58.920 --> 00:13:03.360]   so basically in black we have everything that's under the threshold and white
[00:13:03.360 --> 00:13:08.400]   everything that's above is so in the first threshold we are still kind of we
[00:13:08.400 --> 00:13:14.240]   got actually rid of most of the noise and we're having Bill's face kind of nicely
[00:13:14.240 --> 00:13:19.520]   highlighted but as you can see also the the borders are really thick while in
[00:13:19.520 --> 00:13:23.840]   the second threshold this looks much better but we are already starting to
[00:13:23.840 --> 00:13:29.040]   lose parts of the details that we can see in Bill's face and then the third one it
[00:13:29.040 --> 00:13:33.440]   looks better but yeah like as an example here the cheek we totally lost these
[00:13:33.440 --> 00:13:41.720]   details basically because it was not a strong enough edge so now let's move to
[00:13:41.720 --> 00:13:53.280]   the sobel filter doing exactly the same as we have done before doing exactly the
[00:13:53.280 --> 00:13:56.960]   same thing as we have done before you can see that again we're having the noise
[00:13:56.960 --> 00:14:02.320]   I recommend when Philip uploads you this slides to actually check it's really
[00:14:02.320 --> 00:14:05.560]   difficult to see the difference between a pre-wit and a sobel filter I
[00:14:05.560 --> 00:14:10.040]   recommend you to for me kind of the difference you can see it mostly on his
[00:14:10.040 --> 00:14:16.400]   forehead I can try to move it quickly but not sure how good that works but you
[00:14:16.400 --> 00:14:20.320]   can see that the pattern changes which basically shows you that both filters are
[00:14:20.320 --> 00:14:25.440]   not doing the same thing but basically when we apply the different thresholds
[00:14:25.440 --> 00:14:42.320]   you can see that both of them are just removing the same kind of lines yes I
[00:14:42.320 --> 00:14:52.560]   assume because this lecture was made maybe 15 years ago it's really so a lot
[00:14:52.560 --> 00:14:56.040]   of things you will actually see I mean since you're currently starting you will
[00:14:56.040 --> 00:15:00.680]   have actually have a lot of things that will repeat and a lot of old images and
[00:15:00.680 --> 00:15:05.640]   a lot of things that nowadays we actually use in a much improved way so but it
[00:15:05.640 --> 00:15:10.880]   still means that you need to learn the how it all kind of started and how the
[00:15:10.880 --> 00:15:17.640]   basics are so yeah in this case it's just also to highlight you mostly that
[00:15:17.640 --> 00:15:21.600]   you're kind of having here a lot of noise and a lot of kind of issues actually
[00:15:21.600 --> 00:15:27.200]   with the sensitivity of these filters but yeah you can see that actually the
[00:15:27.200 --> 00:15:35.240]   outcome is really similar between sobel and pruit what this looks like so just
[00:15:35.240 --> 00:15:40.360]   to show you also the diagonal ones we're having again bill and in this case we
[00:15:40.360 --> 00:15:45.080]   are applying the logarithm magnitude of the image filter by the diagonals so
[00:15:45.080 --> 00:15:50.880]   again you can see on one of them we're having this part of the forehead actually
[00:15:50.880 --> 00:15:55.920]   we can actually have edges here well in this one we're actually not capable of
[00:15:55.920 --> 00:16:01.720]   having kind of there is some information but not too much well on the other side
[00:16:01.720 --> 00:16:05.640]   here actually in this one was not capable of having any edges here while this
[00:16:05.640 --> 00:16:12.200]   diagonal was actually capable of predicting the edges okay so doing
[00:16:12.200 --> 00:16:19.840]   exactly the same threshold we are having the same prediction of the of
[00:16:19.840 --> 00:16:25.120]   the summation of the gradients and applying the threshold you can actually
[00:16:25.120 --> 00:16:30.400]   see that this one is actually capable of having at least some more details in
[00:16:30.400 --> 00:16:34.720]   the face than pruit and so will wear for the horizontal and vertical ones so we're
[00:16:34.720 --> 00:16:39.320]   actually capable of keeping more of these so it shows that actually the edges
[00:16:39.320 --> 00:16:46.640]   that were predicted were more robust okay so which brings us now to the
[00:16:46.640 --> 00:16:51.880]   laplation operator so basically the laplation operator text discontinuities
[00:16:51.880 --> 00:16:56.600]   by considering second derivatives so this is kind of the how we write the
[00:16:56.600 --> 00:17:00.880]   laplation operator down what is interesting about this one is is
[00:17:00.880 --> 00:17:07.200]   rotationally invariant so that's basically one of the most important
[00:17:07.200 --> 00:17:12.560]   things we have the zero crossing mass mark edge location and we are basically
[00:17:12.560 --> 00:17:16.200]   using discrete space approximations by convolutions with three by three impulse
[00:17:16.200 --> 00:17:20.440]   response so as you can see here we're highlighting the central pixel and the
[00:17:20.440 --> 00:17:24.320]   neighborhood and in this one we are highlighting all the pixels that are
[00:17:24.320 --> 00:17:29.520]   surrounding the central pixel and of course the pixel so to show this in a
[00:17:29.520 --> 00:17:35.440]   visual way we are showing the the derivatives the first one so we have
[00:17:35.440 --> 00:17:39.760]   the edge profile then we have the first derivative and this is the second one so
[00:17:39.760 --> 00:17:44.400]   you can see here this detects the zero crossing which in our case we are
[00:17:44.400 --> 00:17:51.040]   interested in because that will show us where the edge is actually located so
[00:17:51.040 --> 00:17:57.000]   applying this directly to an image looks like this so if the two that you saw
[00:17:57.000 --> 00:18:03.800]   before were sensitive this is even worse so how do we apply actually this well
[00:18:03.800 --> 00:18:07.640]   first we need to blur the image so by blurring the image we're actually capable
[00:18:07.640 --> 00:18:10.960]   of removing all of this what you can see here because this is quite an old
[00:18:10.960 --> 00:18:14.880]   picture so there are actually a lot of like details in the image in the
[00:18:14.880 --> 00:18:18.720]   background so if you blur the image we're actually capable of removing most
[00:18:18.720 --> 00:18:26.720]   of the noise and then basically it reacts the same to weak and strong edges so
[00:18:26.720 --> 00:18:30.080]   what we're doing is we need to kind of come up with a suppression way to the
[00:18:30.080 --> 00:18:37.480]   edges with the low gradient so how do we do that well we blur the image with a
[00:18:37.480 --> 00:18:42.040]   Gaussian and Laplacian operator so which we can combine in the Laplacian of a
[00:18:42.040 --> 00:18:48.120]   Gaussian which you can see here this is the 3d representation of the 2d this is
[00:18:48.120 --> 00:18:54.360]   how it would look like in a matrix form on a standard deviation of 1.4
[00:18:54.360 --> 00:19:00.760]   okay so how does this look in an image this is without the Gaussian we're
[00:19:00.760 --> 00:19:06.360]   having this really noisy image because we haven't applied the blur before so by
[00:19:06.360 --> 00:19:11.560]   applying the standard deviation of 1.4 on the zero crossing of the Laplacian of
[00:19:11.560 --> 00:19:16.000]   the Gaussian you can see that we are able with this really noisy image to
[00:19:16.000 --> 00:19:22.200]   reduce some of the noise but by actually increasing the standard deviation you
[00:19:22.200 --> 00:19:29.040]   can see that we are actually also getting rid of the details in Bill's face so by
[00:19:29.040 --> 00:19:35.520]   a standard deviation of 6 you can still kind of guess that this is a face but if
[00:19:35.520 --> 00:19:41.000]   I would not give you any of these before images you would not really know so
[00:19:41.000 --> 00:19:50.160]   that's kind of showing you how this looks so doing this but before using
[00:19:50.160 --> 00:19:54.400]   actually the gradient-based threshold you can see that already without the
[00:19:54.400 --> 00:20:00.000]   Gaussian we are not having all this noise that was here in the image so with
[00:20:00.000 --> 00:20:05.440]   the standard deviation 1.4 this one actually looks really good we are of
[00:20:05.440 --> 00:20:09.480]   course losing the information here on the side of the face but in a standard
[00:20:09.480 --> 00:20:15.800]   deviation of 3 most of the side of his face is already gone and then standard
[00:20:15.800 --> 00:20:22.280]   deviation of 6 again we are having just the contours of his face left I mean of
[00:20:22.280 --> 00:20:27.520]   course this depends on what you want to do if this is your hoped outcome then of
[00:20:27.520 --> 00:20:35.960]   course this is the perfect so okay now that you have seen this once let move to
[00:20:35.960 --> 00:20:42.600]   the canny edge detector so basically how we do this so first we smooth the image
[00:20:42.600 --> 00:20:46.920]   with the Gaussian filter then we need to calculate the magnitude of the angle in
[00:20:46.920 --> 00:20:50.560]   the gradient which you have defined here this is the magnitude and this is the
[00:20:50.560 --> 00:20:57.040]   angle this we do it by using solval or prudent then we use the non-maximal
[00:20:57.040 --> 00:21:01.480]   suppression to gradient magnitude images then we double the threshold to
[00:21:01.480 --> 00:21:05.000]   detect strong and weak edges which is exactly what we want we want to get rid
[00:21:05.000 --> 00:21:09.560]   of the ones that are not interesting for us and have the big ones and then we
[00:21:09.560 --> 00:21:16.240]   reject the weak edges with no connectors with strong edge pixels so this last
[00:21:16.240 --> 00:21:21.600]   information looks like something like this so what we are doing is we're doing
[00:21:21.600 --> 00:21:25.640]   the edge normals to one of four directions so we have the horizontal minus
[00:21:25.640 --> 00:21:33.480]   55 degrees vertical and plus 45 degrees so to kind of show you a bit let's say
[00:21:33.480 --> 00:21:38.000]   we're doing this diagonal we're doing this point so what we are checking is if
[00:21:38.000 --> 00:21:43.240]   the neighbors of this point are stronger or weaker if this one is stronger than
[00:21:43.240 --> 00:21:48.400]   is two neighbors then we keep it if not we suppress it so in this case I don't
[00:21:48.400 --> 00:21:54.480]   really know what you see from there but this and this are supposed to be like a
[00:21:54.480 --> 00:21:59.800]   more like kind of not that white color well this one is white which is supposed
[00:21:59.800 --> 00:22:05.040]   to mean that it is a stronger one so that's why we keep this one because it's
[00:22:05.040 --> 00:22:08.600]   stronger than his two neighbors
[00:22:08.600 --> 00:22:18.080]   so to basically do this to apply this threshold and suppress the weak edges
[00:22:18.080 --> 00:22:22.480]   what we are doing is we are doubling the threshold of the gradient magnitude so
[00:22:22.480 --> 00:22:27.960]   we are using these two variables with our theta high and theta low which
[00:22:27.960 --> 00:22:32.640]   basically defined for us what the strong edges and what the weak edges so the
[00:22:32.640 --> 00:22:37.760]   normally the the ratio between both of them is between two and three and what
[00:22:37.760 --> 00:22:42.720]   we are doing is if the magnitude is bigger or equal to theta high then it's a
[00:22:42.720 --> 00:22:48.840]   strong edge well if we are having this equation so the magnitude is actually
[00:22:48.840 --> 00:22:56.480]   being smaller than theta high but higher than theta low then it's a weak edge so
[00:22:56.480 --> 00:23:02.160]   yep and of course we reject an entire region if there is no edge pixels because
[00:23:02.160 --> 00:23:07.800]   there is nothing interesting for us so how does this look in the same manner as
[00:23:07.800 --> 00:23:13.680]   we have done before on build space we can see the canny edge detection so we're
[00:23:13.680 --> 00:23:18.440]   having his face we are applying all the thresholds we are filtering out all the
[00:23:18.440 --> 00:23:25.200]   kind of weak edges and what you can see is with the standard deviation 1.4 we're
[00:23:25.200 --> 00:23:28.880]   actually having a good reconstruction of his face and most of the small details
[00:23:28.880 --> 00:23:34.760]   we are of course having the background which are these two lines but I think
[00:23:34.760 --> 00:23:40.440]   they are too strong in the as edges to actually be filtered out standard
[00:23:40.440 --> 00:23:44.960]   deviation of three still doesn't get rid of this one but is actually mostly
[00:23:44.960 --> 00:23:49.840]   getting rid of most of the things in his face why not a standard deviation of six
[00:23:49.840 --> 00:23:56.600]   we're actually left with less than we had before also just that you kind of
[00:23:56.600 --> 00:23:59.920]   notice which is a kind of interesting thing it's also taking into account of
[00:23:59.920 --> 00:24:06.280]   course image issues so we're having the wide line that basically this image has
[00:24:06.280 --> 00:24:11.760]   from actually being cropped out wrongly so to have another image which maybe if
[00:24:11.760 --> 00:24:17.640]   you have done a computer vision course before you have seen it as well we're
[00:24:17.640 --> 00:24:21.840]   having this photographer and you can see that with the canny edge detector we are
[00:24:21.840 --> 00:24:27.200]   able to predict without too much noise in the background the nice and stronger
[00:24:27.200 --> 00:24:30.440]   edges of the image which are the most interesting for us which is mostly the
[00:24:30.440 --> 00:24:38.120]   photographer the the the camera and the the background as in the buildings still
[00:24:38.120 --> 00:24:47.120]   we are having here this grass area is creating noise for us so we still we still
[00:24:47.120 --> 00:24:57.400]   have there some filtering to do so let's introduce the half transformation so
[00:24:57.400 --> 00:25:03.600]   okay so to do this since we are talking about edges the half transformation is
[00:25:03.600 --> 00:25:10.360]   to be able to actually know where those edges are localized so the problem is
[00:25:10.360 --> 00:25:16.880]   we want to fit a straight line or curve to a set of edge pixels so which by the
[00:25:16.880 --> 00:25:21.680]   way this was invented in 1962 and we're still using it nowadays so nobody has
[00:25:21.680 --> 00:25:27.600]   come up with anything better so this what it's doing is generalizing the
[00:25:27.600 --> 00:25:32.000]   template matching technique so what we do is we consider a straight line which
[00:25:32.000 --> 00:25:37.640]   you have the equation y is equal to the slope multiplied by x plus the
[00:25:37.640 --> 00:25:45.200]   intercept and we draw both of these axes y and x and the slope and the
[00:25:45.200 --> 00:25:51.240]   intercept and we draw our edge pixel which is our first edge pixel which we
[00:25:51.240 --> 00:25:56.000]   put randomly here and we draw a first line the only condition we have in this
[00:25:56.000 --> 00:26:02.520]   case is that we want to cross the edge pixel you will see in a second why so
[00:26:02.520 --> 00:26:09.040]   we're having our first line so what does this line mean since y is constant we
[00:26:09.040 --> 00:26:16.600]   can actually draw it on the intercept axis let's draw another line this one of
[00:26:16.600 --> 00:26:21.640]   course is perfectly drawn so it goes through the origin what does this mean
[00:26:21.640 --> 00:26:27.040]   that x is zero so we can actually draw it on the slope let's put a third line
[00:26:27.040 --> 00:26:33.960]   which is randomly in between both of them and as you can see here if the lines
[00:26:33.960 --> 00:26:41.000]   crossed across the edge pixel we're actually able to adjust them to a line
[00:26:41.000 --> 00:26:46.320]   which is you will see some more examples but will actually show us in what
[00:26:46.320 --> 00:26:52.520]   position we can actually find the edge so let's see how we do this and how we
[00:26:52.520 --> 00:26:59.200]   find out actually the position so we first of all divide the slope and the
[00:26:59.200 --> 00:27:06.520]   intercept in bins so we create our grid and then what we are doing is we draw our
[00:27:06.520 --> 00:27:13.720]   first line in the slope and in the intercept first line is actually a point
[00:27:13.720 --> 00:27:20.040]   in the in the so yeah we're drawing basically our lines so we draw a second
[00:27:20.040 --> 00:27:27.360]   line we draw a third line fourth line so this is how we actually find what is an
[00:27:27.360 --> 00:27:32.560]   edge because you can see all the lines that cross one point in the slope and in
[00:27:32.560 --> 00:27:40.360]   the intercept this point is our peak and that peak in the x and y axis can
[00:27:40.360 --> 00:27:47.160]   actually adjust it to a line and this will be an edge on our image we will do
[00:27:47.160 --> 00:27:52.480]   some more examples where you can kind of see this in a abstract way with points
[00:27:52.480 --> 00:27:58.640]   and then later we will see some real example so that this will be much clearer
[00:27:58.640 --> 00:28:08.360]   for you hopefully so I mean does anyone have an idea yeah
[00:28:09.360 --> 00:28:17.420]   of course of course let me try to come up with a different way okay so what we
[00:28:17.420 --> 00:28:21.200]   want to do with the half transformation I'm showing you in this both slides it's
[00:28:21.200 --> 00:28:28.760]   kind of the idea that you see so that in one case we are having points which are
[00:28:28.760 --> 00:28:33.880]   projected on our image which is y and x and then we're having a slope and or
[00:28:33.880 --> 00:28:37.760]   intercept which is because we want to adjust this to a line since in the end
[00:28:37.760 --> 00:28:44.400]   the edge is a line so I'm showing you the both ways of doing this what is the
[00:28:44.400 --> 00:28:48.960]   computer going to do so we're having our edge pixel which is going to be
[00:28:48.960 --> 00:28:57.320]   detected by this filters so what happens is normally the lines in this case
[00:28:57.320 --> 00:29:05.280]   cross edge pixels so we are run randomly drawing lines I don't know what's the
[00:29:05.280 --> 00:29:12.360]   part of drawing it on the slope and intercept clear no okay so I mean you
[00:29:12.360 --> 00:29:20.040]   have the equation of a line right so when e is constant you know that you can
[00:29:20.040 --> 00:29:29.160]   draw c is also so it's basically going to the point is going to here okay so you
[00:29:29.160 --> 00:29:36.240]   need to envision that e is going to be constant x is going to be constant no y
[00:29:36.240 --> 00:29:40.920]   is going to be constant so basically while applying this equation you have
[00:29:40.920 --> 00:29:51.120]   that the slope and the it's just on the on the intercept when you're having x
[00:29:51.120 --> 00:29:57.920]   equal to zero which is here this case since this goes to zero you're having it
[00:29:57.920 --> 00:30:06.560]   on the on the intercept now yes okay and what this shows you is that you have a
[00:30:06.560 --> 00:30:12.000]   correlation between the intercept and the slope and the image so you're having
[00:30:12.000 --> 00:30:16.640]   all the lines that cross an edge pixel in the slope and in the intercept are
[00:30:16.640 --> 00:30:24.320]   forming a line so this is to showcase you the idea of the half transformation but
[00:30:24.320 --> 00:30:32.560]   the interesting part which is what how we are going to do this is the other way
[00:30:32.560 --> 00:30:41.000]   around so we are having lines in the intercept and in the slope which we are
[00:30:41.000 --> 00:30:44.680]   drawing which are equivalent so we're doing the opposite of the equation that
[00:30:44.680 --> 00:30:51.600]   you had of the line so you're putting them as points and you're having this peak
[00:30:51.600 --> 00:30:59.640]   and this peak so the interception of all the lines in the slope it believe me in
[00:30:59.640 --> 00:31:03.520]   the example in the visual example it is easier to show this is really that you
[00:31:03.520 --> 00:31:08.000]   get the idea that there is a correlation between all the edges that we have on an
[00:31:08.000 --> 00:31:12.320]   image and the representation of the slope and the interception where we have our
[00:31:12.320 --> 00:31:17.440]   peak which is clearly actually showing us where the position is sadly this is not
[00:31:17.440 --> 00:31:22.120]   really a good example to like visualize this but there are really practical
[00:31:22.120 --> 00:31:32.440]   examples further on okay I mean well was that clear okay but I had a question
[00:31:32.440 --> 00:31:38.680]   here can someone come up with a reason why this cannot work always
[00:31:43.840 --> 00:31:51.680]   well we're well we are doing right now adjusting it to a line that part is good
[00:31:51.680 --> 00:31:56.120]   but I want the more obvious one we are having lines in what case the line
[00:31:56.120 --> 00:32:10.400]   doesn't work yes yes so we will come to this to the curved part but can someone
[00:32:10.400 --> 00:32:25.040]   see in this case how it could not work yes no that works yeah
[00:32:25.040 --> 00:32:38.080]   exactly and some other where you're going to say the same thing exactly yeah
[00:32:38.080 --> 00:32:45.240]   that then doesn't work because it's not an edge anyone else has an idea well
[00:32:45.240 --> 00:32:50.760]   basically when the line is vertical the slope is infinite so this would not
[00:32:50.760 --> 00:33:00.560]   really work so to do that we have to pass it to actually angles so what we are
[00:33:00.560 --> 00:33:05.680]   doing is we're having our line which we had before with is our edge and what we're
[00:33:05.680 --> 00:33:10.000]   doing is we are finding the point which is the closest to the region and we
[00:33:10.000 --> 00:33:18.120]   define theta angle and draw and we can write this equation so instead of having
[00:33:18.120 --> 00:33:24.080]   a line we will have this equation for me that's why I added this it was easier to
[00:33:24.080 --> 00:33:28.440]   understand how this equation comes from if you think about the two corner cases
[00:33:28.440 --> 00:33:35.360]   which is when y is equal to zero and x is equal to zero I hope that makes it more
[00:33:35.360 --> 00:33:41.480]   logic but to showcase how this looks of course since you're working with now
[00:33:41.480 --> 00:33:47.920]   casines and scenes we have our edge pixel which is this one which is far away
[00:33:47.920 --> 00:33:56.360]   from the origin and we're having a senoidal wave well if the edge is
[00:33:56.360 --> 00:34:06.000]   actually on the axis y we're having it cross the origin this is how this looks
[00:34:06.000 --> 00:34:11.080]   which now come the examples now it makes it easier to understand so first of all
[00:34:11.080 --> 00:34:14.880]   one of you said what happens when you're having points and we're having one which
[00:34:14.880 --> 00:34:19.200]   is not crossing so we're having here four which are actually forming a line and
[00:34:19.200 --> 00:34:25.760]   this one is not doing anything like it's not crossing at all so represented in the
[00:34:25.760 --> 00:34:33.760]   in the intercept in the row and cita you can see here the curve representation of
[00:34:33.760 --> 00:34:39.960]   the curves so these four points are these four so you can see they're all
[00:34:39.960 --> 00:34:45.920]   intercepting because they form a line so this is our peak while this fourth line
[00:34:45.920 --> 00:34:52.040]   you can see it has some correspondence with the other lines but it is really not
[00:34:52.040 --> 00:34:58.240]   like we cannot find a point where all five intercept so represented in 3d
[00:34:58.240 --> 00:35:05.320]   format you can see here this is the highest point this is our our peak and
[00:35:05.320 --> 00:35:11.120]   this four are every time the outlayer is actually crossing with one of the lines
[00:35:11.120 --> 00:35:16.840]   but you can see that it is less important than this one
[00:35:19.120 --> 00:35:24.360]   let's try another example this is actually a dense line which in the end is
[00:35:24.360 --> 00:35:31.840]   formed by a lot of points so I'm not sure if you can see even the red lines but
[00:35:31.840 --> 00:35:39.280]   this is really formed with a lot of synodal curves so these all are coming
[00:35:39.280 --> 00:35:44.520]   together to this one point which is our edge pixel because all of them are on
[00:35:44.520 --> 00:35:48.960]   one line so we're having something like this in 3d which all go together
[00:35:48.960 --> 00:35:53.000]   to our one peak point
[00:35:53.000 --> 00:35:59.360]   does this make sense yes yeah
[00:35:59.360 --> 00:36:09.480]   it's not really that it is inconclusive what we would know is then that these
[00:36:09.480 --> 00:36:15.200]   four are actually an edge while these four is not really part of it so it's
[00:36:15.200 --> 00:36:20.280]   just that this one is weak while these ones are strong so the these ones are
[00:36:20.280 --> 00:36:25.480]   also forming in the end interceptions but as you can see they are not strong
[00:36:25.480 --> 00:36:34.880]   enough okay so to show you an example of we are having here of course this is a
[00:36:34.880 --> 00:36:39.680]   dense line so that's why this one has sadly not really too visible but you
[00:36:39.680 --> 00:36:43.240]   have a lot of lines that are coming together on this one point that's why
[00:36:43.240 --> 00:36:48.360]   this actually this peak is really high well if we do this which is actually a
[00:36:48.360 --> 00:36:52.160]   dotted line so it's less dense than before we having less points that are
[00:36:52.160 --> 00:36:59.440]   all on the same line not visible but here we have now actually less curves and
[00:36:59.440 --> 00:37:07.080]   if you have noticed on the axis this peak is of course clearly the one where all
[00:37:07.080 --> 00:37:12.760]   the lines intercept but at the same time it's less high than this one it's
[00:37:12.760 --> 00:37:20.520]   basically because this one is crossed by much more points than this one okay so
[00:37:20.520 --> 00:37:24.000]   now to apply this of course you're seeing this all with points with kind of
[00:37:24.000 --> 00:37:31.320]   lines but in our idea what we want to do is we want to know edges right so here
[00:37:31.320 --> 00:37:36.840]   you're having a map of the building so what we first do because of course this
[00:37:36.840 --> 00:37:41.760]   is color this is kind of to showcase how the map looks we do an edge detection
[00:37:41.760 --> 00:37:46.160]   with Pruitt which gives us this and what we want to do is we want to find
[00:37:46.160 --> 00:37:54.120]   actually the the edges in the images so we represent Aurora and Arthita and we're
[00:37:54.120 --> 00:37:58.680]   having all these curves and it's exactly what you were actually pointing you can
[00:37:58.680 --> 00:38:04.640]   see this has of course a lot of lines so each of these points which is a peak
[00:38:04.640 --> 00:38:13.240]   corresponds to one of those lines does anyone want to try to guess what line is
[00:38:13.240 --> 00:38:23.200]   this point you mean this one yes it is so why well we're having a raw and
[00:38:23.200 --> 00:38:30.440]   Arthita representation so we are actually going down we take our angle we divide
[00:38:30.440 --> 00:38:36.920]   the image in x and y we're having our angle represented and since we know that
[00:38:36.920 --> 00:38:43.880]   the angle and bra are actually orthogonal to each other we know the value it is
[00:38:43.880 --> 00:38:50.520]   which is -110 we draw the line and that line is actually corresponding to this
[00:38:50.520 --> 00:38:56.120]   first line where the second point is corresponding to this one and now you
[00:38:56.120 --> 00:39:01.360]   can actually see in this one they are actually not clearly defined here are
[00:39:01.360 --> 00:39:04.680]   some that are clearly defined would are probably the big ones that you can see
[00:39:04.680 --> 00:39:09.520]   in the upper part of the image but all these ones are all the small ones so of
[00:39:09.520 --> 00:39:15.120]   course compared with the huge ones in the image they are much weaker than all
[00:39:15.120 --> 00:39:27.560]   the others okay is that clear okay then I would maybe do the break now because the
[00:39:27.560 --> 00:39:33.720]   next comes the circles which you all wanted to hear about so I would do the
[00:39:33.720 --> 00:39:42.760]   break now and then basically explain to you the rest after hello together can
[00:39:42.760 --> 00:39:49.120]   you hear me I would like to introduce eth juniors to you to give a rough overview
[00:39:49.120 --> 00:39:57.840]   what we actually do first of all what's our mission so we are doing knowledge
[00:39:57.840 --> 00:40:04.040]   transfer between the academia and the industry by doing projects so with this
[00:40:04.040 --> 00:40:09.200]   we actually achieved a quite extensive network over the past 27 years since our
[00:40:09.200 --> 00:40:15.360]   foundation and that's something where you can also profit from so what's in for
[00:40:15.360 --> 00:40:23.320]   you yeah we have really nice projects to do where you can apply the algorithms
[00:40:23.320 --> 00:40:27.520]   the things you actually learn here and in the programming exercises so you can
[00:40:27.520 --> 00:40:32.960]   tackle real-world challenges and have a huge impact and then you also profit from
[00:40:32.960 --> 00:40:38.800]   the extensive network so you get in contact with potential companies you
[00:40:38.800 --> 00:40:43.200]   could work for and additionally on top of that we have very competitive salary
[00:40:43.200 --> 00:40:48.800]   because our projects or the companies actually benefit from our projects and
[00:40:48.800 --> 00:40:57.840]   in the end we have also flexible working for once locations but also part-time
[00:40:57.840 --> 00:41:01.280]   jobs which you can do besides your studies or in the masters or at the end
[00:41:01.280 --> 00:41:07.320]   of the bachelors and I think these are the main usp's from our side and with the
[00:41:07.320 --> 00:41:15.040]   next video I think it's the best way to give you a good yeah image of ETH juniors
[00:41:15.040 --> 00:41:18.880]   hopefully the sound is good
[00:41:18.880 --> 00:41:28.080]   let me tell you something about uni the professors really do everything began
[00:41:28.080 --> 00:41:34.000]   way back I just started out as a student and the semester was still fresh like a
[00:41:34.000 --> 00:41:41.160]   blank sheet of paper and just then something crossed my mind it was a
[00:41:41.160 --> 00:41:47.200]   simple thought that threatened to spoil the delicate ideal that studying is
[00:41:47.200 --> 00:41:54.400]   it dawned on me that the day would come when I would finally have that beautiful
[00:41:54.400 --> 00:42:04.960]   piece of paper on my bedroom wall and I would ask myself what now
[00:42:04.960 --> 00:42:11.480]   sure I thought the parents at home will be proud and sigh with relief but was
[00:42:11.480 --> 00:42:16.680]   that the reason I was doing this for of course not
[00:42:21.840 --> 00:42:26.880]   and it was that very night where the bar was busier than usual and like so often
[00:42:26.880 --> 00:42:31.200]   before I was fixing up drinks for people that wouldn't know the difference
[00:42:31.200 --> 00:42:36.960]   between a cosmopolitan and the sazerac
[00:42:36.960 --> 00:42:50.080]   and it was right there where I finally found an opening in the world and
[00:42:50.080 --> 00:42:57.520]   naturally I was provided with a little help and suddenly everything started
[00:42:57.520 --> 00:43:02.520]   moving really fast I only realized it once I was already in the midst of it
[00:43:02.520 --> 00:43:09.880]   like a door that I didn't even know existed before sprung wide open and
[00:43:09.880 --> 00:43:15.320]   once I stepped through that door I found myself in open waters
[00:43:15.760 --> 00:43:20.840]   and it felt like I was finally invited to something
[00:43:20.840 --> 00:43:29.080]   the multitude of what like before me became clear and clear
[00:43:29.080 --> 00:43:36.200]   and finally the rooms at no walls in the water
[00:43:40.520 --> 00:43:47.400]   and this is it it's just all from my place
[00:43:47.400 --> 00:44:00.520]   so what about you do you want to just wait it out or go and find out for
[00:44:00.520 --> 00:44:02.800]   yourself
[00:44:04.200 --> 00:44:10.880]   as promised I'm gonna let you in on a little secret
[00:44:10.880 --> 00:44:28.800]   I hope you enjoyed
[00:44:30.960 --> 00:44:35.200]   we actually spent quite a lot of time into this video and now we have those
[00:44:35.200 --> 00:44:43.280]   two QR codes where you can register you would also participate on a game to win
[00:44:43.280 --> 00:44:51.680]   Digitech Aloxos vouchers if you participate and yeah happy to contact
[00:44:51.680 --> 00:44:57.960]   you sometime for a project or even for getting into the ETH juniors team I'm
[00:44:57.960 --> 00:45:03.720]   curious how many of you are doing this to get a piece of paper in the end and
[00:45:03.720 --> 00:45:07.040]   very night bartender ring
[00:45:07.040 --> 00:45:22.640]   okay so let's continue so as you were already thrilled before about let's talk
[00:45:22.640 --> 00:45:27.400]   about it's not curves I know you wanted curves but sadly it's circles
[00:45:27.400 --> 00:45:32.640]   so how to apply this actually on a circle because in the end the curve is not an
[00:45:32.640 --> 00:45:38.840]   edge we are currently focusing on straight ones that's why we're doing
[00:45:38.840 --> 00:45:44.400]   lines and that where we are kind of trying to actually adjust them to lines
[00:45:44.400 --> 00:45:51.880]   so let's apply actually the half transformation into a circle so the
[00:45:51.880 --> 00:45:55.920]   difference to do this with the circle so first of all this just works with the
[00:45:55.920 --> 00:46:02.360]   fixed radius ones and in this case we are representing the X and the Y axis and
[00:46:02.360 --> 00:46:08.040]   the center of the circle so the coordinates of the center of the
[00:46:08.040 --> 00:46:17.880]   circle so we are drawing our first edge pixel as we did before which in the
[00:46:17.880 --> 00:46:25.480]   center of the circle is here for the first one you can see so we are drawing
[00:46:25.480 --> 00:46:29.320]   our circles that are the only condition is that we are going through the edge
[00:46:29.320 --> 00:46:37.360]   pixel so we're having here a bunch of circles and as you probably are already
[00:46:37.360 --> 00:46:42.680]   guessing so before when we had a line we could actually adjust this to a line
[00:46:42.680 --> 00:46:54.560]   well you can adjust this to a circle so well my god so what we can see here is
[00:46:54.560 --> 00:47:00.560]   that when we have a circle of a fixed radius and the circle goes through an
[00:47:00.560 --> 00:47:06.880]   edge pixel on the circle center we're actually able to have a circle which is
[00:47:06.880 --> 00:47:13.160]   all the circle centers will form a circle on their own but where do we apply
[00:47:13.160 --> 00:47:21.080]   this well we apply this for a case like this so which I haven't said how what
[00:47:21.080 --> 00:47:24.880]   do we do when we have of course that circle we're doing exactly what we did
[00:47:24.880 --> 00:47:31.120]   before so we are having our beam grid and we are getting all these pixels we are
[00:47:31.120 --> 00:47:35.960]   doing the same bin voting as we did before with the lines and what we can
[00:47:35.960 --> 00:47:43.120]   do with this is this is a blood image so in our case what we want to do is we
[00:47:43.120 --> 00:47:47.280]   want to get the edges but we want to also get the circle like the center of
[00:47:47.280 --> 00:47:50.760]   the circle which you can see here of course for these ones where are like
[00:47:50.760 --> 00:47:56.400]   really perfect circles this works for the ones where we are not having an
[00:47:56.400 --> 00:48:03.200]   entire like in a border it can be more difficult and especially for this ones
[00:48:03.200 --> 00:48:09.880]   we're having actually two that are overlapping getting the the circle
[00:48:09.880 --> 00:48:16.080]   calculated like this is a good way to actually have the information about the
[00:48:16.080 --> 00:48:24.480]   centers okay so what is the problem with edges that you have now learned to love
[00:48:24.480 --> 00:48:32.560]   over the last lectures well edges just work in one direction so this is the
[00:48:32.560 --> 00:48:37.880]   main issue with an edge well then what do we do if we can't predict an edge well
[00:48:37.880 --> 00:48:44.240]   we predict a corner because in the end what a corner what we want with a corner
[00:48:44.240 --> 00:48:49.240]   it's its rotation invariant in its position so what we want desirable in
[00:48:49.240 --> 00:48:53.360]   a corner detector is it's accurate in its location invariance to shift
[00:48:53.360 --> 00:48:59.920]   rotations scale brightness and it's robust against noise and higher repeatability
[00:48:59.920 --> 00:49:08.960]   so to set this in a more mathematical way we are writing down the sense of it
[00:49:08.960 --> 00:49:13.200]   sensitivity which in this case it's exactly like we had before between the
[00:49:13.200 --> 00:49:17.440]   image and the template just in this case what we're having is a patch of an image
[00:49:17.440 --> 00:49:24.840]   and it's the difference squared of a slightly moved patch in the same image so
[00:49:24.840 --> 00:49:29.920]   then we are defining the linear approximation of a delta x and delta y
[00:49:29.920 --> 00:49:38.000]   small ones which is this which we then can define this huge equation which of
[00:49:38.000 --> 00:49:42.480]   course we're taking out this my matrix just defining it by M you will see it
[00:49:42.480 --> 00:49:49.680]   further more often appearing and this actually shows us that it's an ellipsis
[00:49:49.680 --> 00:49:54.120]   so if we are representing this in a 3d space the form that we're normally
[00:49:54.120 --> 00:50:01.680]   getting is an ellipsis so how does this look in reality looks like this so we're
[00:50:01.680 --> 00:50:06.440]   having a sensitivity we're having here this is no edge this is no corner this
[00:50:06.440 --> 00:50:11.680]   is anything so it's a homogeneous space so in 3d representation we have it like
[00:50:11.680 --> 00:50:23.440]   this so this one has two small yeah two small eigenvalues then we have the edge
[00:50:23.440 --> 00:50:30.240]   which is here a point it has this shape so it does one eigenvalue then we have a
[00:50:30.240 --> 00:50:36.440]   corner which is probably this one supposed to be and it has two large
[00:50:36.440 --> 00:50:45.120]   eigenvalues so this is how kind of what we are trying to do would look like
[00:50:45.120 --> 00:50:52.600]   represented in the 3d space and this of course what we're trying to do as an
[00:50:52.600 --> 00:50:58.600]   example we could try to maximize the eigenvalues of the magnitude so how do
[00:50:58.600 --> 00:51:08.600]   we kind of define this kind of in space so we can draw this down in so to do
[00:51:08.600 --> 00:51:12.600]   key point detection which is what we are aiming to do since the beginning of
[00:51:12.600 --> 00:51:17.760]   this lecture is we often have the eigenvalues which are delta 1 and delta
[00:51:17.760 --> 00:51:24.040]   2 and these eigenvalues are also the normal matrix they are the structure of
[00:51:24.040 --> 00:51:29.680]   the matrix so what we have is the matrix that we had defined before which is the
[00:51:29.680 --> 00:51:36.000]   big M and what we are doing is we can define actually the spaces like this so
[00:51:36.000 --> 00:51:41.480]   we have the both eigenvalues normally we're having our flat region which was
[00:51:41.480 --> 00:51:45.560]   our sky and then we have all this region which is like corners which is when a
[00:51:45.560 --> 00:51:50.680]   delta 1 and delta 2 are large values so the eigenvalues are really big and then
[00:51:50.680 --> 00:51:57.160]   on the edges which are on the sides we have actually the eigenvalues one of
[00:51:57.160 --> 00:52:02.640]   them is much larger than the other one so of course this is not constant and
[00:52:02.640 --> 00:52:07.160]   it's can if you cannot apply this always so that's why we have the measure of
[00:52:07.160 --> 00:52:15.600]   cornerness so the measure of cornerness is this amazing equation which he's most
[00:52:15.600 --> 00:52:23.240]   important element is K what does K do to showcase this in a easy way so K moves
[00:52:23.240 --> 00:52:28.240]   actually the borders between the different edges of flat region and
[00:52:28.240 --> 00:52:36.280]   corners so you can see here this is a K of 0.2 applied in the in the cornerness
[00:52:36.280 --> 00:52:42.080]   and you can see that the area of the corner is actually much smaller while the
[00:52:42.080 --> 00:52:51.880]   area of the edges becomes much bigger while with the K of 0.05 the space of
[00:52:51.880 --> 00:52:57.800]   the corner is much bigger than the space we have actually on the edges the only
[00:52:57.800 --> 00:53:03.840]   one that stays more or less constant the flat area so how does this look in
[00:53:03.840 --> 00:53:08.120]   reality because that's kind of what we are mostly interested we're having our
[00:53:08.120 --> 00:53:13.480]   image we're having two viewpoints we are having different illuminations we're
[00:53:13.480 --> 00:53:19.520]   but it's in the end there should be enough key points to actually find in
[00:53:19.520 --> 00:53:23.880]   this image so first of all we apply the Harris cornerness so you can see the more
[00:53:23.880 --> 00:53:31.640]   red the actually more stronger corners while the bluer the less importance we
[00:53:31.640 --> 00:53:36.800]   are giving them we apply a threshold as we have been doing for the entire
[00:53:36.800 --> 00:53:42.400]   lecture but you can already see here this image we are also having key points on
[00:53:42.400 --> 00:53:50.960]   the reflection we had on the table which is not ideal so this image I was
[00:53:50.960 --> 00:53:55.080]   thinking how to showcase this better to you but there is no way that you can see
[00:53:55.080 --> 00:53:59.200]   this like there are just some points which are supposed to be the four
[00:53:59.200 --> 00:54:03.200]   corners that we kind of filtered out but on the image you can actually see them
[00:54:03.200 --> 00:54:07.160]   much better overlaid so all the red points are actually the key points that
[00:54:07.160 --> 00:54:15.320]   we actually found in the image they are all looking fine except for this so you
[00:54:15.320 --> 00:54:19.320]   can see that with our approximation that we're using right now we are super
[00:54:19.320 --> 00:54:25.400]   sensitive to light changes to reflections and to basically other kind of
[00:54:25.400 --> 00:54:32.080]   environmental changes that we are actually not interested in having so how
[00:54:32.080 --> 00:54:38.480]   can we do better localization of this so what we can do is we can give more
[00:54:38.480 --> 00:54:43.320]   importance to centred pixels by using the Gaussian weighting function which we
[00:54:43.320 --> 00:54:49.520]   define as this so to do this we compute the super pixel localization by fitting
[00:54:49.520 --> 00:54:56.560]   a parabola to the cornerness function so why basically we are having some issues
[00:54:56.560 --> 00:55:01.760]   as you already pointed out before as an example the robustness of Harris of the
[00:55:01.760 --> 00:55:06.280]   corner detector we of course want this to be as more robust as possible so we
[00:55:06.280 --> 00:55:10.120]   introduced the Gaussian which already gives us more robustness we are invariant
[00:55:10.120 --> 00:55:14.920]   to brightness offset so okay we fix that we're invariant to shift and rotation
[00:55:14.920 --> 00:55:21.640]   that's great but we are not invariant of scaling if I zoom in on a corner it's an
[00:55:21.640 --> 00:55:29.040]   edge while if I zoom out it's a corner so in this case we're actually with the
[00:55:29.040 --> 00:55:34.680]   original the robustness of Paris we're still not invariant to scaling which
[00:55:34.680 --> 00:55:39.720]   luckily in 2002 Schmidt actually came up with a improved way of doing this which
[00:55:39.720 --> 00:55:46.520]   we are not seeing in this lecture so yeah I don't know if you saw it in the
[00:55:46.520 --> 00:55:51.760]   video but actually one of the things that was shown in the Coliseum construction
[00:55:51.760 --> 00:55:57.320]   was they're using sift features sift is nowadays so that what you are seeing a
[00:55:57.320 --> 00:56:01.160]   structure from motion called so it's actually meant that you're using the
[00:56:01.160 --> 00:56:05.280]   motion of the camera moving to actually reconstruct a 3d object which in this
[00:56:05.280 --> 00:56:11.040]   case was the Coliseum so you can see this was the paper was written in 99 and
[00:56:11.040 --> 00:56:17.840]   nowadays this is still a super trending topic in research so of course not the
[00:56:17.840 --> 00:56:24.360]   original one but variations of it so what does sift actually do we're having
[00:56:24.360 --> 00:56:28.520]   our image of a track we are getting features out of it which are these kind
[00:56:28.520 --> 00:56:32.840]   of image patches that are kind of more significant we're having them here so
[00:56:32.840 --> 00:56:38.200]   that you can see them and sift is capable to actually recover all these
[00:56:38.200 --> 00:56:42.640]   features in this other image so of course this one he's not seeing it in the
[00:56:42.640 --> 00:56:46.560]   image because well he's actually seeing it in the image but you just can't
[00:56:46.560 --> 00:56:52.280]   recover that one well with all the others he's doing a good job so actually sift
[00:56:52.280 --> 00:56:59.800]   is actually it recovers with position orientation and scale so now it's
[00:56:59.800 --> 00:57:04.840]   actually like solving most of our problems so let's talk about all the
[00:57:04.840 --> 00:57:12.360]   all the things about sift so first let's define the position so sift what it does
[00:57:12.360 --> 00:57:17.400]   it's looking for the strong response of the difference of the Gaussian so we're
[00:57:17.400 --> 00:57:21.840]   doing the two difference between two gaussians in this case and it only
[00:57:21.840 --> 00:57:28.880]   considers the local maxima so that's for the position second for the scale how do
[00:57:28.880 --> 00:57:37.480]   we solve this to scale so the scale phase as you can see is defined as applying
[00:57:37.480 --> 00:57:43.040]   a Gaussian over and over again over an image so what we're doing with this then
[00:57:43.040 --> 00:57:49.200]   is we're doing the difference of the Gaussian of the of the succeeding images
[00:57:49.200 --> 00:57:55.800]   that we have calculated the Gaussian which you can see here and yeah next we
[00:57:55.800 --> 00:58:02.600]   only consider local maxima of the column and rows and scale number of times we
[00:58:02.600 --> 00:58:10.400]   did this process so to explain this let's say we're doing this on three
[00:58:10.400 --> 00:58:19.080]   consecutive scales and basically we're detecting the middle which is this x so
[00:58:19.080 --> 00:58:24.080]   basically the it must be larger than all the neighbors that are surrounding it so
[00:58:24.080 --> 00:58:29.480]   larger than all of this this and this for this a pixel accuracy this is kind of
[00:58:29.480 --> 00:58:35.480]   the rows that we're trying to why we're how we are filtering out for having the
[00:58:35.480 --> 00:58:44.760]   scale so now with the orientation so we create a histogram which you can see
[00:58:44.760 --> 00:58:49.640]   here so normally the highest point of the histogram will be the the assigned
[00:58:49.640 --> 00:58:53.160]   canonical orientation so it's going to be the designed one what you're going to
[00:58:53.160 --> 00:58:58.720]   use each key specifies the stable to decoordinate and in the end an orientation
[00:58:58.720 --> 00:59:02.160]   map looks actually like something like this so each of these lines is kind of
[00:59:02.160 --> 00:59:07.680]   pointing us an orientation that is kind of like found in the in the edges and
[00:59:07.680 --> 00:59:15.760]   lines so this looks like this which is total cows we're having our original
[00:59:15.760 --> 00:59:24.200]   image we are trying to find edges and corners so we first have here everything
[00:59:24.200 --> 00:59:29.320]   that was detected so this is of course full of lines which is not ideal and
[00:59:29.320 --> 00:59:33.600]   full of orientations in the second image what we are doing is we're already
[00:59:33.600 --> 00:59:38.200]   applying a threshold to actually just take the ones with the minimum contrast
[00:59:38.200 --> 00:59:43.120]   so we already filtered out you can see some here on the roof they are not
[00:59:43.120 --> 00:59:49.440]   present anymore so we already got this down from 800 points to actually 729
[00:59:49.440 --> 00:59:54.720]   and then the last step which is we are taking out the curvatures so all the
[00:59:54.720 --> 00:59:57.840]   curves that you wanted before we are actually removing them applying a
[00:59:57.840 --> 01:00:03.120]   threshold and in the end we get a filtered image with the most important
[01:00:03.120 --> 01:00:11.680]   contrast and corners on this image so okay so let's continue with sift
[01:00:11.680 --> 01:00:18.000]   descriptors so this is how a sift descriptor looks like is the threshold
[01:00:18.000 --> 01:00:23.840]   image gradient or sampled over 16 by 16 in this case it's 12 by 12 I think no
[01:00:23.840 --> 01:00:29.000]   it's 8 by 8 okay so it's 8 by 8 but yeah you have normally eight orientations by
[01:00:29.000 --> 01:00:33.240]   four by four histogram array which is 128 dimensions which are normally going
[01:00:33.240 --> 01:00:39.120]   to work with this is just an expample made smaller that's why your descriptor
[01:00:39.120 --> 01:00:44.600]   is just two by two but this is normally how this kind of would look like with
[01:00:44.600 --> 01:00:49.240]   the orientations that you would have the key point descriptors and this was the
[01:00:49.240 --> 01:00:54.280]   first time this was actually used for it's actually to do panoramic stitching I
[01:00:54.280 --> 01:01:01.880]   don't know if you have done this before I don't think so so what we do is in
[01:01:01.880 --> 01:01:05.760]   this paper what they did is they have this huge amount of images you can see
[01:01:05.760 --> 01:01:09.040]   that they are actually really random you have some that are of course from the
[01:01:09.040 --> 01:01:14.440]   mountain but you have also some pictures here is a C-star or some people and what
[01:01:14.440 --> 01:01:19.560]   they did in this paper which you have here listed they found key points that
[01:01:19.560 --> 01:01:23.200]   were actually matching between the random images and try to find as many
[01:01:23.200 --> 01:01:27.640]   images they could put together so this is actually what you are probably going
[01:01:27.640 --> 01:01:33.600]   to see for their own I assume they were capable with this bunch of images to do
[01:01:33.600 --> 01:01:38.360]   these two panoramas so you have all this one which basically they are matching
[01:01:38.360 --> 01:01:41.880]   and kind of adjusting the different images with the key points they actually
[01:01:41.880 --> 01:01:49.080]   were able to get out and this creates actually this really long panorama
[01:01:49.080 --> 01:01:56.040]   nowadays cameras use another algorithm but I don't know if you had I don't know
[01:01:56.040 --> 01:02:02.800]   how old you are so but there were the first digital cameras that you kind of
[01:02:02.800 --> 01:02:05.640]   would have smaller they were actually doing when you were doing a panorama
[01:02:05.640 --> 01:02:09.240]   image or even your iPhone I think it does something really similar to this
[01:02:09.240 --> 01:02:12.640]   they're actually trying to find key points and they're actually matching
[01:02:12.640 --> 01:02:16.080]   them together and that's why when you're actually moving it too much it
[01:02:16.080 --> 01:02:20.440]   can't recover it and that's why in this case you can see normally what your
[01:02:20.440 --> 01:02:25.360]   iPhone does of course it cuts it out so it looks beautiful but this would
[01:02:25.360 --> 01:02:29.760]   actually be what you can see because none of us has a good enough hand to
[01:02:29.760 --> 01:02:35.400]   actually hold the camera static so normally you would have this funny
[01:02:35.400 --> 01:02:40.880]   artifacts which are basically related to that your hand is just not stable and
[01:02:40.880 --> 01:02:45.360]   this kind of the the real application of this and well you have seen the video
[01:02:45.360 --> 01:02:49.680]   so this is actually used like most of the things you have seen today are used and
[01:02:49.680 --> 01:02:56.080]   just to give you some other information I'm still this is a paper from Jiri
[01:02:56.080 --> 01:03:03.160]   Matas you have learned lines you have learned to apply this on circles but you
[01:03:03.160 --> 01:03:06.120]   can see here in this paper they are actually capable of applying this
[01:03:06.120 --> 01:03:10.040]   actually to other shapes and to actually recover key points and some other
[01:03:10.040 --> 01:03:15.400]   like blobs they don't even need to be some kind of fixed geometrical figure so
[01:03:15.400 --> 01:03:21.120]   actually key point matching is kind of an active field which people are actually
[01:03:21.120 --> 01:03:26.920]   researching on if someone is interested you can actually look at at the paper and
[01:03:26.920 --> 01:03:34.320]   yeah besides that next week you're learning with mark for your transformations
[01:03:34.320 --> 01:03:39.880]   and hopefully Philip will have uploaded you until then the slides so that you
[01:03:39.880 --> 01:03:43.320]   can look into them
[01:03:43.320 --> 01:03:46.680]   [APPLAUSE]

